# Methodology {#sec-method}

In the following we first set out a generalized framework for gradient-based counterfactual search in @sec-method-general to introduce the various counterfactual generators we have chosen to use in our experiments. We then describe the experimental setup in @sec-method-experiment and introduce several evaluation metrics used to benchmark the different generators.

## A Generalized Framework for Gradient-Based Counterfactual Search {#sec-method-general}

In this work we have chosen to focus on a number of gradient-based counterfactual generators to investigate the endogenous dynamics we introduced in @sec-intro. Gradient-based counterfactual search is well-suited for differentiable black-box models like deep neural networks. 

### From individual recourse ...

We can restate @eq-solution in a more general form that encompasses most gradient-based approaches to counterfactual search:

$$
\begin{aligned}
\mathbf{s}^\prime &= \arg \min_{\mathbf{s}^\prime \in \mathcal{S}} \left\{ \sum_{k=1}^{K} {\ell(M(f(s_k^\prime)),t)}+ \lambda {h(f(s_k^\prime)) }  \right\}
\end{aligned}
$$ {#eq-general}

Here $\mathbf{s}^\prime=\left\{s_k^\prime\right\}_K$ is the stacked $K$-dimensional array of counterfactual states  and $f: \mathcal{S} \mapsto \mathcal{X}$ maps from the counterfactual state space to the feature space. In the case of the baseline counterfactual generator [@wachter2017counterfactual] $f$ is just the idendity function and the number of counterfactuals $K$ is equal to one. This generator, which we shall refer to as **Wachter** in the following, shall serve as the baseline against which all other gradient-based methodologies will be compared. In particular, we include include the following generator in our benchmarking exercises: REVISE [@joshi2019towards], CLUE [@antoran2020getting], DICE [@mothilal2020explaining] and a greedy approach that relies on probabilistic models [@schut2021generating]. 

Both **REVISE** and **CLUE** search counterfactuals in some latent embedding $S \subset \mathcal{S}$ instead of the feature space directly. The latent embedding is learned by a separate generative model that is tasked with learning the data generating process (DGP) of $X$. In this case $f$ in @eq-general corresponds to the decoder part of the generative model, in other words the deterministic function that maps back from the latent embedding to the feature space. Provided the generative model is well-specified, traversing the latent embedding typically results in realistic and plausible counterfactuals, because they are implicitly generated by the (learned) DGP [@joshi2019towards]. CLUE distinguishes itself from REVISE and other counterfactual generators in that it aims to minimize the predictive uncertainty of the model in question $M$. To quantify predictive uncertainty the authors rely on entropy estimates for probabilistic models. The **Greedy** approach proposed by @schut2021generating also works with the subclass of models $\tilde{\mathcal{M}}\subset\mathcal{M}$ that can produce predictive uncertainty estimates. The authors show that in this setting the complexity penalty $h(\cdot)$ in @eq-general is redundant and meaningful counterfactuals can be generated in a fast and efficient manner through a modified Jacobian-based Saliency Map Attack (JSMA). Finally, **DICE** distinguishes itself from all other generators considered here in that it aims to generate a diverse set of $K>1$ counterfactuals. To this end the authors use a complexity penalty $h(\mathbf{s}^\prime)$ that favours diverse outcomes, in the sense that $s_1, ... , s_K$ look as different from each other as possible. 

Our motivation for including these different generators in our analysis, is that they all offer slightly different approaches to generate meaningful counterfactuals for differentiable black-box models. We hypothesize that generating more **meaningful** counterfactuals should mitigate the endogenous dynamics illustrated in @fig-poc in @sec-intro. This intuition stems from the underlying idea that more meaningful counterfactuals are generated by the same or at least a very similar data generating process as the training data. All else equal, counterfactuals that fulfill this basic requirement should be less prone to trigger domain and model shifts. 

### ... towards collective recourse

To explicitly address the notion that individual recourse may affect the outcome and prospect of other individuals, we propose to extend @eq-general as follows:

$$
\begin{aligned}
\mathbf{s}^\prime &= \arg \min_{\mathbf{s}^\prime \in \mathcal{S}}  \sum_{k=1}^{K} {\ell(M(f(s_k^\prime)),t)} \\ &+ \lambda_1 {h(f(s_k^\prime)) } + \lambda_2 {g(f(s_k^\prime))}  
\end{aligned}
$$ {#eq-collective}

Here $h(f(s_k^\prime))$ denotes the proxy for private costs faced by the individual as before and $\lambda_1$ governs to what extent that private cost ought to be penalised. The newly introduced term $g(f(s_k^\prime))$ is meant to capture and address social costs incurred by the collective of individuals in response to changes in $\mathbf{s}^\prime$. The trade-off between private and social costs is determined by the ratio between $\lambda_1$ and $\lambda_2$. As with individual recourse, the exact choice of $g(\cdot)$ is not obvious, nor do we intend to provide a definite answer in this work, if such even exists. A straight-forward choice simply extends the baseline approach by @wachter2017counterfactual: instead of only penalizing the distance of the individuals' counterfactual to its factual, we propose penalizing its distance to some sensible point in the target domain, for example the sample average: $\bar{\mathbf{x}}$. For such a recourse objective, higher choices of $\lambda_2$ relative to $\lambda_1$ will lead counterfactuals to gravitate towards the specified point in the target domain. In the remainder of this paper we will therefore refer to this approach as **Gravitational** generator, when we investigate its potential usefulness for mitigating endongenous macrodynamics^[Note that despite the naming convention our goal here is not to provide yet another counterfactual generator, but merely investigate the most simple penalty we can think of with respect to its effectiveness.].

### A note on convergence

For this simple mitigating strategy underlying the Gravitational generator to work as expected, one needs to ensure that counterfactual search continues, even after a predetermined threshold probability $\gamma$ has potentially already been reached. @fig-convergence illustrates this distinction: if one chooses to terminate search once the desired threshold is reached (left panel) the gravitational pull towards $\bar{\mathbf{x}}$ is never actually satisfied (compare to right panel). More generally, if convergence is defined simply in terms of flipping the predicted label with some desired degree of confidence, this corresponds to essentially ignoring any parts of the counterfactual search objective that do not involve $\ell(M(f(s_k^\prime)),t)$ beyond that point. While this may be appropriate for some applications, in general this seems like an odd convention. Since we nonetheless seen convergence specified simply in terms of reaching the threshold probability in some places^[@joshi2019towards define convergence of Algorithm 1 in this way. The implementation of @wachter2017counterfactual in CARLA is also defined in this way.], we thought it worth making this distinction explicit.

![Comparison of counterfactual search outcome with simple (left) and strict convergence (right).](www/gravitational_generator_comparison.png){#fig-convergence fig.pos="h" width=45%}

## Experimental Setup {#sec-method-experiment}

The dynamics illustrated in @fig-poc in @sec-intro were generated through a simple experiment that aims to simulate the process of algorithmic recourse in practice. We begin in the static setting at time time $t=0$: given some pre-trained classifier $M$ we generate recourse for a random batch of $B$ individuals in the non-target class. Note that we focus our attention on classification problems, since classification poses the most common practical use-case for algorithmic recourse. In order to simulate the dynamical process we suppose that the model $M$ is retrained following the actual implementation of recourse in time $t=0$. Following the update to the model, we assume that at time $t=1$ recourse is generated for yet another random subset of individuals in the non-target class. This process is repeated for a number of time periods $T$. To get a clean read on endogenous dynamics we keep the total population of samples closed: we allow existing samples to move from factual to counterfactual states, but do not allow any entirely new samples to enter the population. The experimental setup is summarized in Algorithm \ref{algo-experiment}

\begin{algorithm}
\caption{Experiment}\label{algo-experiment}
\begin{algorithmic}[1]
\Procedure{Experiment}{$M,D,G$}
\State $t\gets 0$
\While{$t<T$}
\State $D_B \subset D$
\State $D_B\gets G(D_B)$ \Comment{Generate counterfactuals.}
\State $M\gets M(D)$ \Comment{Retrain model.}
\EndWhile
\State \textbf{return} $M,D$
\EndProcedure
\end{algorithmic}
\end{algorithm}


A noteworthy practical consideration is the choice of $T$ and $B$. The higher these values, the more factual instances undergo recourse throughout the entire experiment. Of course, this is likely to lead to more pronounced domain and model shifts by time $T$. At the same time, it is generally improbable that a very large part of the population would request an explanation of the algorithm’s decisions. In our experiments, we choose the values such that $T \cdot B$ corresponds to the application of recourse on $25-50\%$ of the negative instances from the initial dataset. As we collect data at each time $t$, we can also verify the impact of recourse when it is implemented for a smaller number of individuals. Using our framework the experiment can be conducted on an arbitrary number of algorithmic recourse generators. As all generators make use of the same initial model and initial dataset, the differences in domain and model shifts observed throughout the rounds depend solely on the employed generator.

## Data {#sec-method-data}

We have chosen to work with both synthetic and real-world datasets. Using synthetic data allows us to impose distributional properties that may affect the resulting recourse dynamics. Following @upadhyay2021towards we generate synthetic data in $\mathbb{R}^2$ to also allow for a visual interpretation of the results. Real-world data is used in order to assess if endogenous dynamics also occur in higher-dimensional settings.

### Synthetic data

We use 6 synthetic binary classification datasets consisting of 200-400 samples grouped in normally-distributed clusters.^[To see how the data is generated see here: [https://github.com/pat-alt/AlgorithmicRecourseDynamics.jl/blob/main/notebooks/synthetic_datasets.ipynb](https://github.com/pat-alt/AlgorithmicRecourseDynamics.jl/blob/main/notebooks/synthetic_datasets.ipynb)] The datasets are presented in @fig-synthetic-data (see also Appendix A for a formal description). Samples from the negative class are marked in blue while samples of the positive class are marked in orange.

![PLACEHOLDER: A visualization of the synthetic classification datasets used in our experiments.](www/synthetic_data.png){#fig-synthetic-data}

Ex-ante we expect to see that Wachter will create a new cluster of counterfactual instances in the proximity of the initial decision boundary. Thus, the choice of a black-box model may have an impact on the paths of the recourse. For generators that use latent space search (@joshi2019towards, @antoran2020getting) or rely on (and have access to) probabilistic models (@antoran2020getting, @schut2021generating) we expect that counterfactuals will end up in regions of the target domain that are densely populated by training samples. Finally, we expect to see the counterfactuals generated by DiCE to be uniformly spread around the feature space inside the target class. 

### Real-world data

Additionally, we use two real-world datasets from the Finance domain. Firstly, we use the Give Me Some Credit dataset which was open-sourced on Kaggle for the task to predict whether a borrower is likely to experience financial difficulties in the next two years [@gmsc_data]. Originally consisting of 250,000 instances with 11 numerical attributes, the dataset was randomly undersampled to result in a balanced subsample made up of 3000 individuals. Secondly, we the German Credit dataset which involves the task of predicting if bank customers are credit-worthy or not [@germancredit1994]. It consists of 700 positive and 300 negative instances charaterized by 7 numerical and 13 categorical attributes. We process the dataset in two ways: (1) the values of the "Personal status and sex" feature are aggregated by the two represented genders; (2) the most common values are calculated for all categorical features such that a feature $x_d$ with the mode $\bar{x}_d$ is transformed into a new binary feature $\tilde{x}_d=\mathbb{I}_{x_{d,i}>=\bar{x}_d}$. Binarization ensures that we can use all counterfactual generators in the benchmark.

## Evaluation Metrics {#sec-method-metrics}

We formulate two desiderata for the set of metrics used to measure domain and model shifts induced by recourse. First, the metrics should be applicable regardless of the dataset or classification technique so that they allow for the meaningful comparison of the generators in various scenarios. As the knowledge of the underlying probability distribution is rarely available, the metrics should be empirical and non-parametric. This further ensures that we can also measure large datasets by sampling from the available data. Moreover, while our study was conducted in a two-class classification setting, our choice of metrics should remain applicable in the future research on multi- class recourse problems. Second, the set of metrics should allow to capture various aspects of the previously mentioned magnitude, path, and tempo of changes while remaining as small as possible.

### Domain Shifts

To quantify the magnitude of domain shifts we rely on an unbiased estimate of the squared population **Maximum Mean Discrepancy (MMD)** given as:

$$
\begin{aligned}
MMD^2_u[F,{X}^\prime,\tilde{X}^\prime] &= \frac{1}{m(m-1)}\sum_{i=1}^m\sum_{j\neq i}^m k(x_i,x_j) \\ &+ \frac{1}{n(n-1)}\sum_{i=1}^n\sum_{j\neq i}^n k(\tilde{x}_i,\tilde{x}_j) \\ &- \frac{2}{mn}\sum_{i=1}^m\sum_{j=1}^n k(x_i,\tilde{x}_j)
\end{aligned}
$$ {#eq-mmd}

where $\mathcal{F}$ is a unit ball in a Reproducing Kernel Hilbert Space H [27], and $X$, $\tilde{X}$ represent independent and identically distributed samples drawn from probability distributions $p$ and $q$ respectively [28]. MMD is a measure of the distance between the kernel mean embeddings of $p$ and $q$ in RKHS $\mathcal{H}$. An important consideration is the choice of the kernel function $k(\cdot,\cdot)$. In our implementation we make use of the radial basis function (RBF) kernel with a constant length-scale parameter of $0.5$. As RBF captures all moments of distributions $p$ and $q$, we have that $MMD_u^2[F,X,\tilde{X}]=0$ if and only if $X=\tilde{X}$.

The evaluation metric in @eq-mmd is computed after every round $t=1,...,T$ of the experiment. To assess the statistical significance of the observed shifts under the null hypothesis that samples $X$ and $\tilde{X}$ were drawn from the same probability distribution we follow @arcones1992bootstrap. To that end, we combine the two samples and generate a large number of permutations of $X + \tilde{X}$. Then, we split the permuted data into two new samples $X^\prime$ and $\tilde{X}^\prime$ having the same size as the original samples. Then under the null hypothesis we should have that $MMD_u^2[F,X^\prime,\tilde{X}^\prime]$ be approximately equal to $MMD_u^2[F,X,\tilde{X}]$. The corresponding $p$-value can then be calculated by counting how these two quantities are not equal.

We calculate the MMD for both classes individually based on the ground truth labels, i.e. the labels that samples were assigned in time $t=0$. Throughout our experiments we generally do not expect the distribution of the negative class to change over time – application of recourse reduces the size of this class, but since individuals are sampled uniformly the distribution should remain unaffected. Conversely, unless a recourse generator can perfectly replicate the original probability distribution, we expect the MMD of the positive class to increase. Thus, when discussing MMD, we generally mean the shift in the distribution of the positive class.

Finally, **feature mean and feature standard deviation** are also calculated to verify how the implementation of recourse impacts every attribute in the dataset. Although MMD already captures information about the expected value and variance, we may also be interested in a more granular look at individual features. 

### Model Shifts

As our baseline for quantifying model shifts we measure perturbations to the model parameters at each point in time $t$ following @upadhyay2021towards. We define $\Delta=||\theta_{t+1}-\theta_{t}||^2$, that is the euclidean distance between the vectors of parameters before and after retraining the model $M$. We shall refer to this baseline metric simply as **Perturbations**.

We extend the metric in @eq-mmd for the purpose of quantifying model shifts. Specifically, we introduce **Predicted Probability MMD (PP MMD)**: instead of applying @eq-mmd to features directly, we apply it to the predicted probabilities assigned to a set of samples by the model $M$. If the model shifts, the probabilities assigned to each sample will change; again, this metric will equal 0 only if the two classifiers are the same. It is worth noting that while we apply the technique to samples drawn uniformly from the dataset, it can also be employed on arbitrary points in the entire feature space (or a subspace). The latter approach is theoretically more robust. Unfortunately, in practice this approachs suffers from the curse of dimensionality, since it becomes increasingly difficult to select enough points to overcome noise as the dimension $D$ grows.

As an alternative to PP MMD we use a pseudo-distance for the **Disagreement Coefficient** (Disagreement). This metric was introduced in @hanneke2007bound and estimates $p(M(x) \neq M^\prime(x))$, that is the probability that two classifiers do not agree on the predicted outcome for a randomly chosen sample. Thus, it is not relevant whether the classification is correct according to the ground truth, but only whether the sample lies on the same side of the two respective decision boundaries. In our context, this metric quantifies the overlap between the initial model (trained before the application of recourse) and the updated model. A Disagreement Coefficient unequal to zero is indicative of a model shift. The opposite is not true: even if the Disagreement Coefficient is equal to zero a model shift may still have occured. This is one reason for why PP MMD is our our preferred metric.

Finally, we introduce **Decisiveness** as a metric that quantifies the likelihood that a model assigns a high probability to its classification of any given sample. We define the metric simply as ${1\over{N}}\sum_{i=0}^N(\sigma(M(x)) − 0.5)^2$ where $M(x)$ are predicted logits from a binary classifier and $\sigma$ denotes the sigmoid function. This metric provides an unbiased estimate of the binary classifier's tendency to produce high-confidence predictions in either one of the two classes. Although the exact values for this metric are not important for our study, they can be used to detect model shifts. If decisiveness changes over time, then this is indicative of the decision boundary moves towards either one of the two classes.

