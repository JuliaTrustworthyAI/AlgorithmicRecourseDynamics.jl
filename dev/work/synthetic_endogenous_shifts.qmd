---
title: Synthetic data
jupyter: julia-1.6
---

In this notebook we will use toy data to see how endogenous domain shifts and the resulting model shifts can have implications on the validity and cost of algorithmic recourse.

```{julia}
include("src/load.jl")
using CounterfactualExplanations, Flux, Plots, PlotThemes, Random, BayesLaplace, LinearAlgebra
using Base.Filesystem: joinpath
theme(:wong)
using Logging
disable_logging(Logging.Info)
output_path = "dev/work/output/synthetic"
www_path = "dev/work/www/synthetic"
```

## Classifiers

```{julia}
# Number of points to generate.
N = 100
using CounterfactualExplanations.Data
xs, ys = Data.toy_data_linear(N)
X = hcat(xs...) # bring into tabular format
data = zip(xs,ys)
```

### Logistic Regression

```{julia}
nn = Chain(Dense(2,1))
Î» = 0.5
sqnorm(x) = sum(abs2, x)
weight_regularization(Î»=Î») = 1/2 * Î»^2 * sum(sqnorm, Flux.params(nn))
loss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y) + weight_regularization();
```

```{julia}
using Flux.Optimise: update!, ADAM
opt = ADAM()
epochs = 50
function _forward_nn(nn, data, epochs)
    for epoch = 1:epochs
        for d in data
            gs = gradient(params(nn)) do
            l = loss(d...)
            end
            update!(opt, params(nn), gs)
        end
    end
    return nn
end
_forward_nn(nn, data, epochs)
```

```{julia}
using CounterfactualExplanations.Models
import CounterfactualExplanations.Models: logits, probs # import functions in order to extend
import Main.Models: retrain

# Step 1)
struct NeuralNetwork <: Models.AbstractFittedModel
    model::Any
end

# Step 2)
logits(M::NeuralNetwork, X::AbstractArray) = M.model(X)
probs(M::NeuralNetwork, X::AbstractArray)= Ïƒ.(logits(M, X))

# Step 3)
function retrain(M::NeuralNetwork, data; n_epochs=10, Ï„=1.0) 
    nn = M.model
    nn = _forward_nn(nn, data, n_epochs)
    M = NeuralNetwork(nn)
    return M
end

M = NeuralNetwork(nn)
```

```{julia}
#| echo: false
# Plot the posterior distribution with a contour plot.
plt = plot_contour(X',ys,M);
savefig(plt, joinpath(www_path, "nn_contour.png"))
```

![](www/nn_contour.png)

### Laplace Redux

```{julia}
la = Laplace(nn, Î»=Î»)
BayesLaplace.fit!(la, data)
```

```{julia}
# Step 1)
struct LaplaceNeuralNetwork <: Models.AbstractFittedModel
    la::BayesLaplace.Laplace
end

# Step 2)
logits(M::LaplaceNeuralNetwork, X::AbstractArray) = M.la.model(X)
probs(M::LaplaceNeuralNetwork, X::AbstractArray)= BayesLaplace.predict(M.la, X)

# Step 3)
function retrain(M::LaplaceNeuralNetwork, data; n_epochs=10, Ï„=1.0) 
    nn = M.la.model
    nn = _forward_nn(nn, data, n_epochs)
    la = Laplace(nn, Î»=Î»)
    BayesLaplace.fit!(la, data)
    M = LaplaceNeuralNetwork(la)
    return M
end

Má´¸ = LaplaceNeuralNetwork(la)
```

```{julia}
#| echo: false
# Plot the posterior distribution with a contour plot.
plt = plot_contour(X',ys,Má´¸);
savefig(plt, joinpath(www_path, "la_contour.png"))
```

![](www/la_contour.png)

## Experiment

```{julia}
# Models:
models = (Bayesian=Má´¸, Plugin=M)

# Generators:
generators = (Bayesian=GreedyGenerator(), Plugin=GenericGenerator())

# Variables:
Î¼ = [0.01,0.1,0.25]
Î³ = [0.50,0.75,0.9]
grid_ = Experiments.GridVariables(Î¼, Î³)
n_rounds = 10
n_folds = 5
target = 1.0
T = 1000
```

```{julia}
# Experiments:
experiment = Experiments.Experiment(X,ys,models[:Bayesian],target,grid_,n_rounds)
experiment_plugin = Experiments.Experiment(X,ys,models[:Plugin],target,grid_,n_rounds)
experiments = (experiment, experiment_plugin)
```

```{julia}
#| eval: false
using Random, CSV, DataFrames
Random.seed!(1234)
for j in 1:length(generators)
    outcome, path = Experiments.run_experiment(experiments[j], generators[j], n_folds, T=T, store_path=true)
    Experiments.save_path(joinpath(output_path, string(keys(generators)[j])),path)
    CSV.write(joinpath(output_path, string(keys(generators)[j]) * ".csv"), DataFrame(outcome))
end
```

```{julia}
#| eval: false
using Main.Experiments: plot_path
paths = [Experiments.load_path(joinpath(output_path, string(keys(generators)[j]))) for j âˆˆ 1:length(generators)];
anim = Experiments.plot_path(paths[1],Î“=Î³,ð™ˆ=Î¼)
gif(anim,joinpath(www_path,"bayesian.gif"), fps=2)
anim = Experiments.plot_path(paths[2],Î“=Î³,ð™ˆ=Î¼)
gif(anim,joinpath(www_path,"plugin.gif"), fps=2)
```

For the Bayesian classifier with greedy recourse...

![](www/synthetic/bayesian.gif)

For the non-Bayesian classifier with generic recourse...

![](www/synthetic/plugin.gif)

```{julia}
using Main.Experiments: prepare_results, plot_results
validity = DataFrame()
cost = DataFrame()
files = (Bayesian=joinpath(output_path, "Bayesian.csv"), Plugin=joinpath(output_path, "Plugin.csv"))
results = map(file -> CSV.read(file, DataFrame), files)
p_val, p_cost = plot_results(results);
```

```{julia}
p_val
```

```{julia}
p_cost
```

## Neural network

```{julia}
using Flux
data = Models.prepare_data(X', y')
nn = Models.build_model(input_dim=size(X')[1], n_hidden=100)
loss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y);
```

Training a single neural network...

```{julia}
using BSON
run = false
opt = ADAM()
if run
  # Train model:
  using Flux.Optimise: update!, ADAM
  using Statistics, StatsBase
  epochs = 10
  avg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))
  accuracy(data) = sum(map(d ->round.(Flux.Ïƒ.(nn(d[1]))) .== d[2], data))[1]/length(data)

  using Plots
  anim = Animation()
  avg_l = [avg_loss(data)]
  p1 = scatter(ylim=(0,avg_l[1]), xlim=(0,epochs), legend=false, xlab="Epoch", title="Average loss")
  acc = [accuracy(data)]
  p2 = scatter(ylim=(0.5,1), xlim=(0,epochs), legend=false, xlab="Epoch", title="Accuracy")
  
  Ï„ = 1.1
  stopping_criterium_reached = accuracy(data) >= Ï„
  epoch = 1

  while epoch <= epochs && !stopping_criterium_reached
    for d in data
      gs = gradient(Flux.params(nn)) do
        l = loss(d...)
      end
      update!(opt, Flux.params(nn), gs)
    end
    avg_l = vcat(avg_l,avg_loss(data))
    plot!(p1, [0:epoch], avg_l, color=1)
    scatter!(p1, [0:epoch], avg_l, color=1)
    acc = vcat(acc,accuracy(data))
    plot!(p2, [0:epoch], acc, color=1)
    scatter!(p2, [0:epoch], acc, color=1)
    plt=plot(p1,p2, size=(600,300))
    frame(anim, plt)

    # Check if desired accuracy reached:
    stopping_criterium_reached = accuracy(data) >= Ï„

    epoch += 1

  end

  gif(anim, joinpath(www_path
, "single_nn.gif"), fps=10)

  BSON.@save joinpath(output_path
, "nn.bson") nn

end
```

```{julia}
using BSON: @load
@load joinpath(output_path, "nn.bson") nn
Mâ‚™â‚™ = Models.FittedNeuralNet(nn, opt, loss);
```

Training a deep ensemble...

```{julia}
opt = ADAM()
loss_type = :logitbinarycrossentropy
run = false
if run
    K = 50
    ensemble = Models.build_ensemble(K,kw=(input_dim=size(X')[1], n_hidden=100));
    ensemble, anim = Models.forward(ensemble, data, opt, n_epochs=10, plot_every=10, loss_type=loss_type, Ï„=1.1); # fit the ensemble
    Models.save_ensemble(ensemble, root=joinpath(output_path
, "ensemble")) # save to disk
    gif(anim, joinpath(www_path
, "ensemble_loss.gif"), fps=25);
end
```

```{julia}
ensemble = Models.load_ensemble(root=joinpath(output_path, "ensemble"))
M = Models.FittedEnsemble(ensemble, opt, loss_type)
models = (Bayesian=M, Plugin=Mâ‚™â‚™);
```

```{julia}
using Main.Experiments: plot_contour
p1 = plot_contour(X,y,models[1],title="Bayesian")
p2 = plot_contour(X,y,models[2],title="Plugin")
plot(p1,p2,size=(1000,400))
```

```{julia}
# Experiments:
experiment = Experiments.Experiment(X',y',models[:Bayesian],target,grid_,n_rounds)
experiment_plugin = Experiments.Experiment(X',y',models[:Plugin],target,grid_,n_rounds)
experiments = (experiment, experiment_plugin);
```

```{julia}
run = true
if run
    using Random
    Random.seed!(1234)
    for j in 1:length(generators)
        outcome, path = Experiments.run_experiment(experiments[j], generators[j], n_folds, T=T, store_path=true)
        Experiments.save_path(joinpath(output_path
    , string(keys(generators)[j])),path)
        CSV.write(joinpath(output_path
    , string(keys(generators)[j]) * "_deep.csv"), DataFrame(outcome))
    end
end
```

