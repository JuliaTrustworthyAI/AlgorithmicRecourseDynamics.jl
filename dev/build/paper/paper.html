<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-0.9.629">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Endogenous Macrodynamics in Algorithmic Recourse</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="paper_files/libs/clipboard/clipboard.min.js"></script>
<script src="paper_files/libs/quarto-html/quarto.js"></script>
<script src="paper_files/libs/quarto-html/popper.min.js"></script>
<script src="paper_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="paper_files/libs/quarto-html/anchor.min.js"></script>
<link href="paper_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="paper_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="paper_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="paper_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="paper_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Endogenous Macrodynamics in Algorithmic Recourse</h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  
<div>
  <div class="abstract">
    <div class="abstract-title">Abstract</div>
    <p>Existing work on Counterfactual Explanations (CE) and Algorithmic Recourse (AR) has largely been limited to the static setting: given some classifier we are interested in finding close, actionable, realistic, sparse, diverse and ideally causally founded counterfactuals. The ability of CE to handle dynamics like data and model drift remains a largely unexplored research challenge at this point. Only one recent work considers the implications of exogenous domain and model shifts. This project instead focuses on endogenous dynamics, that is shifts that occur when AR is actually implemented by a proportion of individuals. Early findings suggest that the involved shifts may be large with important implications on the validity of AR and the overall characteristics of the sample population.</p>
  </div>
</div>

</header>

<section id="sec-intro" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>Recent advances in Artificial Intelligence (AI) have propelled its adoption in scientific domains outside of Computer Science including Healthcare, Bioinformatics, Genetics and the Social Sciences. While this has in many cases brought benefits in terms of efficiency, state-of-the-art models like Deep Neural Networks (DNN) have also given rise a new type of principal-agent problem in the context of data-driven decision-making. It involves a group of <strong>principals</strong> - i.e.&nbsp;human stakeholders - that fail to understand the behaviour of their <strong>agent</strong> - i.e.&nbsp;the model used for automated decision-making <span class="citation" data-cites="borch2022machine"><a href="#ref-borch2022machine" role="doc-biblioref">[1]</a></span>.</p>
<p>Models or algorithms that fall into this category are typically referred to <strong>black-box</strong> models. Despite their shortcomings, black-box models have grown in popularity in recent years and have at times created undesirable societal outcomes <span class="citation" data-cites="o2016weapons"><a href="#ref-o2016weapons" role="doc-biblioref">[2]</a></span>. The scientific community has tackled this issue from two different angles: while some have appealed for a strict focus on inherently iterpretable models <span class="citation" data-cites="rudin2019stop"><a href="#ref-rudin2019stop" role="doc-biblioref">[3]</a></span>, others have investigated different ways to explain the behaviour of black-box models. These two sub-domains can be broadly referred to as <strong>interpretable AI</strong> and <strong>explainable AI</strong> (XAI), respectively.</p>
<p>Among the approaches to XAI that have recently grown in popularity are <strong>Counterfactual Explanations</strong> (CE). They explain how inputs into a model need to change for it to produce different outputs. Counterfactual Explanations that involve realistic and actionable changes can be used for the purpose of <strong>Algorithmic Recourse</strong> (AR) to help individuals who face adverse outcomes. An example relevant to the Social Sciences is consumer credit: in this context AR can be used to guide individuals in improving their creditworthiness, should they have previously been denied access to credit based on an automated decision-making system. A meaningful recourse recommendation for a denied applicant could be: <em>“If your net savings rate had been 10% of your monthly income instead of the actual 8%, your application would have been successful. See if you can temporarily cut down on consumption.”</em> In the remainder of this paper we will use both terminologies - recourse and counterfactual - interchangeably to refer to situations where counterfactuals are generated with the intent to provide individual recourse.</p>
<p>Existing work in this field has largely worked in a static setting: various approaches have been proposed to generate counterfactuals for a given individual that is subject to some pre-trained model. More recent work has compared different approaches within this static setting <span class="citation" data-cites="pawelczyk2021carla"><a href="#ref-pawelczyk2021carla" role="doc-biblioref">[4]</a></span>. In this work we go one step further and ask ourselves: what happens if recourse is provided and implemented repeatedly? What types of dynamics are introduced and how do different counterfactual generators compare in this context?</p>
<p>Research on algorithmic recourse has also so far typically addressed the issue from the perspective of one single individual and has indeed been referred to as <strong>individual recourse</strong> in some places. Arguably though, most real-world applications that warrant algorithmic recourse involve potentially large groups of individuals typically competing for scarce resources. Our work demonstrates that in such scenarios, choices made by or for one single individual are likely to affect the broader collective of individuals in ways that current approaches to AR fail to account for. More specifically, we argue that a strict focus on minimizing the private costs faced by individuals may be too narrow an objective.</p>
<p><a href="#fig-poc">Figure&nbsp;1</a> illustrates this idea for a binary problem involving a probabilistic classifier and the counterfactual generator proposed by <span class="citation" data-cites="wachter2017counterfactual"><a href="#ref-wachter2017counterfactual" role="doc-biblioref">[5]</a></span>: the implementation of AR for a subset of individuals leads to a domain shift (b), which in turn triggers a model shift (c). As this game of implementing AR and updating the classifier is repeated, the decision boundary moves away from training samples that were originally in the target class (d). We refer to these types of dynamics as <strong>endogenous</strong> because they are induced by the implementation of recourse itself. The term <strong>macrodynamics</strong> is borrowed from the economics literature and used to describe processes involving whole groups or societies.</p>
<div id="fig-poc" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="www/poc.png" class="img-fluid figure-img" data-fig.pos="h" style="width:45.0%"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 1: Dynamics in Algorithmic Recourse: we have a simple Bayesian model trained for binary classification (a); the implementation of AR for a random subset of individuals leads to a domain shift (b); as the classifier is retrained we observe a model shift (c); as this process is repeated, the decision boundary moves away from the target class (d).</figcaption><p></p>
</figure>
</div>
<p>We think that these types of endogenous dynamics may be problematic and warrant our attention. Firstly, model shifts may inadvertently change classification outcomes for individuals who never received and implemented recourse. Secondly and relatedly, we observe in <a href="#fig-poc">Figure&nbsp;1</a> that as the decision boundary moves in the direction of the non-target class, counterfactual paths become shorter: in the consumer credit example, individuals that previously would have been denied credit based on their input features are suddenly considered as creditworthy. Average default risk across all borrowers can therefore be expected to increase. Conversely, lenders that anticipate such dynamics may choose to refrain from offering recourse (and hence credit) to more than just a tiny share of individuals. In that latter and perhaps more likely scenario, the probability of being offered recourse decreases with every individual that implements recourse: in other words, the actions of first-movers exert a negative externality on future would-be borrowers.</p>
<p>To the best of our knowledge this is the first work investigating endogenous macrodynamics in AR. Our contributions to the state of knowledge are as follows: firstly, we posit a compelling argument that calls for a novel perspective on algorithmic recourse extending our focus from single individuals to groups. Secondly, we introduce an experimental framework extending previous work by <span class="citation" data-cites="altmeyer2022CounterfactualExplanations"><a href="#ref-altmeyer2022CounterfactualExplanations" role="doc-biblioref">[6]</a></span>, which enables us to study macrodynamics of algorithmic recourse through simulations that can be fully parallelized. Thirdly, we use this framework to provide a first in-depth analysis of endogenous recourse dynamics induced by various popular counterfactual generators including <span class="citation" data-cites="wachter2017counterfactual"><a href="#ref-wachter2017counterfactual" role="doc-biblioref">[5]</a></span>, <span class="citation" data-cites="schut2021generating"><a href="#ref-schut2021generating" role="doc-biblioref">[7]</a></span>, <span class="citation" data-cites="joshi2019towards"><a href="#ref-joshi2019towards" role="doc-biblioref">[8]</a></span>, <span class="citation" data-cites="mothilal2020explaining"><a href="#ref-mothilal2020explaining" role="doc-biblioref">[9]</a></span> and <span class="citation" data-cites="antoran2020getting"><a href="#ref-antoran2020getting" role="doc-biblioref">[10]</a></span>. To this end we propose a number of novel evaluation metrics that can be used to quantify and benchmark the macrodynamics introduced by the different generators. Finally, we also discuss what drives endogenous dynamics and propose strategies to mitigate them.</p>
<p>The remainder of the paper is structured as follows: <a href="#sec-related">Section&nbsp;2</a> places our work in the broader context of related literature. <a href="#sec-method">Section&nbsp;3</a> presents our methodology and data. <a href="#sec-empirical">Section&nbsp;5</a> presents our empirical findings which are then discussed in the broader context of the literature in <a href="#sec-discussion">Section&nbsp;7</a>. We also point to some of the limitations or our work as well as avenues for future research in <a href="#sec-limit">Section&nbsp;8</a>. Finally, <a href="#sec-conclusion">Section&nbsp;9</a> concludes.</p>
</section>
<section id="sec-related" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Background</h1>
<p>In this Section we provide a review of the relevant literature. First, <a href="#sec-related-recourse">Section&nbsp;2.1</a> discusses the existing research within the domain of counterfactual explanations and algorithmic recourse. Then, <a href="#sec-related-shifts">Section&nbsp;2.2</a> presents some of the previous work on the measurement of dataset and model shifts.</p>
<section id="sec-related-recourse" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="sec-related-recourse"><span class="header-section-number">2.1</span> Algorithmic Recourse</h2>
<p>A framework for Counterfactual Explanations was first proposed in 2017 by <span class="citation" data-cites="wachter2017counterfactual"><a href="#ref-wachter2017counterfactual" role="doc-biblioref">[5]</a></span> and has served as the baseline for most methodologies that have been proposed since then. Let <span class="math inline">\(M: \mathcal{X} \mapsto \mathcal{Y}\)</span> denote some pre-trained model that maps from inputs <span class="math inline">\(X \in \mathcal{X}\)</span> to outputs <span class="math inline">\(Y \in \mathcal{Y}\)</span>. Then we are interested in minimizing the complexity or effort <span class="math inline">\(H=h(x\prime)\)</span> associated with moving an individual <span class="math inline">\(x\)</span> to a counterfactual state <span class="math inline">\(x\prime\)</span> such that the predicted outcome <span class="math inline">\(M(x\prime)\)</span> corresponds to some target outcome <span class="math inline">\(t\)</span>:</p>
<p><span id="eq-obj"><span class="math display">\[
\min_{x\prime \in \mathcal{x}} c(x\prime) \ \ \ \mbox{s. t.} \ \ \ M(x\prime) = t
\tag{1}\]</span></span></p>
<p>For implementation purposes, <a href="#eq-obj">Equation&nbsp;1</a> is typically approximated through regularization:</p>
<p><span id="eq-solution"><span class="math display">\[
x\prime = \arg \min_{x\prime}  \ell(M(x\prime),t) + \lambda h(x\prime)
\tag{2}\]</span></span></p>
<p>In the baseline work and many subsequent approaches the complexity function <span class="math inline">\(h: \mathcal{X} \mapsto \mathbb{R}\)</span> is proxied by some distance metric based on the simple intuition that large perturbations of <span class="math inline">\(x\)</span> are costly.</p>
<p>Many approaches for the generation of algorithmic recourse have been described in the literature. An October 2020 survey by Karimi et al.&nbsp;layed out 60 algorithms that have been proposed since 2014 <span class="citation" data-cites="karimi2020survey"><a href="#ref-karimi2020survey" role="doc-biblioref">[11]</a></span>. Another survey published around the same time by Verma et al.&nbsp;described 29 algorithms <span class="citation" data-cites="verma2020counterfactual"><a href="#ref-verma2020counterfactual" role="doc-biblioref">[12]</a></span>. Different approaches vary primarily in terms of the objective functions they impose, how they optimize said objective (from brute force through gradient-based approaches to graph traversal algorithms), and how the ensure that certain requirements for CE are met. Regarding the latter, the literature has produced an extensive list of desiderata each addressing different needs. To name but a few, we are interested in generating counterfactuals that close <span class="citation" data-cites="wachter2017counterfactual"><a href="#ref-wachter2017counterfactual" role="doc-biblioref">[5]</a></span>, actionable <span class="citation" data-cites="ustun2019actionable"><a href="#ref-ustun2019actionable" role="doc-biblioref">[13]</a></span>, realistic <span class="citation" data-cites="joshi2019towards"><a href="#ref-schut2021generating" role="doc-biblioref">[7]</a></span>, sparse, diverse <span class="citation" data-cites="mothilal2020explaining"><a href="#ref-mothilal2020explaining" role="doc-biblioref">[9]</a></span> and if possible causally founded <span class="citation" data-cites="karimi2021algorithmic"><a href="#ref-karimi2021algorithmic" role="doc-biblioref">[14]</a></span>.</p>
<p>Efforts so far have largely been directed at improving the quality of counterfactual explanations within a static context: given some pre-trained classifier <span class="math inline">\(M: \mathcal{X} \mapsto \mathcal{Y}\)</span> we are interested in generating one or multiple meaningful counterfactual explanations for some individual characterized by <span class="math inline">\(x_i\)</span>. The ability of counterfactual explanations to handle dynamics like data and model shifts remains a largely unexplored research challenge at this point <span class="citation" data-cites="verma2020counterfactual"><a href="#ref-verma2020counterfactual" role="doc-biblioref">[12]</a></span>. We have been able to identify only one recent work that considers the implications of <strong>exogenous</strong> domain and model shifts in the context of AR <span class="citation" data-cites="upadhyay2021towards"><a href="#ref-upadhyay2021towards" role="doc-biblioref">[15]</a></span>. Exogenous shifts are strictly of external origin. For example, they might stem from data correction, temporal shifts or geospatial changes <span class="citation" data-cites="upadhyay2021towards"><a href="#ref-upadhyay2021towards" role="doc-biblioref">[15]</a></span>. The authors of <span class="citation" data-cites="upadhyay2021towards"><a href="#ref-upadhyay2021towards" role="doc-biblioref">[15]</a></span> propose framework for algorithmic recourse (ROAR) that evidently improves robustness to such exogenous shifts.</p>
<p>As mentioned earlier, research has so far also generally focused on generating counterfactuals for single individuals or instances. We have been able to identify only one existing work that investigates black-box model behavior towards a group of individuals <span class="citation" data-cites="carrizosa2021generating"><a href="#ref-carrizosa2021generating" role="doc-biblioref">[16]</a></span>. The authors propose an optimization framework that generates collective counterfactuals. We provide a motivation for doing so from the perspective of endogenous macrodynamics of algorithmic recourse.</p>
</section>
<section id="sec-related-shifts" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="sec-related-shifts"><span class="header-section-number">2.2</span> Domain and Model Shifts</h2>
<p>Much attention has been paid to the detection of dataset shifts – situations where the distribution of data changes over time. Rabanser et al.&nbsp;suggest a framework to detect data drift from a minimal number of samples through the application of two-sample tests <span class="citation" data-cites="rabanser2019failing"><a href="#ref-rabanser2019failing" role="doc-biblioref">[17]</a></span>. This task is a generalization of the anomaly detection problem for large datasets, which aims to answer the question if two sets of samples could have been generated from the same probability distribution. Numerous approaches to anomaly detection have been summarized <span class="citation" data-cites="chandola2009anomaly"><a href="#ref-chandola2009anomaly" role="doc-biblioref">[18]</a></span>. Another well-established research topic is that of concept drift – situations where external variables influence the patterns between the input and the output of a model <span class="citation" data-cites="widmer1996learning"><a href="#ref-widmer1996learning" role="doc-biblioref">[19]</a></span>. For instance, Gama et al.&nbsp;offer a review of the adaptive learning techniques which can handle concept drift <span class="citation" data-cites="gama2014survey"><a href="#ref-gama2014survey" role="doc-biblioref">[20]</a></span>. Less previous work is available on the related topic of model drift - changes in model performance over time. Nelson et al.&nbsp;review how resistant different machine learning models are to the model drift <span class="citation" data-cites="nelson2015evaluating"><a href="#ref-nelson2015evaluating" role="doc-biblioref">[21]</a></span>. Ackerman et al.&nbsp;offer a method to detect changes in model performance when ground truth is not available <span class="citation" data-cites="ackerman2021machine"><a href="#ref-ackerman2021machine" role="doc-biblioref">[22]</a></span>.</p>
<p>In the context of algorithmic recourse, domain and model shifts were first brought up by the authors behind ROAR <span class="citation" data-cites="upadhyay2021towards"><a href="#ref-upadhyay2021towards" role="doc-biblioref">[15]</a></span>. In their work they refer to model shifts as simply any perturbation <span class="math inline">\(\Delta\)</span> to the parameters of the model in question: <span class="math inline">\(M\)</span>. While this also sets the baseline for our analysis here, it is worth noting that in <span class="citation" data-cites="upadhyay2021towards"><a href="#ref-upadhyay2021towards" role="doc-biblioref">[15]</a></span> these perturbations are mechanically introduced. In contrast we are interested in quantifying model shifts that arise endogenously as part of a dynamic recourse process. In addition to quantifying the magnitude of shifts <span class="math inline">\(\Delta\)</span>, we aim to also analyse the characteristics of changes to the model, such as the position of the decision boundary and the overall decisiveness of the model. We have not been able to identify previous work on this topic.</p>
</section>
<section id="sec-related-benchmark" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="sec-related-benchmark"><span class="header-section-number">2.3</span> Benchmarking Counterfactual Generators</h2>
<p>Despite the large and growing number of approaches to counterfactual search there have been surprisingly few benchmark studies that compare different methodologies. This may be partially due to limited software availability in this space. Recent work has started to address this gap: firstly, <span class="citation" data-cites="de2021framework"><a href="#ref-de2021framework" role="doc-biblioref">[23]</a></span> run a large benchmarking study using different algorithmic aproaches and numerous tabular datasets; secondly, <span class="citation" data-cites="pawelczyk2021carla"><a href="#ref-pawelczyk2021carla" role="doc-biblioref">[4]</a></span> introduce a Python framework that can be used to apply and benchmark different methodologies; finally, <span class="citation" data-cites="altmeyer2022CounterfactualExplanations"><a href="#ref-altmeyer2022CounterfactualExplanations" role="doc-biblioref">[6]</a></span> provides an extensible, fast and language-agnostic implementation in Julia. Since the experiments presented here involve extensive simulations, we have relied on and extended the Julia implementation due to the associated performance benefits.</p>
</section>
</section>
<section id="sec-method" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Gradient-Based Recourse Revisited</h1>
<p>In the following we first set out a generalized framework for gradient-based counterfactual search in <a href="#sec-method-general">Section&nbsp;3.1</a> to introduce the various counterfactual generators we have chosen to use in our experiments. We then describe the experimental setup in <a href="#sec-method-2-experiment">Section&nbsp;4.1</a> and introduce several evaluation metrics used to benchmark the different generators.</p>
<section id="sec-method-general" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="sec-method-general"><span class="header-section-number">3.1</span> From individual recourse …</h2>
<p>In this work we have chosen to focus on a number of gradient-based counterfactual generators to investigate the endogenous dynamics we introduced in <a href="#sec-intro">Section&nbsp;1</a>. Gradient-based counterfactual search is well-suited for differentiable black-box models like deep neural networks. We can restate <a href="#eq-solution">Equation&nbsp;2</a> in a more general form that encompasses most gradient-based approaches to counterfactual search:</p>
<p><span id="eq-general"><span class="math display">\[
\begin{aligned}
\mathbf{s}^\prime &amp;= \arg \min_{\mathbf{s}^\prime \in \mathcal{S}} \left\{ \sum_{k=1}^{K} {\ell(M(f(s_k^\prime)),t)}+ \lambda {h(f(s_k^\prime)) }  \right\}
\end{aligned}
\tag{3}\]</span></span></p>
<p>Here <span class="math inline">\(\mathbf{s}^\prime=\left\{s_k^\prime\right\}_K\)</span> is the stacked <span class="math inline">\(K\)</span>-dimensional array of counterfactual states and <span class="math inline">\(f: \mathcal{S} \mapsto \mathcal{X}\)</span> maps from the counterfactual state space to the feature space.</p>
</section>
<section id="towards-collective-recourse" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="towards-collective-recourse"><span class="header-section-number">3.2</span> … towards collective recourse</h2>
<p>To explicitly address the notion that individual recourse may affect the outcome and prospect of other individuals, we propose to extend <a href="#eq-general">Equation&nbsp;3</a> as follows:</p>
<p><span id="eq-collective"><span class="math display">\[
\begin{aligned}
\mathbf{s}^\prime &amp;= \arg \min_{\mathbf{s}^\prime \in \mathcal{S}}  \sum_{k=1}^{K} {\ell(M(f(s_k^\prime)),t)} \\ &amp;+ \lambda_1 {h(f(s_k^\prime)) } + \lambda_2 {g(f(s_k^\prime))}  
\end{aligned}
\tag{4}\]</span></span></p>
<p>Here <span class="math inline">\(h(f(s_k^\prime))\)</span> denotes the proxy for private costs faced by the individual as before and <span class="math inline">\(\lambda_1\)</span> governs to what extent that private cost ought to be penalised. The newly introduced term <span class="math inline">\(g(f(s_k^\prime))\)</span> is meant to capture and address social costs incurred by the collective of individuals in response to changes in <span class="math inline">\(\mathbf{s}^\prime\)</span>. The trade-off between private and social costs is determined by the ratio between <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span>. As with individual recourse, the exact choice of <span class="math inline">\(g(\cdot)\)</span> is not obvious, nor do we intend to provide a definite answer in this work, if such even exists. A straight-forward choice simply extends the baseline approach by <span class="citation" data-cites="wachter2017counterfactual"><a href="#ref-wachter2017counterfactual" role="doc-biblioref">[5]</a></span>: instead of only penalizing the distance of the individuals’ counterfactual to its factual, we propose penalizing its distance to some sensible point in the target domain, for example the sample average: <span class="math inline">\(\bar{\mathbf{x}}\)</span>. For such a recourse objective, higher choices of <span class="math inline">\(\lambda_2\)</span> relative to <span class="math inline">\(\lambda_1\)</span> will lead counterfactuals to gravitate towards the specified point in the target domain. In the remainder of this paper we will therefore refer to this approach as <strong>Gravitational</strong> generator, when we investigate its potential usefulness for mitigating endongenous macrodynamics<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
</section>
<section id="a-note-on-convergence" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="a-note-on-convergence"><span class="header-section-number">3.3</span> A note on convergence</h2>
<p>For this simple mitigating strategy underlying the Gravitational generator to work as expected, one needs to ensure that counterfactual search continues, even after a predetermined threshold probability <span class="math inline">\(\gamma\)</span> has potentially already been reached. <a href="#fig-convergence">Figure&nbsp;2</a> illustrates this distinction: if one chooses to terminate search once the desired threshold is reached (left panel) the gravitational pull towards <span class="math inline">\(\bar{\mathbf{x}}\)</span> is never actually satisfied (compare to right panel). More generally, if convergence is defined simply in terms of flipping the predicted label with some desired degree of confidence, this corresponds to essentially ignoring any parts of the counterfactual search objective that do not involve <span class="math inline">\(\ell(M(f(s_k^\prime)),t)\)</span> beyond that point. While this may be appropriate for some applications, in general this seems like an odd convention. Since we nonetheless seen convergence specified simply in terms of reaching the threshold probability in some places<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, we thought it worth making this distinction explicit.</p>
<div id="fig-convergence" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="www/gravitational_generator_comparison.png" class="img-fluid figure-img" data-fig.pos="h" style="width:45.0%"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 2: Comparison of counterfactual search outcome with simple (left) and strict convergence (right).</figcaption><p></p>
</figure>
</div>
</section>
</section>
<section id="sec-method-2" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Modeling Endogenous Macrodynamics in Algorithmic Recourse</h1>
<p>In this …</p>
<section id="sec-method-2-experiment" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="sec-method-2-experiment"><span class="header-section-number">4.1</span> Simulations</h2>
<p>The dynamics illustrated in <a href="#fig-poc">Figure&nbsp;1</a> in <a href="#sec-intro">Section&nbsp;1</a> were generated through a simple experiment that aims to simulate the process of algorithmic recourse in practice. We begin in the static setting at time time <span class="math inline">\(t=0\)</span>: given some pre-trained classifier <span class="math inline">\(M\)</span> we generate recourse for a random batch of <span class="math inline">\(B\)</span> individuals in the non-target class. Note that we focus our attention on classification problems, since classification poses the most common practical use-case for algorithmic recourse. In order to simulate the dynamical process we suppose that the model <span class="math inline">\(M\)</span> is retrained following the actual implementation of recourse in time <span class="math inline">\(t=0\)</span>. Following the update to the model, we assume that at time <span class="math inline">\(t=1\)</span> recourse is generated for yet another random subset of individuals in the non-target class. This process is repeated for a number of time periods <span class="math inline">\(T\)</span>. To get a clean read on endogenous dynamics we keep the total population of samples closed: we allow existing samples to move from factual to counterfactual states, but do not allow any entirely new samples to enter the population. The experimental setup is summarized in Algorithm <span class="math inline">\(\ref{algo-experiment}\)</span></p>
<p>A noteworthy practical consideration is the choice of <span class="math inline">\(T\)</span> and <span class="math inline">\(B\)</span>. The higher these values, the more factual instances undergo recourse throughout the entire experiment. Of course, this is likely to lead to more pronounced domain and model shifts by time <span class="math inline">\(T\)</span>. At the same time, it is generally improbable that a very large part of the population would request an explanation of the algorithm’s decisions. In our experiments, we choose the values such that <span class="math inline">\(T \cdot B\)</span> corresponds to the application of recourse on <span class="math inline">\(25-50\%\)</span> of the negative instances from the initial dataset. As we collect data at each time <span class="math inline">\(t\)</span>, we can also verify the impact of recourse when it is implemented for a smaller number of individuals. Using our framework the experiment can be conducted on an arbitrary number of algorithmic recourse generators. As all generators make use of the same initial model and initial dataset, the differences in domain and model shifts observed throughout the rounds depend solely on the employed generator.</p>
</section>
<section id="sec-method-2-metrics" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="sec-method-2-metrics"><span class="header-section-number">4.2</span> Evaluation Metrics</h2>
<p>We formulate two desiderata for the set of metrics used to measure domain and model shifts induced by recourse. First, the metrics should be applicable regardless of the dataset or classification technique so that they allow for the meaningful comparison of the generators in various scenarios. As the knowledge of the underlying probability distribution is rarely available, the metrics should be empirical and non-parametric. This further ensures that we can also measure large datasets by sampling from the available data. Moreover, while our study was conducted in a two-class classification setting, our choice of metrics should remain applicable in the future research on multi- class recourse problems. Second, the set of metrics should allow to capture various aspects of the previously mentioned magnitude, path, and tempo of changes while remaining as small as possible.</p>
<section id="domain-shifts" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="domain-shifts"><span class="header-section-number">4.2.1</span> Domain Shifts</h3>
<p>To quantify the magnitude of domain shifts we rely on an unbiased estimate of the squared population <strong>Maximum Mean Discrepancy (MMD)</strong> given as:</p>
<p><span id="eq-mmd"><span class="math display">\[
\begin{aligned}
MMD^2_u[F,{X}^\prime,\tilde{X}^\prime] &amp;= \frac{1}{m(m-1)}\sum_{i=1}^m\sum_{j\neq i}^m k(x_i,x_j) \\ &amp;+ \frac{1}{n(n-1)}\sum_{i=1}^n\sum_{j\neq i}^n k(\tilde{x}_i,\tilde{x}_j) \\ &amp;- \frac{2}{mn}\sum_{i=1}^m\sum_{j=1}^n k(x_i,\tilde{x}_j)
\end{aligned}
\tag{5}\]</span></span></p>
<p>where <span class="math inline">\(\mathcal{F}\)</span> is a unit ball in a Reproducing Kernel Hilbert Space H [27], and <span class="math inline">\(X\)</span>, <span class="math inline">\(\tilde{X}\)</span> represent independent and identically distributed samples drawn from probability distributions <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> respectively [28]. MMD is a measure of the distance between the kernel mean embeddings of <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> in RKHS <span class="math inline">\(\mathcal{H}\)</span>. An important consideration is the choice of the kernel function <span class="math inline">\(k(\cdot,\cdot)\)</span>. In our implementation we make use of the radial basis function (RBF) kernel with a constant length-scale parameter of <span class="math inline">\(0.5\)</span>. As RBF captures all moments of distributions <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>, we have that <span class="math inline">\(MMD_u^2[F,X,\tilde{X}]=0\)</span> if and only if <span class="math inline">\(X=\tilde{X}\)</span>.</p>
<p>The evaluation metric in <a href="#eq-mmd">Equation&nbsp;5</a> is computed after every round <span class="math inline">\(t=1,...,T\)</span> of the experiment. To assess the statistical significance of the observed shifts under the null hypothesis that samples <span class="math inline">\(X\)</span> and <span class="math inline">\(\tilde{X}\)</span> were drawn from the same probability distribution we follow <span class="citation" data-cites="arcones1992bootstrap"><a href="#ref-arcones1992bootstrap" role="doc-biblioref">[24]</a></span>. To that end, we combine the two samples and generate a large number of permutations of <span class="math inline">\(X + \tilde{X}\)</span>. Then, we split the permuted data into two new samples <span class="math inline">\(X^\prime\)</span> and <span class="math inline">\(\tilde{X}^\prime\)</span> having the same size as the original samples. Then under the null hypothesis we should have that <span class="math inline">\(MMD_u^2[F,X^\prime,\tilde{X}^\prime]\)</span> be approximately equal to <span class="math inline">\(MMD_u^2[F,X,\tilde{X}]\)</span>. The corresponding <span class="math inline">\(p\)</span>-value can then be calculated by counting how these two quantities are not equal.</p>
<p>We calculate the MMD for both classes individually based on the ground truth labels, i.e.&nbsp;the labels that samples were assigned in time <span class="math inline">\(t=0\)</span>. Throughout our experiments we generally do not expect the distribution of the negative class to change over time – application of recourse reduces the size of this class, but since individuals are sampled uniformly the distribution should remain unaffected. Conversely, unless a recourse generator can perfectly replicate the original probability distribution, we expect the MMD of the positive class to increase. Thus, when discussing MMD, we generally mean the shift in the distribution of the positive class.</p>
<p>Finally, <strong>feature mean and feature standard deviation</strong> are also calculated to verify how the implementation of recourse impacts every attribute in the dataset. Although MMD already captures information about the expected value and variance, we may also be interested in a more granular look at individual features.</p>
</section>
<section id="model-shifts" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="model-shifts"><span class="header-section-number">4.2.2</span> Model Shifts</h3>
<p>As our baseline for quantifying model shifts we measure perturbations to the model parameters at each point in time <span class="math inline">\(t\)</span> following <span class="citation" data-cites="upadhyay2021towards"><a href="#ref-upadhyay2021towards" role="doc-biblioref">[15]</a></span>. We define <span class="math inline">\(\Delta=||\theta_{t+1}-\theta_{t}||^2\)</span>, that is the euclidean distance between the vectors of parameters before and after retraining the model <span class="math inline">\(M\)</span>. We shall refer to this baseline metric simply as <strong>Perturbations</strong>.</p>
<p>We extend the metric in <a href="#eq-mmd">Equation&nbsp;5</a> for the purpose of quantifying model shifts. Specifically, we introduce <strong>Predicted Probability MMD (PP MMD)</strong>: instead of applying <a href="#eq-mmd">Equation&nbsp;5</a> to features directly, we apply it to the predicted probabilities assigned to a set of samples by the model <span class="math inline">\(M\)</span>. If the model shifts, the probabilities assigned to each sample will change; again, this metric will equal 0 only if the two classifiers are the same. It is worth noting that while we apply the technique to samples drawn uniformly from the dataset, it can also be employed on arbitrary points in the entire feature space (or a subspace). The latter approach is theoretically more robust. Unfortunately, in practice this approachs suffers from the curse of dimensionality, since it becomes increasingly difficult to select enough points to overcome noise as the dimension <span class="math inline">\(D\)</span> grows.</p>
<p>As an alternative to PP MMD we use a pseudo-distance for the <strong>Disagreement Coefficient</strong> (Disagreement). This metric was introduced in <span class="citation" data-cites="hanneke2007bound"><a href="#ref-hanneke2007bound" role="doc-biblioref">[25]</a></span> and estimates <span class="math inline">\(p(M(x) \neq M^\prime(x))\)</span>, that is the probability that two classifiers do not agree on the predicted outcome for a randomly chosen sample. Thus, it is not relevant whether the classification is correct according to the ground truth, but only whether the sample lies on the same side of the two respective decision boundaries. In our context, this metric quantifies the overlap between the initial model (trained before the application of recourse) and the updated model. A Disagreement Coefficient unequal to zero is indicative of a model shift. The opposite is not true: even if the Disagreement Coefficient is equal to zero a model shift may still have occured. This is one reason for why PP MMD is our our preferred metric.</p>
<p>Finally, we introduce <strong>Decisiveness</strong> as a metric that quantifies the likelihood that a model assigns a high probability to its classification of any given sample. We define the metric simply as <span class="math inline">\({1\over{N}}\sum_{i=0}^N(\sigma(M(x)) − 0.5)^2\)</span> where <span class="math inline">\(M(x)\)</span> are predicted logits from a binary classifier and <span class="math inline">\(\sigma\)</span> denotes the sigmoid function. This metric provides an unbiased estimate of the binary classifier’s tendency to produce high-confidence predictions in either one of the two classes. Although the exact values for this metric are not important for our study, they can be used to detect model shifts. If decisiveness changes over time, then this is indicative of the decision boundary moves towards either one of the two classes.</p>
</section>
</section>
</section>
<section id="sec-empirical" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Experiment Setup</h1>
<section id="sec-empirical-recourse" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="sec-empirical-recourse"><span class="header-section-number">5.1</span> Recourse Generators</h2>
<p>In the case of the baseline counterfactual generator <span class="citation" data-cites="wachter2017counterfactual"><a href="#ref-wachter2017counterfactual" role="doc-biblioref">[5]</a></span> <span class="math inline">\(f\)</span> is just the idendity function and the number of counterfactuals <span class="math inline">\(K\)</span> is equal to one. This generator, which we shall refer to as <strong>Wachter</strong> in the following, shall serve as the baseline against which all other gradient-based methodologies will be compared. In particular, we include include the following generator in our benchmarking exercises: REVISE <span class="citation" data-cites="joshi2019towards"><a href="#ref-joshi2019towards" role="doc-biblioref">[8]</a></span>, CLUE <span class="citation" data-cites="antoran2020getting"><a href="#ref-antoran2020getting" role="doc-biblioref">[10]</a></span>, DICE <span class="citation" data-cites="mothilal2020explaining"><a href="#ref-mothilal2020explaining" role="doc-biblioref">[9]</a></span> and a greedy approach that relies on probabilistic models <span class="citation" data-cites="schut2021generating"><a href="#ref-schut2021generating" role="doc-biblioref">[7]</a></span>.</p>
<p>Both <strong>REVISE</strong> and <strong>CLUE</strong> search counterfactuals in some latent embedding <span class="math inline">\(S \subset \mathcal{S}\)</span> instead of the feature space directly. The latent embedding is learned by a separate generative model that is tasked with learning the data generating process (DGP) of <span class="math inline">\(X\)</span>. In this case <span class="math inline">\(f\)</span> in <a href="#eq-general">Equation&nbsp;3</a> corresponds to the decoder part of the generative model, in other words the deterministic function that maps back from the latent embedding to the feature space. Provided the generative model is well-specified, traversing the latent embedding typically results in realistic and plausible counterfactuals, because they are implicitly generated by the (learned) DGP <span class="citation" data-cites="joshi2019towards"><a href="#ref-joshi2019towards" role="doc-biblioref">[8]</a></span>. CLUE distinguishes itself from REVISE and other counterfactual generators in that it aims to minimize the predictive uncertainty of the model in question <span class="math inline">\(M\)</span>. To quantify predictive uncertainty the authors rely on entropy estimates for probabilistic models. The <strong>Greedy</strong> approach proposed by <span class="citation" data-cites="schut2021generating"><a href="#ref-schut2021generating" role="doc-biblioref">[7]</a></span> also works with the subclass of models <span class="math inline">\(\tilde{\mathcal{M}}\subset\mathcal{M}\)</span> that can produce predictive uncertainty estimates. The authors show that in this setting the complexity penalty <span class="math inline">\(h(\cdot)\)</span> in <a href="#eq-general">Equation&nbsp;3</a> is redundant and meaningful counterfactuals can be generated in a fast and efficient manner through a modified Jacobian-based Saliency Map Attack (JSMA). Finally, <strong>DICE</strong> distinguishes itself from all other generators considered here in that it aims to generate a diverse set of <span class="math inline">\(K&gt;1\)</span> counterfactuals. To this end the authors use a complexity penalty <span class="math inline">\(h(\mathbf{s}^\prime)\)</span> that favours diverse outcomes, in the sense that <span class="math inline">\(s_1, ... , s_K\)</span> look as different from each other as possible.</p>
<p>Our motivation for including these different generators in our analysis, is that they all offer slightly different approaches to generate meaningful counterfactuals for differentiable black-box models. We hypothesize that generating more <strong>meaningful</strong> counterfactuals should mitigate the endogenous dynamics illustrated in <a href="#fig-poc">Figure&nbsp;1</a> in <a href="#sec-intro">Section&nbsp;1</a>. This intuition stems from the underlying idea that more meaningful counterfactuals are generated by the same or at least a very similar data generating process as the training data. All else equal, counterfactuals that fulfill this basic requirement should be less prone to trigger domain and model shifts.</p>
</section>
<section id="sec-empirical-data" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="sec-empirical-data"><span class="header-section-number">5.2</span> Data</h2>
<p>We have chosen to work with both synthetic and real-world datasets. Using synthetic data allows us to impose distributional properties that may affect the resulting recourse dynamics. Following <span class="citation" data-cites="upadhyay2021towards"><a href="#ref-upadhyay2021towards" role="doc-biblioref">[15]</a></span> we generate synthetic data in <span class="math inline">\(\mathbb{R}^2\)</span> to also allow for a visual interpretation of the results. Real-world data is used in order to assess if endogenous dynamics also occur in higher-dimensional settings.</p>
<section id="synthetic-data" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="synthetic-data"><span class="header-section-number">5.2.1</span> Synthetic data</h3>
<p>We use four synthetic binary classification datasets consisting of 1000 samples each.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> The datasets are presented in <a href="#fig-synthetic-data">Figure&nbsp;3</a> (see also Appendix A for a formal description). Samples from the negative class are marked in blue while samples of the positive class are marked in orange.</p>
<div id="fig-synthetic-data" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="www/synthetic_data.png" data-fig.pos="h" style="width:8cm;height:2cm" class="figure-img"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 3: Synthetic classification datasets used in our experiments.</figcaption><p></p>
</figure>
</div>
<p>Ex-ante we expect to see that Wachter will create a new cluster of counterfactual instances in the proximity of the initial decision boundary. Thus, the choice of a black-box model may have an impact on the paths of the recourse. For generators that use latent space search (<span class="citation" data-cites="joshi2019towards"><a href="#ref-joshi2019towards" role="doc-biblioref">[8]</a></span>, <span class="citation" data-cites="antoran2020getting"><a href="#ref-antoran2020getting" role="doc-biblioref">[10]</a></span>) or rely on (and have access to) probabilistic models (<span class="citation" data-cites="antoran2020getting"><a href="#ref-antoran2020getting" role="doc-biblioref">[10]</a></span>, <span class="citation" data-cites="schut2021generating"><a href="#ref-schut2021generating" role="doc-biblioref">[7]</a></span>) we expect that counterfactuals will end up in regions of the target domain that are densely populated by training samples. Finally, we expect to see the counterfactuals generated by DiCE to be uniformly spread around the feature space inside the target class.</p>
</section>
<section id="real-world-data" class="level3" data-number="5.2.2">
<h3 data-number="5.2.2" class="anchored" data-anchor-id="real-world-data"><span class="header-section-number">5.2.2</span> Real-world data</h3>
<p>Additionally, we use two real-world datasets from the Finance domain. Firstly, we use the Give Me Some Credit dataset which was open-sourced on Kaggle for the task to predict whether a borrower is likely to experience financial difficulties in the next two years <span class="citation" data-cites="gmsc_data"><a href="#ref-gmsc_data" role="doc-biblioref">[26]</a></span>. Originally consisting of 250,000 instances with 11 numerical attributes, the dataset was randomly undersampled to result in a balanced subsample made up of 3000 individuals. Secondly, we the German Credit dataset which involves the task of predicting if bank customers are credit-worthy or not <span class="citation" data-cites="germancredit1994"><a href="#ref-germancredit1994" role="doc-biblioref">[27]</a></span>. It consists of 700 positive and 300 negative instances charaterized by 7 numerical and 13 categorical attributes. We process the dataset in two ways: (1) the values of the “Personal status and sex” feature are aggregated by the two represented genders; (2) the most common values are calculated for all categorical features such that a feature <span class="math inline">\(x_d\)</span> with the mode <span class="math inline">\(\bar{x}_d\)</span> is transformed into a new binary feature <span class="math inline">\(\tilde{x}_d=\mathbb{I}_{x_{d,i}&gt;=\bar{x}_d}\)</span>. Binarization ensures that we can use all counterfactual generators in the benchmark.</p>
</section>
</section>
</section>
<section id="sec-empirical-2" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Experiments</h1>
<section id="endogenous-macrodynamics" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="endogenous-macrodynamics"><span class="header-section-number">6.1</span> Endogenous Macrodynamics</h2>
</section>
<section id="potential-mitigation-strategies" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="potential-mitigation-strategies"><span class="header-section-number">6.2</span> Potential Mitigation Strategies</h2>
</section>
</section>
<section id="sec-discussion" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Discussion</h1>
<ol type="1">
<li>Shift of focus from individual to group of individuals (related: https://www.researchgate.net/publication/353073138_Generating_Collective_Counterfactual_Explanations_in_Score-Based_Classification_via_Mathematical_Optimization)</li>
<li>Convergence criterium matters: terminating once threshold probability is reached may not be optimal (see e.g.&nbsp;REVISE)</li>
<li>Optimizer choice matters: dimensionality is typically low, so no obvious benefit to using ADAM.
<ul>
<li>This might be better placed in JuliaCon proceedings, perhaps backed by small blog post on the matter.</li>
</ul></li>
<li>Mitigating strategy: penaliye distance from centroid.</li>
</ol>
</section>
<section id="sec-limit" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Limitations and Future Work</h1>
<p>While we believe that this work constitutes a valuable starting point for addressing existing issues in algorithmic recourse from a fresh perspective, we are aware of several of its limitations. In the following we highlight some of these limitations and point to avenues for future research.</p>
<section id="experimental-setup" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="experimental-setup"><span class="header-section-number">8.1</span> Experimental Setup</h2>
<p>The experimental setup proposed here is designed to mimic a real-world recourse process in a simple fashion. In practice, models are in fact updated on a regular basis <span class="citation" data-cites="upadhyay2021towards"><a href="#ref-upadhyay2021towards" role="doc-biblioref">[15]</a></span>. We also find it plausible to assume that the implementation of recourse happens periodically for different individuals, rather that all at once at time <span class="math inline">\(t=0\)</span>. That being said, our experimental design is a vast over-simplification of potential real-world scenarios. In practice, any endogenous shifts that may occur can be expected to be entangled with exogenous shifts of the nature investigated in <span class="citation" data-cites="upadhyay2021towards"><a href="#ref-upadhyay2021towards" role="doc-biblioref">[15]</a></span>. We also make implicit assumptions about the utility functions of the involved agents that may well be too simple: individuals seeking recourse are assumed to always implement the proposed counterfactual explanations; conversely, the agent in charge of the model <span class="math inline">\(M\)</span> is assumed to always treat individuals that have implemented valid recourse as if they were truly now in the target class. Relating this back to the consumer credit example, we assume that the would-be borrowers are always willing and able to implement recourse and the bank is always willing to provide credit as would-be borrowers move across the decision boundary. In practice it is doubtful that agents behave according to such simple rules. Nonetheless, we think that our simple framework offers a starting point for future work on recourse dynamics (both endogenous and exogenous dynamics).</p>
</section>
<section id="data" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="data"><span class="header-section-number">8.2</span> Data</h2>
<p>Largely in line with the existing literature on algorithmic recourse, we have limited our analysis of real-world data to three commonly used benchmark datasets that involve binary prediction tasks. Future work may benefit from including novel datasets or extending the analysis to multi-class or regression problems, the latter arguably representing the most common objective in Finance and Economics. It is also worth mentioning that the use of real-world datasets considered in this work is constrained by the fact that at the time of writing <code>CounterfactualExplanations.jl</code> only supports continuous features, at least of some of the counterfactual generators considered here. The fact that we therefore had to discard discrete features led to relatively poor initial performance of our classifiers in some cases. While this is indeed a limitation we intend to address in future and derivative work, our findings with respect to endogenous macrodynamics do not hinge on strong classifier performance.</p>
</section>
<section id="classifiers" class="level2" data-number="8.3">
<h2 data-number="8.3" class="anchored" data-anchor-id="classifiers"><span class="header-section-number">8.3</span> Classifiers</h2>
<p>For reasons stated earlier we have limited our analysis to differentiable linear and non-linear classifiers, in particular logistic regression and deep neural networks. While these sorts of classifiers have also typically been analyzed in the existing literature on counterfactual explanations and algorithmic recourse, they represent only a subset of popular machine learning models employed in practice - both black-box and glass-box. Despite the success and popularity of deep learning in the context of high-dimensional data such as image, audio and video, empirical evidence suggests that other models such as boosted decision trees may have an edge when it comes to lower-dimensional tabular datasets, such as the ones considered here <span class="citation" data-cites="borisov2021deep"><a href="#ref-grinsztajn2022tree" role="doc-biblioref">[29]</a></span>.</p>
</section>
</section>
<section id="sec-conclusion" class="level1" data-number="9">
<h1 data-number="9"><span class="header-section-number">9</span> Concluding Remarks</h1>
</section>
<section id="acknowledgment" class="level1 unnumbered">
<h1 class="unnumbered">Acknowledgment</h1>
<p>P. A. thanks …</p>
</section>
<section id="references" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div id="ref-borch2022machine" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">C. Borch, <span>“Machine learning, knowledge risk, and principal-agent problems in automated trading,”</span> <em>Technology in Society</em>, p. 101852, 2022.</div>
</div>
<div id="ref-o2016weapons" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">C. O’neil, <em>Weapons of math destruction: How big data increases inequality and threatens democracy</em>. Crown, 2016.</div>
</div>
<div id="ref-rudin2019stop" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">C. Rudin, <span>“Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead,”</span> <em>Nature Machine Intelligence</em>, vol. 1, no. 5, pp. 206–215, 2019.</div>
</div>
<div id="ref-pawelczyk2021carla" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">M. Pawelczyk, S. Bielawski, J. van den Heuvel, T. Richter, and G. Kasneci, <span>“Carla: A python library to benchmark algorithmic recourse and counterfactual explanation algorithms,”</span> <em>arXiv preprint arXiv:2108.00783</em>, 2021.</div>
</div>
<div id="ref-wachter2017counterfactual" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">S. Wachter, B. Mittelstadt, and C. Russell, <span>“Counterfactual explanations without opening the black box: Automated decisions and the GDPR,”</span> <em>Harv. JL &amp; Tech.</em>, vol. 31, p. 841, 2017.</div>
</div>
<div id="ref-altmeyer2022CounterfactualExplanations" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">P. Altmeyer, <em><span class="nocase">CounterfactualExplanations.jl - a Julia package for Counterfactual Explanations and Algorithmic Recourse</span></em>. 2022. Available: <a href="https://github.com/pat-alt/CounterfactualExplanations.jl">https://github.com/pat-alt/CounterfactualExplanations.jl</a></div>
</div>
<div id="ref-schut2021generating" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">L. Schut <em>et al.</em>, <span>“Generating interpretable counterfactual explanations by implicit minimisation of epistemic and aleatoric uncertainties,”</span> in <em>International conference on artificial intelligence and statistics</em>, 2021, pp. 1756–1764.</div>
</div>
<div id="ref-joshi2019towards" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">S. Joshi, O. Koyejo, W. Vijitbenjaronk, B. Kim, and J. Ghosh, <span>“Towards realistic individual recourse and actionable explanations in black-box decision making systems,”</span> <em>arXiv preprint arXiv:1907.09615</em>, 2019.</div>
</div>
<div id="ref-mothilal2020explaining" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">R. K. Mothilal, A. Sharma, and C. Tan, <span>“Explaining machine learning classifiers through diverse counterfactual explanations,”</span> in <em>Proceedings of the 2020 conference on fairness, accountability, and transparency</em>, 2020, pp. 607–617.</div>
</div>
<div id="ref-antoran2020getting" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline">J. Antorán, U. Bhatt, T. Adel, A. Weller, and J. M. Hernández-Lobato, <span>“Getting a clue: A method for explaining uncertainty estimates,”</span> <em>arXiv preprint arXiv:2006.06848</em>, 2020.</div>
</div>
<div id="ref-karimi2020survey" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline">A.-H. Karimi, G. Barthe, B. Schölkopf, and I. Valera, <span>“A survey of algorithmic recourse: Definitions, formulations, solutions, and prospects,”</span> <em>arXiv preprint arXiv:2010.04050</em>, 2020.</div>
</div>
<div id="ref-verma2020counterfactual" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline">S. Verma, J. Dickerson, and K. Hines, <span>“Counterfactual explanations for machine learning: A review,”</span> <em>arXiv preprint arXiv:2010.10596</em>, 2020.</div>
</div>
<div id="ref-ustun2019actionable" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline">B. Ustun, A. Spangher, and Y. Liu, <span>“Actionable recourse in linear classification,”</span> in <em>Proceedings of the conference on fairness, accountability, and transparency</em>, 2019, pp. 10–19.</div>
</div>
<div id="ref-karimi2021algorithmic" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[14] </div><div class="csl-right-inline">A.-H. Karimi, B. Schölkopf, and I. Valera, <span>“Algorithmic recourse: From counterfactual explanations to interventions,”</span> in <em>Proceedings of the 2021 ACM conference on fairness, accountability, and transparency</em>, 2021, pp. 353–362.</div>
</div>
<div id="ref-upadhyay2021towards" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[15] </div><div class="csl-right-inline">S. Upadhyay, S. Joshi, and H. Lakkaraju, <span>“Towards robust and reliable algorithmic recourse,”</span> <em>arXiv preprint arXiv:2102.13620</em>, 2021.</div>
</div>
<div id="ref-carrizosa2021generating" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[16] </div><div class="csl-right-inline">E. Carrizosa, J. Ramırez-Ayerbe, and D. Romero, <span>“Generating collective counterfactual explanations in score-based classification via mathematical optimization,”</span> 2021.</div>
</div>
<div id="ref-rabanser2019failing" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[17] </div><div class="csl-right-inline">S. Rabanser, S. Günnemann, and Z. Lipton, <span>“Failing loudly: An empirical study of methods for detecting dataset shift,”</span> <em>Advances in Neural Information Processing Systems</em>, vol. 32, 2019.</div>
</div>
<div id="ref-chandola2009anomaly" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[18] </div><div class="csl-right-inline">V. Chandola, A. Banerjee, and V. Kumar, <span>“Anomaly detection: A survey,”</span> <em>ACM computing surveys (CSUR)</em>, vol. 41, no. 3, pp. 1–58, 2009.</div>
</div>
<div id="ref-widmer1996learning" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[19] </div><div class="csl-right-inline">G. Widmer and M. Kubat, <span>“Learning in the presence of concept drift and hidden contexts,”</span> <em>Machine learning</em>, vol. 23, no. 1, pp. 69–101, 1996.</div>
</div>
<div id="ref-gama2014survey" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[20] </div><div class="csl-right-inline">J. Gama, I. Žliobaitė, A. Bifet, M. Pechenizkiy, and A. Bouchachia, <span>“A survey on concept drift adaptation,”</span> <em>ACM computing surveys (CSUR)</em>, vol. 46, no. 4, pp. 1–37, 2014.</div>
</div>
<div id="ref-nelson2015evaluating" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[21] </div><div class="csl-right-inline">K. Nelson, G. Corbin, M. Anania, M. Kovacs, J. Tobias, and M. Blowers, <span>“Evaluating model drift in machine learning algorithms,”</span> in <em>2015 IEEE symposium on computational intelligence for security and defense applications (CISDA)</em>, 2015, pp. 1–8.</div>
</div>
<div id="ref-ackerman2021machine" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[22] </div><div class="csl-right-inline">S. Ackerman, P. Dube, E. Farchi, O. Raz, and M. Zalmanovici, <span>“Machine learning model drift detection via weak data slices,”</span> in <em>2021 IEEE/ACM third international workshop on deep learning for testing and testing for deep learning (DeepTest)</em>, 2021, pp. 1–8.</div>
</div>
<div id="ref-de2021framework" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[23] </div><div class="csl-right-inline">R. M. B. de Oliveira and D. Martens, <span>“A framework and benchmarking study for counterfactual generating methods on tabular data,”</span> <em>Applied Sciences</em>, vol. 11, no. 16, p. 7274, 2021.</div>
</div>
<div id="ref-arcones1992bootstrap" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[24] </div><div class="csl-right-inline">M. A. Arcones and E. Gine, <span>“On the bootstrap of u and v statistics,”</span> <em>The Annals of Statistics</em>, pp. 655–674, 1992.</div>
</div>
<div id="ref-hanneke2007bound" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[25] </div><div class="csl-right-inline">S. Hanneke, <span>“A bound on the label complexity of agnostic active learning,”</span> in <em>Proceedings of the 24th international conference on machine learning</em>, 2007, pp. 353–360.</div>
</div>
<div id="ref-gmsc_data" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[26] </div><div class="csl-right-inline">K. Competition, <span>“Give me some credit, improve on the state of the art in credit scoring by predicting the probability that somebody will experience financial distress in the next two years.”</span> <a href="https://www.kaggle.com/c/GiveMeSomeCredit">https://www.kaggle.com/c/GiveMeSomeCredit</a></div>
</div>
<div id="ref-germancredit1994" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[27] </div><div class="csl-right-inline">H. Hoffman, <span>“German credit data,”</span> 1994. <a href="https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)">https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)</a></div>
</div>
<div id="ref-borisov2021deep" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[28] </div><div class="csl-right-inline">V. Borisov, T. Leemann, K. Seßler, J. Haug, M. Pawelczyk, and G. Kasneci, <span>“Deep neural networks and tabular data: A survey,”</span> <em>arXiv preprint arXiv:2110.01889</em>, 2021.</div>
</div>
<div id="ref-grinsztajn2022tree" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[29] </div><div class="csl-right-inline">L. Grinsztajn, E. Oyallon, and G. Varoquaux, <span>“Why do tree-based models still outperform deep learning on tabular data?”</span> <em>arXiv preprint arXiv:2207.08815</em>, 2022.</div>
</div>
</div>
</section>





<div id="quarto-appendix" class="default"><section id="tables" class="level1 appendix" data-number="10"><h2 class="quarto-appendix-heading"><span class="header-section-number">10</span> Tables</h2><div class="quarto-appendix-contents">

<p>…</p>
</div></section><section id="figures" class="level1 appendix" data-number="11"><h2 class="quarto-appendix-heading"><span class="header-section-number">11</span> Figures</h2><div class="quarto-appendix-contents">

<p>…</p>
</div></section><section id="code" class="level1 appendix" data-number="12"><h2 class="quarto-appendix-heading"><span class="header-section-number">12</span> Code</h2><div class="quarto-appendix-contents">

<p>…</p>
</div></section><section class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1" role="doc-endnote"><p>Note that despite the naming convention our goal here is not to provide yet another counterfactual generator, but merely investigate the most simple penalty we can think of with respect to its effectiveness.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p><span class="citation" data-cites="joshi2019towards"><a href="#ref-joshi2019towards" role="doc-biblioref">[8]</a></span> define convergence of Algorithm 1 in this way. The implementation of <span class="citation" data-cites="wachter2017counterfactual"><a href="#ref-wachter2017counterfactual" role="doc-biblioref">[5]</a></span> in CARLA is also defined in this way.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>To see how the data is generated see here: <a href="https://github.com/pat-alt/AlgorithmicRecourseDynamics.jl/blob/main/notebooks/synthetic_datasets.ipynb">https://github.com/pat-alt/AlgorithmicRecourseDynamics.jl/blob/main/notebooks/synthetic_datasets.ipynb</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>