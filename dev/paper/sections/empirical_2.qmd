# Experiments {#sec-empirical-2}

Below we first present our main findings with respect to the presence of endogenous macrodynamics in Algorithmic Recourse. We then go on to introduce and test a number of potential mitigation strategies.

## Endogenous Macrodynamics

In light of the huge number of experiments we have conducted, we shall start this section off with a few high-level observations. Across all datasets (synthetic and real), classifiers and counterfactual generators we observe either most or all of the following dynamics at varying degrees:

- Statistically significant domain and model shift as measured by MMD. 
- A deterioration in out-of-sample model performance as measured by the F-Score evaluated on a test sample. In many cases this drop in performance is substantial.
- Significant perturbations to the model parameters as well as an increase in the model's decisiveness.
- Disagreement between the original and retrained model, in some cases large.

There is also some clear heterogeneity across the results: 

- The observed, adverse dynamics are generally speaking of the highest magnitude for the simple linear classifier. Differences in results for the MLP and Deep Ensemble are mostly negligible. 
- The reduction in model performance appears to be most severe when classes are not perfectly separable.
- With the exception of the Greedy generator, all other generators generally perform somewhat better than Wachter overall as expected. 

Granular tables and visualizations summarizing all of our experimental results are published in the supplementary appendix. Here we will zoom in on illustrative examples that nicely reflect some of the broader patterns we have observed and listed above. @fig-syn presents a small subset of our results for the synthetic dataset with overlapping classes. It shows the resulting values for some of our evaluation metrics at the end of the experiment, so after all $T=50$ rounds, along with error bars indicating the variation across folds. The top row shows the estimated domain shifts. While it is not straight-forward to interpret the exact magnitude of MMD, we can see clearly that the values are different from zero and there is essentially no variation across our five folds. With respect to the domain shifts, the Greedy generator actually induces the smallest shifts. In general, we have observed the opposite. The second row shows the estimated model shifts, where here we have used the meshgrid approach explained earlier. As with the domain shifts, the observed values are clearly different from zero and variation across folds is once again small. In this case, the results for this particular dataset very much reflect the broader patterns we have observed: Latent Space generators induce the smallest shifts, followed by DiCE, then Wachter and finally Greedy. The same broad pattern also emerges in the bottom row: we observe the smallest deterioration in model performance for Latent Space generators, albeit still a reduction in the F-Score of around 5 percentage points on average. 

![Results for synthetic data with overlapping classes. The shown model MMD (PP MMD) was computed over a meshgrid of points. Error bars indicate the standard deviation across folds.](www/synthetic_results.png){#fig-syn fig.pos="h" width="9cm" height="9cm"}

Turning to the real-world data next ...

![PLACEHOLDER: Results for real-world as table.](www/placeholder.png){#fig-real fig.pos="h" width="9cm" height="9cm"}

## Potential Mitigation Strategies {#sec-empirical-2-mitigate}

In the following we explore several ideas for strategies that may help to mitigate those types of endogenous recourse dynamics we observed in the previous subsection. All of them essentially boil down to one simple principle: to avoid substantial domain and model shifts, the generated counterfactuals should comply as much as possible with the true data generating process. This principle is really at the core of Latent Space generators, and hence it is not surprising that we have found these types of generators to perform comparably well in the previous subsection. But as we have mentioned earlier, generators that rely on separate generative models carry an additional computational cost and - perhaps more importantly - their performance hinges on the performance of said generative models. Fortunately, it turns out that we can use a number of other, much simpler strategies, which we will discuss now. 

### More Conservative Decision Thresholds

The most obvious and trivial mitigation strategy is to simply choose a higher decision threshold $\gamma$. Under $\gamma=0.5$, counterfactuals will end up near the decision boundary by construction. Since this is the region of maximal aleotoric uncertainty, the classifier is bound to be thrown off. By simply setting a more conservative decision threshold, we can avoid this issue to some extent. A potential drawback of this approach is that a classifier with high decisiveness may classify samples with high confidence even far away from the training data. 

### Classifier Preserving ROAR

Another potential strategy draws inspiration from ROAR @upadhyay2021towards: to preserve the classifier, we propose to simply explicitly penalize the loss it incurs when evaluated on the counterfactual $x^\prime$ at given parameter values. Recall that $g(\cdot)$ denotes what we had defined as the external cost in @eq-collective. Then formally we let

$$
\begin{aligned}
g(f(s_k^\prime)) = l(M(f(s_k^\prime)),y^\prime)
\end{aligned}
$$ {#eq-clap}

for each counterfactual $k$ where $l$ denotes the loss function used to train $M$. This approach, which we shall refer to as **ClapROAR**, is based on the intuition that (endogenous) model shifts will be triggered by counterfactuals that increase classifier loss. It is closely linked to the idea of choosing higher decision threshold, but likely better at avoiding the potential pitfalls associated with highly decisive classifiers. It also makes the private vs. external cost trade-off more explicit and hence manageable.

### Gravitational Counterfactual Explanations 

Yet another strategy simply extends Wachter as follows: instead of only penalizing the distance of the individuals' counterfactual to its factual, we propose penalizing its distance to some sensible point in the target domain, for example the sample average: $\bar{x}$:

$$
\begin{aligned}
g(f(s_k^\prime)) = \text{dist}(f(s_k^\prime),\bar{x})
\end{aligned}
$$ {#eq-clap}

Once again we can putting this in the context of @eq-collective, the former penalty can be thought of here as the private cost incurred by the individual, while the latter reflects the external cost incurred by other individuals. Higher choices of $\lambda_2$ relative to $\lambda_1$ will lead counterfactuals to gravitate towards the specified point $\bar{x}$ in the target domain. In the remainder of this paper we will therefore refer to this approach as **Gravitational** generator, when we investigate its potential usefulness for mitigating endongenous macrodynamics^[Note that despite the naming convention our goal here is not to provide yet another counterfactual generator, but merely investigate the most simple penalty we can think of with respect to its effectiveness.]. 

@fig-mitigation shows an illustrative example that demonstrates the differences in counterfactual outcomes when using the various mitigation strategies compared to the baseline approach, that is Wachter with $\gamma=0.5$: choosing a higher decision threshold pushes the counterfactual a little further into the target domain; this effect is even stronger for ClapROAR; finally, using the Gravitational generator the counterfactual ends up all the way inside the target domain in the neighbourhood of $\bar{x}$^[In order for the Graviational generator and ClapROAR to work as expected, one needs to ensure that counterfactual search continues, independent of the threshold probability $\gamma$.].

```{julia}
#| eval: false
using Random
Random.seed!(2022)

# Data:
using MLJ
N = 1000
X, ys = make_blobs(N, 2; centers=2, as_table=false, center_box=(-5 => 5), cluster_std=0.5)
ys .= ys.==2
X = X'
xs = Flux.unstack(X,2)
data = zip(xs,ys)
counterfactual_data = CounterfactualData(X,ys')

# Models:
using AlgorithmicRecourseDynamics
M = AlgorithmicRecourseDynamics.Models.FluxModel(counterfactual_data)
M = AlgorithmicRecourseDynamics.Models.train(M, counterfactual_data)

# Generators:
generators = Dict(
    "Generic (γ=0.5)" => GenericGenerator(decision_threshold=0.5),
    "Generic (γ=0.9)" => GenericGenerator(decision_threshold=0.9),
    "Gravitational" => GravitationalGenerator(),
    "ClapROAR" => ClapROARGenerator()
)

# Counterfactuals
x = select_factual(counterfactual_data, rand(1:size(X)[2])) 
y = round(probs(M, x)[1])
target = ifelse(y==1.0,0.0,1.0) # opposite label as target
T = 50
counterfactuals = Dict([name => generate_counterfactual(x, target, counterfactual_data, M, gen; T=T, latent_space=false) for (name, gen) in generators])

# Plots:
plts = []
for (name,ce) ∈ counterfactuals
    plt = plot(ce; title=name, colorbar=false, ticks = false, legend=false)
    plts = vcat(plts..., plt)
end
plt = plot(plts..., size=(750,200), layout=(1,4))
savefig(plt, "dev/paper/www/mitigation.png")
```

![The differences in counterfactual outcomes when using the various mitigation strategies compared to the baseline approach, that is Wachter with $\gamma=0.5$](www/mitigation.png){#fig-mitigation fig.pos="h" width=45%}

Our findings indicate that all three mitigation strategies are at least at par with Latent Space generators with respect to their effectiveness at mitigating domain and model shifts (@fig-mitigate-results) ...

An interesting finding is also that the proposed strategies can have a complementary effect when used in combination with Latent Space generators. In experiments we conducted on the synthetic data, the benefits of Latent Space generators were exacerbated further when using a more conservative threshold or combining it with the penalties underlying Gravitational and ClapROAR. 

![PLACEHOLDER: Results for mitigating strategies.](www/synthetic_results.png){#fig-mitigate-results fig.pos="h" width="9cm" height="9cm"}