# Experiments {#sec-empirical-2}

## Endogenous Macrodynamics



## Potential Mitigation Strategies {#sec-empirical-2-mitigate}

In the following we explore several ideas for strategies that may help to mitigate those types of endogenous recourse dynamics we observed in the previous subsection. All of them essentially boil down to one simple principle: to avoid substantial domain and model shifts, the generated counterfactuals should comply as much as possible with the true data generating process. This principle is really at the core of Latent Space generators, and hence it is not surprising that we have found these types of generators to perform comparably well in the previous subsection. But as we have mentioned earlier, generators that rely on separate generative models carry an additional computational cost and - perhaps more importantly - their performance hinges on the performance of said generative models. Fortunately, it turns out that we can use a number of other, much simpler strategies, which we will discuss now. 

### More Conservative Decision Thresholds

The most obvious and trivial mitigation strategy is to simply choose a higher decision threshold $\gamma$. Under $\gamma=0.5$, counterfactuals will end up near the decision boundary by construction. Since this is the region of maximal aleotoric uncertainty, the classifier is bound to be thrown off. By simply setting a more conservative decision threshold, we can avoid this issue to some extent. A potential drawback of this approach is that a classifier with high decisiveness may classify samples with high confidence even far away from the training data. 

### Classifier Preserving ROAR

Another potential strategy draws inspiration from ROAR @upadhyay2021towards: to preserve the classifier, we propose to simply explicitly penalize the loss it incurs when evaluated on the counterfactual $x^\prime$ at given parameter values. Recall that $g(\cdot)$ denotes what we had defined as the external cost in @eq-collective. Then formally we let

$$
\begin{aligned}
g(f(s_k^\prime)) = l(M(f(s_k^\prime)),y^\prime)
\end{aligned}
$$ {#eq-clap}

for each counterfactual $k$ where $l$ denotes the loss function used to train $M$. This approach, which we shall refer to as **ClapROAR**, is based on the intuition that (endogenous) model shifts will be triggered by counterfactuals that increase classifier loss. It is closely linked to the idea of choosing higher decision threshold, but likely better at avoiding the potential pitfalls associated with highly decisive classifiers. It also makes the private vs. external cost trade-off more explicit and hence manageable.

### Gravitational Counterfactual Explanations 

Yet another strategy simply extends Wachter as follows: instead of only penalizing the distance of the individuals' counterfactual to its factual, we propose penalizing its distance to some sensible point in the target domain, for example the sample average: $\bar{x}$:

$$
\begin{aligned}
g(f(s_k^\prime)) = \text{dist}(f(s_k^\prime),\bar{x})
\end{aligned}
$$ {#eq-clap}

Once again we can putting this in the context of @eq-collective, the former penalty can be thought of here as the private cost incurred by the individual, while the latter reflects the external cost incurred by other individuals. Higher choices of $\lambda_2$ relative to $\lambda_1$ will lead counterfactuals to gravitate towards the specified point $\bar{x}$ in the target domain. In the remainder of this paper we will therefore refer to this approach as **Gravitational** generator, when we investigate its potential usefulness for mitigating endongenous macrodynamics^[Note that despite the naming convention our goal here is not to provide yet another counterfactual generator, but merely investigate the most simple penalty we can think of with respect to its effectiveness.]. 

@fig-mitigation shows an illustrative example that demonstrates the differences in counterfactual outcomes when using the various mitigation strategies compared to the baseline approach, that is Wachter with $\gamma=0.5$: choosing a higher decision threshold pushes the counterfactual a little further into the target domain; this effect is even stronger for ClapROAR; finally, using the Gravitational generator the counterfactual ends up all the way inside the target domain in the neighbourhood of $\bar{x}$^[In order for the Graviational generator and ClapROAR to work as expected, one needs to ensure that counterfactual search continues, independent of the threshold probability $\gamma$.].

```{julia}
#| eval: false
using Random
Random.seed!(2022)

# Data:
using MLJ
N = 1000
X, ys = make_blobs(N, 2; centers=2, as_table=false, center_box=(-5 => 5), cluster_std=0.5)
ys .= ys.==2
X = X'
xs = Flux.unstack(X,2)
data = zip(xs,ys)
counterfactual_data = CounterfactualData(X,ys')

# Models:
using AlgorithmicRecourseDynamics
M = AlgorithmicRecourseDynamics.Models.FluxModel(counterfactual_data)
M = AlgorithmicRecourseDynamics.Models.train(M, counterfactual_data)

# Generators:
generators = Dict(
    "Generic (γ=0.5)" => GenericGenerator(decision_threshold=0.5),
    "Generic (γ=0.9)" => GenericGenerator(decision_threshold=0.9),
    "Gravitational" => GravitationalGenerator(),
    "ClapROAR" => ClapROARGenerator()
)

# Counterfactuals
x = select_factual(counterfactual_data, rand(1:size(X)[2])) 
y = round(probs(M, x)[1])
target = ifelse(y==1.0,0.0,1.0) # opposite label as target
T = 50
counterfactuals = Dict([name => generate_counterfactual(x, target, counterfactual_data, M, gen; T=T, latent_space=false) for (name, gen) in generators])

# Plots:
plts = []
for (name,ce) ∈ counterfactuals
    plt = plot(ce; title=name, colorbar=false, ticks = false, legend=false)
    plts = vcat(plts..., plt)
end
plt = plot(plts..., size=(750,200), layout=(1,4))
savefig(plt, "dev/paper/www/mitigation.png")
```

![The differences in counterfactual outcomes when using the various mitigation strategies compared to the baseline approach, that is Wachter with $\gamma=0.5$](www/mitigation.png){#fig-mitigation fig.pos="h" width=45%}

Our findings ...