---
title: Proof of concept
jupyter: julia-1.7
---

```{julia}
#| echo: false

# Environment:
using Pkg; Pkg.activate("docs/src/paper")

# Deps:
using CounterfactualExplanations
using Flux
using LaplaceRedux
using MLJBase
using Plots
using Random

# Setup
Random.seed!(1234)
theme(:wong)
include("docs/src/utils.jl")    # some helper functions
output_path = output_dir("poc") # output directory for artifacts
www_path = www_dir("poc")       # output directory for images
```

To start with, we will look at a proof-of-concept that demonstrates the main observation underlying that paper is framed around. In particular, we will use synthetic data to see how endogenous domain shifts and the resulting model shifts can have implications on the validity and cost of algorithmic recourse.

## Classifiers

```{julia}
N = 1000
xmax = 2
X, ys = make_blobs(
    N, 2; 
    centers=2, as_table=false, center_box=(-xmax => xmax), cluster_std=0.1
)
ys .= ys.==2
X = X'
xs = Flux.unstack(X,2)
data = zip(xs,ys)
counterfactual_data = CounterfactualData(X,ys')
```

### Logistic Regression

```{julia}
n_epochs = 10
model = Chain(Dense(2,1))
Models.forward!(model, data; loss=:logitbinarycrossentropy, opt=:Adam, n_epochs=n_epochs)
```

```{julia}
model = Laplace(model, likelihood=:classification)
LaplaceRedux.fit!(model, data)      # fit Laplace Approximation
optimize_prior!(model)              # optimize prior
mod = LaplaceReduxModel(model)
```

```{julia}
#| output: true
#| label: fig-model
#| fig-cap: "The baseline model: contours indicate the predicted label; dots indicate observed data points."

plt_original = plot(mod, counterfactual_data; zoom=0, colorbar=false, title="(a)")
```

## Single Round

### Generate counterfactual

```{julia}
γ = 0.75
gen = GenericGenerator(;decision_threshold=γ)
```

```{julia}
μ = 0.10
candidates = findall(ys.==0)
chosen = rand(candidates, Int(round(μ*length(candidates))))
X′ = copy(X)
y′ = copy(ys)
using CounterfactualExplanations.Counterfactuals: counterfactual, counterfactual_label
for i in chosen
    x = X[:,i]
    outcome = generate_counterfactual(x, 1, counterfactual_data, mod, gen)
    X′[:,i] = counterfactual(outcome)
    y′[i] = first(counterfactual_label(outcome))
end
counterfactual_data′ = CounterfactualData(X′,y′')
plt_single = plot(mod,counterfactual_data′;zoom=0,colorbar=false,title="(b)")
```

### Retrain

```{julia}
Models.forward!(mod.model.model, data; loss=:logitbinarycrossentropy, opt=:Adam, n_epochs=n_epochs)
LaplaceRedux.fit!(mod.model, data)      # fit Laplace Approximation
optimize_prior!(mod.model)              # optimize prior
plt_single_retrained = plot(mod,counterfactual_data′;zoom=0,colorbar=false,title="(c)")
```

### Repeat

```{julia}
i = 2
while i <= 10
    counterfactual_data′ = CounterfactualData(X′,y′')
    candidates = findall(y′.==0)
    chosen = rand(candidates, Int(round(μ*length(candidates))))
    H₀ = mod.model.H # use posterior as new prior
    Models.forward!(mod.model.model, data; loss=:logitbinarycrossentropy, opt=:Adam, n_epochs=n_epochs)
    LaplaceRedux.fit!(mod.model, data)      # fit Laplace Approximation
    optimize_prior!(mod.model)              # optimize prior
    for i in chosen
        x = X′[:,i]
        outcome = generate_counterfactual(x, 1, counterfactual_data, mod, gen)
        X′[:,i] = counterfactual(outcome)
        y′[i] = first(counterfactual_label(outcome))
    end
    i += 1
end
plt_single_repeat = plot(mod,counterfactual_data′;zoom=0,colorbar=false,title="(d)")
```

```{julia}
plt = plot(plt_original, plt_single, plt_single_retrained, plt_single_repeat, layout=(1,4), legend=false, axis=nothing, size=(600,165))
# savefig(plt, joinpath(www_path, "poc.png"))
```

```{julia}

```


![](../artifacts/upload/www/synthetic/poc.png)