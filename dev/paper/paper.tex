% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  conference]{IEEEtran}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{placeins}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Endogenous Macrodynamics in Algorithmic Recourse},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Endogenous Macrodynamics in Algorithmic Recourse}
\author{}
\date{}

\begin{document}
\maketitle
\begin{abstract}
Existing work on Counterfactual Explanations (CE) and Algorithmic
Recourse (AR) has largely been limited to the static setting: given some
classifier we are interested in finding close, actionable, realistic,
sparse, diverse and ideally causally founded counterfactuals. The
ability of CE to handle dynamics like data and model drift remains a
largely unexplored research challenge at this point. Only one recent
work considers the implications of exogenous domain and model shifts.
This project instead focuses on endogenous dynamics, that is shifts that
occur when AR is actually implemented by a proportion of individuals.
Early findings suggest that the involved shifts may be large with
important implications on the validity of AR and the overall
characteristics of the sample population.
\end{abstract}
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[sharp corners, enhanced, breakable, interior hidden, boxrule=0pt, borderline west={3pt}{0pt}{shadecolor}, frame hidden]}{\end{tcolorbox}}\fi

\hypertarget{sec-intro}{%
\section{Introduction}\label{sec-intro}}

Recent advances in Artificial Intelligence (AI) have propelled its
adoption in scientific domains outside of Computer Science including
Healthcare, Bioinformatics, Genetics and the Social Sciences. While this
has in many cases brought benefits in terms of efficiency,
state-of-the-art models like Deep Neural Networks (DNN) have also given
rise a new type of principal-agent problem in the context of data-driven
decision-making. It involves a group of \textbf{principals} - i.e.~human
stakeholders - that fail to understand the behaviour of their
\textbf{agent} - i.e.~the model used for automated decision-making
\protect\hyperlink{ref-borch2022machine}{{[}1{]}}.

Models or algorithms that fall into this category are typically referred
to \textbf{black-box} models. Despite their shortcomings, black-box
models have grown in popularity in recent years and have at times
created undesirable societal outcomes
\protect\hyperlink{ref-o2016weapons}{{[}2{]}}. The scientific community
has tackled this issue from two different angles: while some have
appealed for a strict focus on inherently iterpretable models
\protect\hyperlink{ref-rudin2019stop}{{[}3{]}}, others have investigated
different ways to explain the behaviour of black-box models. These two
sub-domains can be broadly referred to as \textbf{interpretable AI} and
\textbf{explainable AI} (XAI), respectively.

Among the approaches to XAI that have recently grown in popularity are
\textbf{Counterfactual Explanations} (CE). They explain how inputs into
a model need to change for it to produce different outputs.
Counterfactual Explanations that involve realistic and actionable
changes can be used for the purpose of \textbf{Algorithmic Recourse}
(AR) to help individuals who face adverse outcomes. An example relevant
to the Social Sciences is consumer credit: in this context AR can be
used to guide individuals in improving their creditworthiness, should
they have previously been denied access to credit based on an automated
decision-making system. A meaningful recourse recommendation for a
denied applicant could be: \emph{``If your net savings rate had been
10\% of your monthly income instead of the actual 8\%, your application
would have been successful. See if you can temporarily cut down on
consumption.''} In the remainder of this paper we will use both
terminologies - recourse and counterfactual - interchangeably to refer
to situations where counterfactuals are generated with the intent to
provide individual recourse.

Existing work in this field has largely worked in a static setting:
various approaches have been proposed to generate counterfactuals for a
given individual that is subject to some pre-trained model. More recent
work has compared different approaches within this static setting
\protect\hyperlink{ref-pawelczyk2021carla}{{[}4{]}}. In this work we go
one step further and ask ourselves: what happens if recourse is provided
and implemented repeatedly? What types of dynamics are introduced and
how do different counterfactual generators compare in this context?

Research on algorithmic recourse has also so far typically addressed the
issue from the perspective of one single individual and has indeed been
referred to as \textbf{individual recourse} in some places. Arguably
though, most real-world applications that warrant algorithmic recourse
involve potentially large groups of individuals typically competing for
scarce resources. Our work demonstrates that in such scenarios, choices
made by or for one single individual are likely to affect the broader
collective of individuals in ways that current approaches to AR fail to
account for. More specifically, we argue that a strict focus on
minimizing the private costs faced by individuals may be too narrow an
objective.

Figure~\ref{fig-poc} illustrates this idea for a binary problem
involving a probabilistic classifier and the counterfactual generator
proposed by \protect\hyperlink{ref-wachter2017counterfactual}{{[}5{]}}:
the implementation of AR for a subset of individuals leads to a domain
shift (b), which in turn triggers a model shift (c). As this game of
implementing AR and updating the classifier is repeated, the decision
boundary moves away from training samples that were originally in the
target class (d). We refer to these types of dynamics as
\textbf{endogenous} because they are induced by the implementation of
recourse itself. The term \textbf{macrodynamics} is borrowed from the
economics literature and used to describe processes involving whole
groups or societies.

\begin{figure}

{\centering \includegraphics[width=0.45\textwidth,height=\textheight]{www/poc.png}

}

\caption{\label{fig-poc}Dynamics in Algorithmic Recourse: we have a
simple Bayesian model trained for binary classification (a); the
implementation of AR for a random subset of individuals leads to a
domain shift (b); as the classifier is retrained we observe a model
shift (c); as this process is repeated, the decision boundary moves away
from the target class (d).}

\end{figure}

We think that these types of endogenous dynamics may be problematic and
warrant our attention. Firstly, model shifts may inadvertently change
classification outcomes for individuals who never received and
implemented recourse. Secondly and relatedly, we observe in
Figure~\ref{fig-poc} that as the decision boundary moves in the
direction of the non-target class, counterfactual paths become shorter:
in the consumer credit example, individuals that previously would have
been denied credit based on their input features are suddenly considered
as creditworthy. Average default risk across all borrowers can therefore
be expected to increase. Conversely, lenders that anticipate such
dynamics may choose to refrain from offering recourse (and hence credit)
to more than just a tiny share of individuals. In that latter and
perhaps more likely scenario, the probability of being offered recourse
decreases with every individual that implements recourse: in other
words, the actions of first-movers exert a negative externality on
future would-be borrowers.

To the best of our knowledge this is the first work investigating
endogenous macrodynamics in AR. Our contributions to the state of
knowledge are as follows: firstly, we posit a compelling argument that
calls for a novel perspective on algorithmic recourse extending our
focus from single individuals to groups. Secondly, we introduce an
experimental framework extending previous work by
\protect\hyperlink{ref-altmeyer2022CounterfactualExplanations}{{[}6{]}},
which enables us to study macrodynamics of algorithmic recourse through
simulations that can be fully parallelized. Thirdly, we use this
framework to provide a first in-depth analysis of endogenous recourse
dynamics induced by various popular counterfactual generators including
\protect\hyperlink{ref-wachter2017counterfactual}{{[}5{]}},
\protect\hyperlink{ref-schut2021generating}{{[}7{]}},
\protect\hyperlink{ref-joshi2019towards}{{[}8{]}},
\protect\hyperlink{ref-mothilal2020explaining}{{[}9{]}} and
\protect\hyperlink{ref-antoran2020getting}{{[}10{]}}. To this end we
propose a number of novel evaluation metrics that can be used to
quantify and benchmark the macrodynamics introduced by the different
generators. Finally, we also discuss what drives endogenous dynamics and
propose strategies to mitigate them.

The remainder of the paper is structured as follows:
Section~\ref{sec-related} places our work in the broader context of
related literature. Section~\ref{sec-method} presents our methodology
and data. Section~\ref{sec-empirical} presents our empirical findings
which are then discussed in the broader context of the literature in
Section~\ref{sec-discussion}. We also point to some of the limitations
or our work as well as avenues for future research in
Section~\ref{sec-limit}. Finally, Section~\ref{sec-conclusion}
concludes.

\hypertarget{sec-related}{%
\section{Related Work}\label{sec-related}}

In this Section we provide a review of the relevant literature. First,
Section~\ref{sec-related-recourse} discusses the existing research
within the domain of counterfactual explanations and algorithmic
recourse. Then, Section~\ref{sec-related-shifts} presents some of the
previous work on the measurement of dataset and model shifts.

\hypertarget{sec-related-recourse}{%
\subsection{Algorithmic Recourse}\label{sec-related-recourse}}

A framework for Counterfactual Explanations was first proposed in 2017
by \protect\hyperlink{ref-wachter2017counterfactual}{{[}5{]}} and has
served as the baseline for most methodologies that have been proposed
since then. Let \(M: \mathcal{X} \mapsto \mathcal{Y}\) denote some
pre-trained model that maps from inputs \(X \in \mathcal{X}\) to outputs
\(Y \in \mathcal{Y}\). Then we are interested in minimizing the
complexity or effort \(H=h(x\prime)\) associated with moving an
individual \(x\) to a counterfactual state \(x\prime\) such that the
predicted outcome \(M(x\prime)\) corresponds to some target outcome
\(t\):

\begin{equation}\protect\hypertarget{eq-obj}{}{
\min_{x\prime \in \mathcal{x}} c(x\prime) \ \ \ \mbox{s. t.} \ \ \ M(x\prime) = t
}\label{eq-obj}\end{equation}

For implementation purposes, Equation~\ref{eq-obj} is typically
approximated through regularization:

\begin{equation}\protect\hypertarget{eq-solution}{}{
x\prime = \arg \min_{x\prime}  \ell(M(x\prime),t) + \lambda h(x\prime)
}\label{eq-solution}\end{equation}

In the baseline work and many subsequent approaches the complexity
function \(h: \mathcal{X} \mapsto \mathbb{R}\) is proxied by some
distance metric based on the simple intuition that large perturbations
of \(x\) are costly.

Many approaches for the generation of algorithmic recourse have been
described in the literature. An October 2020 survey by Karimi et
al.~layed out 60 algorithms that have been proposed since 2014
\protect\hyperlink{ref-karimi2020survey}{{[}11{]}}. Another survey
published around the same time by Verma et al.~described 29 algorithms
\protect\hyperlink{ref-verma2020counterfactual}{{[}12{]}}. Different
approaches vary primarily in terms of the objective functions they
impose, how they optimize said objective (from brute force through
gradient-based approaches to graph traversal algorithms), and how the
ensure that certain requirements for CE are met. Regarding the latter,
the literature has produced an extensive list of desiderata each
addressing different needs. To name but a few, we are interested in
generating counterfactuals that close
\protect\hyperlink{ref-wachter2017counterfactual}{{[}5{]}}, actionable
\protect\hyperlink{ref-ustun2019actionable}{{[}13{]}}, realistic
\protect\hyperlink{ref-schut2021generating}{{[}7{]}}, sparse, diverse
\protect\hyperlink{ref-mothilal2020explaining}{{[}9{]}} and if possible
causally founded
\protect\hyperlink{ref-karimi2021algorithmic}{{[}14{]}}.

Efforts so far have largely been directed at improving the quality of
counterfactual explanations within a static context: given some
pre-trained classifier \(M: \mathcal{X} \mapsto \mathcal{Y}\) we are
interested in generating one or multiple meaningful counterfactual
explanations for some individual characterized by \(x_i\). The ability
of counterfactual explanations to handle dynamics like data and model
shifts remains a largely unexplored research challenge at this point
\protect\hyperlink{ref-verma2020counterfactual}{{[}12{]}}. We have been
able to identify only one recent work that considers the implications of
\textbf{exogenous} domain and model shifts in the context of AR
\protect\hyperlink{ref-upadhyay2021towards}{{[}15{]}}. Exogenous shifts
are strictly of external origin. For example, they might stem from data
correction, temporal shifts or geospatial changes
\protect\hyperlink{ref-upadhyay2021towards}{{[}15{]}}. The authors of
\protect\hyperlink{ref-upadhyay2021towards}{{[}15{]}} propose framework
for algorithmic recourse (ROAR) that evidently improves robustness to
such exogenous shifts.

As mentioned earlier, research has so far also generally focused on
generating counterfactuals for single individuals or instances. We have
been able to identify only one existing work that investigates black-box
model behavior towards a group of individuals
\protect\hyperlink{ref-carrizosa2021generating}{{[}16{]}}. The authors
propose an optimization framework that generates collective
counterfactuals. We provide a motivation for doing so from the
perspective of endogenous macrodynamics of algorithmic recourse.

\hypertarget{sec-related-shifts}{%
\subsection{Domain and Model Shifts}\label{sec-related-shifts}}

Much attention has been paid to the detection of dataset shifts --
situations where the distribution of data changes over time. Rabanser et
al.~suggest a framework to detect data drift from a minimal number of
samples through the application of two-sample tests
\protect\hyperlink{ref-rabanser2019failing}{{[}17{]}}. This task is a
generalization of the anomaly detection problem for large datasets,
which aims to answer the question if two sets of samples could have been
generated from the same probability distribution. Numerous approaches to
anomaly detection have been summarized
\protect\hyperlink{ref-chandola2009anomaly}{{[}18{]}}. Another
well-established research topic is that of concept drift -- situations
where external variables influence the patterns between the input and
the output of a model
\protect\hyperlink{ref-widmer1996learning}{{[}19{]}}. For instance, Gama
et al.~offer a review of the adaptive learning techniques which can
handle concept drift \protect\hyperlink{ref-gama2014survey}{{[}20{]}}.
Less previous work is available on the related topic of model drift -
changes in model performance over time. Nelson et al.~review how
resistant different machine learning models are to the model drift
\protect\hyperlink{ref-nelson2015evaluating}{{[}21{]}}. Ackerman et
al.~offer a method to detect changes in model performance when ground
truth is not available
\protect\hyperlink{ref-ackerman2021machine}{{[}22{]}}.

In the context of algorithmic recourse, domain and model shifts were
first brought up by the authors behind ROAR
\protect\hyperlink{ref-upadhyay2021towards}{{[}15{]}}. In their work
they refer to model shifts as simply any perturbation \(\Delta\) to the
parameters of the model in question: \(M\). While this also sets the
baseline for our analysis here, it is worth noting that in
\protect\hyperlink{ref-upadhyay2021towards}{{[}15{]}} these
perturbations are mechanically introduced. In contrast we are interested
in quantifying model shifts that arise endogenously as part of a dynamic
recourse process. In addition to quantifying the magnitude of shifts
\(\Delta\), we aim to also analyse the characteristics of changes to the
model, such as the position of the decision boundary and the overall
decisiveness of the model. We have not been able to identify previous
work on this topic.

\hypertarget{sec-related-benchmark}{%
\subsection{Benchmarking Counterfactual
Generators}\label{sec-related-benchmark}}

Despite the large and growing number of approaches to counterfactual
search there have been surprisingly few benchmark studies that compare
different methodologies. This may be partially due to limited software
availability in this space. Recent work has started to address this gap:
firstly, \protect\hyperlink{ref-de2021framework}{{[}23{]}} run a large
benchmarking study using different algorithmic aproaches and numerous
tabular datasets; secondly,
\protect\hyperlink{ref-pawelczyk2021carla}{{[}4{]}} introduce a Python
framework that can be used to apply and benchmark different
methodologies; finally,
\protect\hyperlink{ref-altmeyer2022CounterfactualExplanations}{{[}6{]}}
provides an extensible, fast and language-agnostic implementation in
Julia. Since the experiments presented here involve extensive
simulations, we have relied on and extended the Julia implementation due
to the associated performance benefits.

\hypertarget{sec-method}{%
\section{Methodology}\label{sec-method}}

In the following we first set out a generalized framework for
gradient-based counterfactual search in Section~\ref{sec-method-general}
to introduce the various counterfactual generators we have chosen to use
in our experiments. We then describe the experimental setup in
Section~\ref{sec-method-experiment} and introduce several evaluation
metrics used to benchmark the different generators.

\hypertarget{sec-method-general}{%
\subsection{A Generalized Framework for Gradient-Based Counterfactual
Search}\label{sec-method-general}}

In this work we have chosen to focus on a number of gradient-based
counterfactual generators to investigate the endogenous dynamics we
introduced in Section~\ref{sec-intro}. Gradient-based counterfactual
search is well-suited for differentiable black-box models like deep
neural networks.

\hypertarget{from-individual-recourse}{%
\subsubsection{From individual recourse
\ldots{}}\label{from-individual-recourse}}

We can restate Equation~\ref{eq-solution} in a more general form that
encompasses most gradient-based approaches to counterfactual search:

\begin{equation}\protect\hypertarget{eq-general}{}{
\begin{aligned}
\mathbf{s}^\prime &= \arg \min_{\mathbf{s}^\prime \in \mathcal{S}} \left\{ \sum_{k=1}^{K} {\ell(M(f(s_k^\prime)),t)}+ \lambda {h(f(s_k^\prime)) }  \right\}
\end{aligned}
}\label{eq-general}\end{equation}

Here \(\mathbf{s}^\prime=\left\{s_k^\prime\right\}_K\) is the stacked
\(K\)-dimensional array of counterfactual states and
\(f: \mathcal{S} \mapsto \mathcal{X}\) maps from the counterfactual
state space to the feature space. In the case of the baseline
counterfactual generator
\protect\hyperlink{ref-wachter2017counterfactual}{{[}5{]}} \(f\) is just
the idendity function and the number of counterfactuals \(K\) is equal
to one. This generator, which we shall refer to as \textbf{Wachter} in
the following, shall serve as the baseline against which all other
gradient-based methodologies will be compared. In particular, we include
include the following generator in our benchmarking exercises: REVISE
\protect\hyperlink{ref-joshi2019towards}{{[}8{]}}, CLUE
\protect\hyperlink{ref-antoran2020getting}{{[}10{]}}, DICE
\protect\hyperlink{ref-mothilal2020explaining}{{[}9{]}} and a greedy
approach that relies on probabilistic models
\protect\hyperlink{ref-schut2021generating}{{[}7{]}}.

Both \textbf{REVISE} and \textbf{CLUE} search counterfactuals in some
latent embedding \(S \subset \mathcal{S}\) instead of the feature space
directly. The latent embedding is learned by a separate generative model
that is tasked with learning the data generating process (DGP) of \(X\).
In this case \(f\) in Equation~\ref{eq-general} corresponds to the
decoder part of the generative model, in other words the deterministic
function that maps back from the latent embedding to the feature space.
Provided the generative model is well-specified, traversing the latent
embedding typically results in realistic and plausible counterfactuals,
because they are implicitly generated by the (learned) DGP
\protect\hyperlink{ref-joshi2019towards}{{[}8{]}}. CLUE distinguishes
itself from REVISE and other counterfactual generators in that it aims
to minimize the predictive uncertainty of the model in question \(M\).
To quantify predictive uncertainty the authors rely on entropy estimates
for probabilistic models. The \textbf{Greedy} approach proposed by
\protect\hyperlink{ref-schut2021generating}{{[}7{]}} also works with the
subclass of models \(\tilde{\mathcal{M}}\subset\mathcal{M}\) that can
produce predictive uncertainty estimates. The authors show that in this
setting the complexity penalty \(h(\cdot)\) in Equation~\ref{eq-general}
is redundant and meaningful counterfactuals can be generated in a fast
and efficient manner through a modified Jacobian-based Saliency Map
Attack (JSMA). Finally, \textbf{DICE} distinguishes itself from all
other generators considered here in that it aims to generate a diverse
set of \(K>1\) counterfactuals. To this end the authors use a complexity
penalty \(h(\mathbf{s}^\prime)\) that favours diverse outcomes, in the
sense that \(s_1, ... , s_K\) look as different from each other as
possible.

Our motivation for including these different generators in our analysis,
is that they all offer slightly different approaches to generate
meaningful counterfactuals for differentiable black-box models. We
hypothesize that generating more \textbf{meaningful} counterfactuals
should mitigate the endogenous dynamics illustrated in
Figure~\ref{fig-poc} in Section~\ref{sec-intro}. This intuition stems
from the underlying idea that more meaningful counterfactuals are
generated by the same or at least a very similar data generating process
as the training data. All else equal, counterfactuals that fulfill this
basic requirement should be less prone to trigger domain and model
shifts.

\hypertarget{towards-collective-recourse}{%
\subsubsection{\ldots{} towards collective
recourse}\label{towards-collective-recourse}}

To explicitly address the notion that individual recourse may affect the
outcome and prospect of other individuals, we propose to extend
Equation~\ref{eq-general} as follows:

\begin{equation}\protect\hypertarget{eq-collective}{}{
\begin{aligned}
\mathbf{s}^\prime &= \arg \min_{\mathbf{s}^\prime \in \mathcal{S}}  \sum_{k=1}^{K} {\ell(M(f(s_k^\prime)),t)} \\ &+ \lambda_1 {h(f(s_k^\prime)) } + \lambda_2 {g(f(s_k^\prime))}  
\end{aligned}
}\label{eq-collective}\end{equation}

Here \(h(f(s_k^\prime))\) denotes the proxy for private costs faced by
the individual as before and \(\lambda_1\) governs to what extent that
private cost ought to be penalised. The newly introduced term
\(g(f(s_k^\prime))\) is meant to capture and address social costs
incurred by the collective of individuals in response to changes in
\(\mathbf{s}^\prime\). The trade-off between private and social costs is
determined by the ratio between \(\lambda_1\) and \(\lambda_2\). As with
individual recourse, the exact choice of \(g(\cdot)\) is not obvious,
nor do we intend to provide a definite answer in this work, if such even
exists. A straight-forward choice simply extends the baseline approach
by \protect\hyperlink{ref-wachter2017counterfactual}{{[}5{]}}: instead
of only penalizing the distance of the individuals' counterfactual to
its factual, we propose penalizing its distance to some sensible point
in the target domain, for example the sample average:
\(\bar{\mathbf{x}}\). For such a recourse objective, higher choices of
\(\lambda_2\) relative to \(\lambda_1\) will lead counterfactuals to
gravitate towards the specified point in the target domain. In the
remainder of this paper we will therefore refer to this approach as
\textbf{Gravitational} generator, when we investigate its potential
usefulness for mitigating endongenous macrodynamics\footnote{Note that
  despite the naming convention our goal here is not to provide yet
  another counterfactual generator, but merely investigate the most
  simple penalty we can think of with respect to its effectiveness.}.

\hypertarget{a-note-on-convergence}{%
\subsubsection{A note on convergence}\label{a-note-on-convergence}}

For this simple mitigating strategy underlying the Gravitational
generator to work as expected, one needs to ensure that counterfactual
search continues, even after a predetermined threshold probability
\(\gamma\) has potentially already been reached.
Figure~\ref{fig-convergence} illustrates this distinction: if one
chooses to terminate search once the desired threshold is reached (left
panel) the gravitational pull towards \(\bar{\mathbf{x}}\) is never
actually satisfied (compare to right panel). More generally, if
convergence is defined simply in terms of flipping the predicted label
with some desired degree of confidence, this corresponds to essentially
ignoring any parts of the counterfactual search objective that do not
involve \(\ell(M(f(s_k^\prime)),t)\) beyond that point. While this may
be appropriate for some applications, in general this seems like an odd
convention. Since we nonetheless seen convergence specified simply in
terms of reaching the threshold probability in some places\footnote{\protect\hyperlink{ref-joshi2019towards}{{[}8{]}}
  define convergence of Algorithm 1 in this way. The implementation of
  \protect\hyperlink{ref-wachter2017counterfactual}{{[}5{]}} in CARLA is
  also defined in this way.}, we thought it worth making this
distinction explicit.

\begin{figure}

{\centering \includegraphics[width=0.45\textwidth,height=\textheight]{www/gravitational_generator_comparison.png}

}

\caption{\label{fig-convergence}Comparison of counterfactual search
outcome with simple (left) and strict convergence (right).}

\end{figure}

\hypertarget{sec-method-experiment}{%
\subsection{Experimental Setup}\label{sec-method-experiment}}

The dynamics illustrated in Figure~\ref{fig-poc} in
Section~\ref{sec-intro} were generated through a simple experiment that
aims to simulate the process of algorithmic recourse in practice. We
begin in the static setting at time time \(t=0\): given some pre-trained
classifier \(M\) we generate recourse for a random batch of \(B\)
individuals in the non-target class. Note that we focus our attention on
classification problems, since classification poses the most common
practical use-case for algorithmic recourse. In order to simulate the
dynamical process we suppose that the model \(M\) is retrained following
the actual implementation of recourse in time \(t=0\). Following the
update to the model, we assume that at time \(t=1\) recourse is
generated for yet another random subset of individuals in the non-target
class. This process is repeated for a number of time periods \(T\). To
get a clean read on endogenous dynamics we keep the total population of
samples closed: we allow existing samples to move from factual to
counterfactual states, but do not allow any entirely new samples to
enter the population. The experimental setup is summarized in Algorithm
\ref{algo-experiment}

\begin{algorithm}
\caption{Experiment}\label{algo-experiment}
\begin{algorithmic}[1]
\Procedure{Experiment}{$M,D,G$}
\State $t\gets 0$
\While{$t<T$}
\State $D_B \subset D$
\State $D_B\gets G(D_B)$ \Comment{Generate counterfactuals.}
\State $M\gets M(D)$ \Comment{Retrain model.}
\EndWhile
\State \textbf{return} $M,D$
\EndProcedure
\end{algorithmic}
\end{algorithm}

A noteworthy practical consideration is the choice of \(T\) and \(B\).
The higher these values, the more factual instances undergo recourse
throughout the entire experiment. Of course, this is likely to lead to
more pronounced domain and model shifts by time \(T\). At the same time,
it is generally improbable that a very large part of the population
would request an explanation of the algorithm's decisions. In our
experiments, we choose the values such that \(T \cdot B\) corresponds to
the application of recourse on \(25-50\%\) of the negative instances
from the initial dataset. As we collect data at each time \(t\), we can
also verify the impact of recourse when it is implemented for a smaller
number of individuals. Using our framework the experiment can be
conducted on an arbitrary number of algorithmic recourse generators. As
all generators make use of the same initial model and initial dataset,
the differences in domain and model shifts observed throughout the
rounds depend solely on the employed generator.

\hypertarget{sec-method-data}{%
\subsection{Data}\label{sec-method-data}}

We have chosen to work with both synthetic and real-world datasets.
Using synthetic data allows us to impose distributional properties that
may affect the resulting recourse dynamics. Following
\protect\hyperlink{ref-upadhyay2021towards}{{[}15{]}} we generate
synthetic data in \(\mathbb{R}^2\) to also allow for a visual
interpretation of the results. Real-world data is used in order to
assess if endogenous dynamics also occur in higher-dimensional settings.

\hypertarget{synthetic-data}{%
\subsubsection{Synthetic data}\label{synthetic-data}}

We use four synthetic binary classification datasets consisting of 1000
samples each.\footnote{To see how the data is generated see here:
  \url{https://github.com/pat-alt/AlgorithmicRecourseDynamics.jl/blob/main/notebooks/synthetic_datasets.ipynb}}
The datasets are presented in Figure~\ref{fig-synthetic-data} (see also
Appendix A for a formal description). Samples from the negative class
are marked in blue while samples of the positive class are marked in
orange.

\begin{figure}

{\centering \includegraphics[width=8cm,height=8cm]{www/placeholder.png}

}

\caption{\label{fig-synthetic-data}PLACEHOLDER: A visualization of the
synthetic classification datasets used in our experiments.}

\end{figure}

Ex-ante we expect to see that Wachter will create a new cluster of
counterfactual instances in the proximity of the initial decision
boundary. Thus, the choice of a black-box model may have an impact on
the paths of the recourse. For generators that use latent space search
(\protect\hyperlink{ref-joshi2019towards}{{[}8{]}},
\protect\hyperlink{ref-antoran2020getting}{{[}10{]}}) or rely on (and
have access to) probabilistic models
(\protect\hyperlink{ref-antoran2020getting}{{[}10{]}},
\protect\hyperlink{ref-schut2021generating}{{[}7{]}}) we expect that
counterfactuals will end up in regions of the target domain that are
densely populated by training samples. Finally, we expect to see the
counterfactuals generated by DiCE to be uniformly spread around the
feature space inside the target class.

\hypertarget{real-world-data}{%
\subsubsection{Real-world data}\label{real-world-data}}

Additionally, we use two real-world datasets from the Finance domain.
Firstly, we use the Give Me Some Credit dataset which was open-sourced
on Kaggle for the task to predict whether a borrower is likely to
experience financial difficulties in the next two years
\protect\hyperlink{ref-gmsc_data}{{[}24{]}}. Originally consisting of
250,000 instances with 11 numerical attributes, the dataset was randomly
undersampled to result in a balanced subsample made up of 3000
individuals. Secondly, we the German Credit dataset which involves the
task of predicting if bank customers are credit-worthy or not
\protect\hyperlink{ref-germancredit1994}{{[}25{]}}. It consists of 700
positive and 300 negative instances charaterized by 7 numerical and 13
categorical attributes. We process the dataset in two ways: (1) the
values of the ``Personal status and sex'' feature are aggregated by the
two represented genders; (2) the most common values are calculated for
all categorical features such that a feature \(x_d\) with the mode
\(\bar{x}_d\) is transformed into a new binary feature
\(\tilde{x}_d=\mathbb{I}_{x_{d,i}>=\bar{x}_d}\). Binarization ensures
that we can use all counterfactual generators in the benchmark.

\hypertarget{sec-method-metrics}{%
\subsection{Evaluation Metrics}\label{sec-method-metrics}}

We formulate two desiderata for the set of metrics used to measure
domain and model shifts induced by recourse. First, the metrics should
be applicable regardless of the dataset or classification technique so
that they allow for the meaningful comparison of the generators in
various scenarios. As the knowledge of the underlying probability
distribution is rarely available, the metrics should be empirical and
non-parametric. This further ensures that we can also measure large
datasets by sampling from the available data. Moreover, while our study
was conducted in a two-class classification setting, our choice of
metrics should remain applicable in the future research on multi- class
recourse problems. Second, the set of metrics should allow to capture
various aspects of the previously mentioned magnitude, path, and tempo
of changes while remaining as small as possible.

\hypertarget{domain-shifts}{%
\subsubsection{Domain Shifts}\label{domain-shifts}}

To quantify the magnitude of domain shifts we rely on an unbiased
estimate of the squared population \textbf{Maximum Mean Discrepancy
(MMD)} given as:

\begin{equation}\protect\hypertarget{eq-mmd}{}{
\begin{aligned}
MMD^2_u[F,{X}^\prime,\tilde{X}^\prime] &= \frac{1}{m(m-1)}\sum_{i=1}^m\sum_{j\neq i}^m k(x_i,x_j) \\ &+ \frac{1}{n(n-1)}\sum_{i=1}^n\sum_{j\neq i}^n k(\tilde{x}_i,\tilde{x}_j) \\ &- \frac{2}{mn}\sum_{i=1}^m\sum_{j=1}^n k(x_i,\tilde{x}_j)
\end{aligned}
}\label{eq-mmd}\end{equation}

where \(\mathcal{F}\) is a unit ball in a Reproducing Kernel Hilbert
Space H {[}27{]}, and \(X\), \(\tilde{X}\) represent independent and
identically distributed samples drawn from probability distributions
\(p\) and \(q\) respectively {[}28{]}. MMD is a measure of the distance
between the kernel mean embeddings of \(p\) and \(q\) in RKHS
\(\mathcal{H}\). An important consideration is the choice of the kernel
function \(k(\cdot,\cdot)\). In our implementation we make use of the
radial basis function (RBF) kernel with a constant length-scale
parameter of \(0.5\). As RBF captures all moments of distributions \(p\)
and \(q\), we have that \(MMD_u^2[F,X,\tilde{X}]=0\) if and only if
\(X=\tilde{X}\).

The evaluation metric in Equation~\ref{eq-mmd} is computed after every
round \(t=1,...,T\) of the experiment. To assess the statistical
significance of the observed shifts under the null hypothesis that
samples \(X\) and \(\tilde{X}\) were drawn from the same probability
distribution we follow
\protect\hyperlink{ref-arcones1992bootstrap}{{[}26{]}}. To that end, we
combine the two samples and generate a large number of permutations of
\(X + \tilde{X}\). Then, we split the permuted data into two new samples
\(X^\prime\) and \(\tilde{X}^\prime\) having the same size as the
original samples. Then under the null hypothesis we should have that
\(MMD_u^2[F,X^\prime,\tilde{X}^\prime]\) be approximately equal to
\(MMD_u^2[F,X,\tilde{X}]\). The corresponding \(p\)-value can then be
calculated by counting how these two quantities are not equal.

We calculate the MMD for both classes individually based on the ground
truth labels, i.e.~the labels that samples were assigned in time
\(t=0\). Throughout our experiments we generally do not expect the
distribution of the negative class to change over time -- application of
recourse reduces the size of this class, but since individuals are
sampled uniformly the distribution should remain unaffected. Conversely,
unless a recourse generator can perfectly replicate the original
probability distribution, we expect the MMD of the positive class to
increase. Thus, when discussing MMD, we generally mean the shift in the
distribution of the positive class.

Finally, \textbf{feature mean and feature standard deviation} are also
calculated to verify how the implementation of recourse impacts every
attribute in the dataset. Although MMD already captures information
about the expected value and variance, we may also be interested in a
more granular look at individual features.

\hypertarget{model-shifts}{%
\subsubsection{Model Shifts}\label{model-shifts}}

As our baseline for quantifying model shifts we measure perturbations to
the model parameters at each point in time \(t\) following
\protect\hyperlink{ref-upadhyay2021towards}{{[}15{]}}. We define
\(\Delta=||\theta_{t+1}-\theta_{t}||^2\), that is the euclidean distance
between the vectors of parameters before and after retraining the model
\(M\). We shall refer to this baseline metric simply as
\textbf{Perturbations}.

We extend the metric in Equation~\ref{eq-mmd} for the purpose of
quantifying model shifts. Specifically, we introduce \textbf{Predicted
Probability MMD (PP MMD)}: instead of applying Equation~\ref{eq-mmd} to
features directly, we apply it to the predicted probabilities assigned
to a set of samples by the model \(M\). If the model shifts, the
probabilities assigned to each sample will change; again, this metric
will equal 0 only if the two classifiers are the same. It is worth
noting that while we apply the technique to samples drawn uniformly from
the dataset, it can also be employed on arbitrary points in the entire
feature space (or a subspace). The latter approach is theoretically more
robust. Unfortunately, in practice this approachs suffers from the curse
of dimensionality, since it becomes increasingly difficult to select
enough points to overcome noise as the dimension \(D\) grows.

As an alternative to PP MMD we use a pseudo-distance for the
\textbf{Disagreement Coefficient} (Disagreement). This metric was
introduced in \protect\hyperlink{ref-hanneke2007bound}{{[}27{]}} and
estimates \(p(M(x) \neq M^\prime(x))\), that is the probability that two
classifiers do not agree on the predicted outcome for a randomly chosen
sample. Thus, it is not relevant whether the classification is correct
according to the ground truth, but only whether the sample lies on the
same side of the two respective decision boundaries. In our context,
this metric quantifies the overlap between the initial model (trained
before the application of recourse) and the updated model. A
Disagreement Coefficient unequal to zero is indicative of a model shift.
The opposite is not true: even if the Disagreement Coefficient is equal
to zero a model shift may still have occured. This is one reason for why
PP MMD is our our preferred metric.

Finally, we introduce \textbf{Decisiveness} as a metric that quantifies
the likelihood that a model assigns a high probability to its
classification of any given sample. We define the metric simply as
\({1\over{N}}\sum_{i=0}^N(\sigma(M(x)) âˆ’ 0.5)^2\) where \(M(x)\) are
predicted logits from a binary classifier and \(\sigma\) denotes the
sigmoid function. This metric provides an unbiased estimate of the
binary classifier's tendency to produce high-confidence predictions in
either one of the two classes. Although the exact values for this metric
are not important for our study, they can be used to detect model
shifts. If decisiveness changes over time, then this is indicative of
the decision boundary moves towards either one of the two classes.

\hypertarget{sec-empirical}{%
\section{Experiments}\label{sec-empirical}}

\hypertarget{sec-discussion}{%
\section{Discussion}\label{sec-discussion}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Shift of focus from individual to group of individuals (related:
  https://www.researchgate.net/publication/353073138\_Generating\_Collective\_Counterfactual\_Explanations\_in\_Score-Based\_Classification\_via\_Mathematical\_Optimization)
\item
  Convergence criterium matters: terminating once threshold probability
  is reached may not be optimal (see e.g.~REVISE)
\item
  Optimizer choice matters: dimensionality is typically low, so no
  obvious benefit to using ADAM.

  \begin{itemize}
  \tightlist
  \item
    This might be better placed in JuliaCon proceedings, perhaps backed
    by small blog post on the matter.
  \end{itemize}
\item
  Mitigating strategy: penaliye distance from centroid.
\end{enumerate}

\hypertarget{sec-limit}{%
\section{Limitations and Future Work}\label{sec-limit}}

While we believe that this work constitutes a valuable starting point
for addressing existing issues in algorithmic recourse from a fresh
perspective, we are aware of several of its limitations. In the
following we highlight some of these limitations and point to avenues
for future research.

\hypertarget{experimental-setup}{%
\subsection{Experimental Setup}\label{experimental-setup}}

The experimental setup proposed here is designed to mimic a real-world
recourse process in a simple fashion. In practice, models are in fact
updated on a regular basis
\protect\hyperlink{ref-upadhyay2021towards}{{[}15{]}}. We also find it
plausible to assume that the implementation of recourse happens
periodically for different individuals, rather that all at once at time
\(t=0\). That being said, our experimental design is a vast
over-simplification of potential real-world scenarios. In practice, any
endogenous shifts that may occur can be expected to be entangled with
exogenous shifts of the nature investigated in
\protect\hyperlink{ref-upadhyay2021towards}{{[}15{]}}. We also make
implicit assumptions about the utility functions of the involved agents
that may well be too simple: individuals seeking recourse are assumed to
always implement the proposed counterfactual explanations; conversely,
the agent in charge of the model \(M\) is assumed to always treat
individuals that have implemented valid recourse as if they were truly
now in the target class. Relating this back to the consumer credit
example, we assume that the would-be borrowers are always willing and
able to implement recourse and the bank is always willing to provide
credit as would-be borrowers move across the decision boundary. In
practice it is doubtful that agents behave according to such simple
rules. Nonetheless, we think that our simple framework offers a starting
point for future work on recourse dynamics (both endogenous and
exogenous dynamics).

\hypertarget{data}{%
\subsection{Data}\label{data}}

Largely in line with the existing literature on algorithmic recourse, we
have limited our analysis of real-world data to three commonly used
benchmark datasets that involve binary prediction tasks. Future work may
benefit from including novel datasets or extending the analysis to
multi-class or regression problems, the latter arguably representing the
most common objective in Finance and Economics. It is also worth
mentioning that the use of real-world datasets considered in this work
is constrained by the fact that at the time of writing
\texttt{CounterfactualExplanations.jl} only supports continuous
features, at least of some of the counterfactual generators considered
here. The fact that we therefore had to discard discrete features led to
relatively poor initial performance of our classifiers in some cases.
While this is indeed a limitation we intend to address in future and
derivative work, our findings with respect to endogenous macrodynamics
do not hinge on strong classifier performance.

\hypertarget{classifiers}{%
\subsection{Classifiers}\label{classifiers}}

For reasons stated earlier we have limited our analysis to
differentiable linear and non-linear classifiers, in particular logistic
regression and deep neural networks. While these sorts of classifiers
have also typically been analyzed in the existing literature on
counterfactual explanations and algorithmic recourse, they represent
only a subset of popular machine learning models employed in practice -
both black-box and glass-box. Despite the success and popularity of deep
learning in the context of high-dimensional data such as image, audio
and video, empirical evidence suggests that other models such as boosted
decision trees may have an edge when it comes to lower-dimensional
tabular datasets, such as the ones considered here
\protect\hyperlink{ref-grinsztajn2022tree}{{[}29{]}}.

\hypertarget{sec-conclusion}{%
\section{Concluding Remarks}\label{sec-conclusion}}

\hypertarget{acknowledgment}{%
\section*{Acknowledgment}\label{acknowledgment}}
\addcontentsline{toc}{section}{Acknowledgment}

P. A. thanks \ldots{}

\pagebreak
\FloatBarrier

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{0}{0}
\leavevmode\vadjust pre{\hypertarget{ref-borch2022machine}{}}%
\CSLLeftMargin{{[}1{]} }%
\CSLRightInline{C. Borch, {``Machine learning, knowledge risk, and
principal-agent problems in automated trading,''} \emph{Technology in
Society}, p. 101852, 2022.}

\leavevmode\vadjust pre{\hypertarget{ref-o2016weapons}{}}%
\CSLLeftMargin{{[}2{]} }%
\CSLRightInline{C. O'neil, \emph{Weapons of math destruction: How big
data increases inequality and threatens democracy}. Crown, 2016.}

\leavevmode\vadjust pre{\hypertarget{ref-rudin2019stop}{}}%
\CSLLeftMargin{{[}3{]} }%
\CSLRightInline{C. Rudin, {``Stop explaining black box machine learning
models for high stakes decisions and use interpretable models
instead,''} \emph{Nature Machine Intelligence}, vol. 1, no. 5, pp.
206--215, 2019.}

\leavevmode\vadjust pre{\hypertarget{ref-pawelczyk2021carla}{}}%
\CSLLeftMargin{{[}4{]} }%
\CSLRightInline{M. Pawelczyk, S. Bielawski, J. van den Heuvel, T.
Richter, and G. Kasneci, {``Carla: A python library to benchmark
algorithmic recourse and counterfactual explanation algorithms,''}
\emph{arXiv preprint arXiv:2108.00783}, 2021.}

\leavevmode\vadjust pre{\hypertarget{ref-wachter2017counterfactual}{}}%
\CSLLeftMargin{{[}5{]} }%
\CSLRightInline{S. Wachter, B. Mittelstadt, and C. Russell,
{``Counterfactual explanations without opening the black box: Automated
decisions and the GDPR,''} \emph{Harv. JL \& Tech.}, vol. 31, p. 841,
2017.}

\leavevmode\vadjust pre{\hypertarget{ref-altmeyer2022CounterfactualExplanations}{}}%
\CSLLeftMargin{{[}6{]} }%
\CSLRightInline{P. Altmeyer, \emph{{CounterfactualExplanations.jl - a
Julia package for Counterfactual Explanations and Algorithmic
Recourse}}. 2022. Available:
\url{https://github.com/pat-alt/CounterfactualExplanations.jl}}

\leavevmode\vadjust pre{\hypertarget{ref-schut2021generating}{}}%
\CSLLeftMargin{{[}7{]} }%
\CSLRightInline{L. Schut \emph{et al.}, {``Generating interpretable
counterfactual explanations by implicit minimisation of epistemic and
aleatoric uncertainties,''} in \emph{International conference on
artificial intelligence and statistics}, 2021, pp. 1756--1764.}

\leavevmode\vadjust pre{\hypertarget{ref-joshi2019towards}{}}%
\CSLLeftMargin{{[}8{]} }%
\CSLRightInline{S. Joshi, O. Koyejo, W. Vijitbenjaronk, B. Kim, and J.
Ghosh, {``Towards realistic individual recourse and actionable
explanations in black-box decision making systems,''} \emph{arXiv
preprint arXiv:1907.09615}, 2019.}

\leavevmode\vadjust pre{\hypertarget{ref-mothilal2020explaining}{}}%
\CSLLeftMargin{{[}9{]} }%
\CSLRightInline{R. K. Mothilal, A. Sharma, and C. Tan, {``Explaining
machine learning classifiers through diverse counterfactual
explanations,''} in \emph{Proceedings of the 2020 conference on
fairness, accountability, and transparency}, 2020, pp. 607--617.}

\leavevmode\vadjust pre{\hypertarget{ref-antoran2020getting}{}}%
\CSLLeftMargin{{[}10{]} }%
\CSLRightInline{J. AntorÃ¡n, U. Bhatt, T. Adel, A. Weller, and J. M.
HernÃ¡ndez-Lobato, {``Getting a clue: A method for explaining uncertainty
estimates,''} \emph{arXiv preprint arXiv:2006.06848}, 2020.}

\leavevmode\vadjust pre{\hypertarget{ref-karimi2020survey}{}}%
\CSLLeftMargin{{[}11{]} }%
\CSLRightInline{A.-H. Karimi, G. Barthe, B. SchÃ¶lkopf, and I. Valera,
{``A survey of algorithmic recourse: Definitions, formulations,
solutions, and prospects,''} \emph{arXiv preprint arXiv:2010.04050},
2020.}

\leavevmode\vadjust pre{\hypertarget{ref-verma2020counterfactual}{}}%
\CSLLeftMargin{{[}12{]} }%
\CSLRightInline{S. Verma, J. Dickerson, and K. Hines, {``Counterfactual
explanations for machine learning: A review,''} \emph{arXiv preprint
arXiv:2010.10596}, 2020.}

\leavevmode\vadjust pre{\hypertarget{ref-ustun2019actionable}{}}%
\CSLLeftMargin{{[}13{]} }%
\CSLRightInline{B. Ustun, A. Spangher, and Y. Liu, {``Actionable
recourse in linear classification,''} in \emph{Proceedings of the
conference on fairness, accountability, and transparency}, 2019, pp.
10--19.}

\leavevmode\vadjust pre{\hypertarget{ref-karimi2021algorithmic}{}}%
\CSLLeftMargin{{[}14{]} }%
\CSLRightInline{A.-H. Karimi, B. SchÃ¶lkopf, and I. Valera,
{``Algorithmic recourse: From counterfactual explanations to
interventions,''} in \emph{Proceedings of the 2021 ACM conference on
fairness, accountability, and transparency}, 2021, pp. 353--362.}

\leavevmode\vadjust pre{\hypertarget{ref-upadhyay2021towards}{}}%
\CSLLeftMargin{{[}15{]} }%
\CSLRightInline{S. Upadhyay, S. Joshi, and H. Lakkaraju, {``Towards
robust and reliable algorithmic recourse,''} \emph{arXiv preprint
arXiv:2102.13620}, 2021.}

\leavevmode\vadjust pre{\hypertarget{ref-carrizosa2021generating}{}}%
\CSLLeftMargin{{[}16{]} }%
\CSLRightInline{E. Carrizosa, J. RamÄ±rez-Ayerbe, and D. Romero,
{``Generating collective counterfactual explanations in score-based
classification via mathematical optimization,''} 2021.}

\leavevmode\vadjust pre{\hypertarget{ref-rabanser2019failing}{}}%
\CSLLeftMargin{{[}17{]} }%
\CSLRightInline{S. Rabanser, S. GÃ¼nnemann, and Z. Lipton, {``Failing
loudly: An empirical study of methods for detecting dataset shift,''}
\emph{Advances in Neural Information Processing Systems}, vol. 32,
2019.}

\leavevmode\vadjust pre{\hypertarget{ref-chandola2009anomaly}{}}%
\CSLLeftMargin{{[}18{]} }%
\CSLRightInline{V. Chandola, A. Banerjee, and V. Kumar, {``Anomaly
detection: A survey,''} \emph{ACM computing surveys (CSUR)}, vol. 41,
no. 3, pp. 1--58, 2009.}

\leavevmode\vadjust pre{\hypertarget{ref-widmer1996learning}{}}%
\CSLLeftMargin{{[}19{]} }%
\CSLRightInline{G. Widmer and M. Kubat, {``Learning in the presence of
concept drift and hidden contexts,''} \emph{Machine learning}, vol. 23,
no. 1, pp. 69--101, 1996.}

\leavevmode\vadjust pre{\hypertarget{ref-gama2014survey}{}}%
\CSLLeftMargin{{[}20{]} }%
\CSLRightInline{J. Gama, I. Å½liobaitÄ—, A. Bifet, M. Pechenizkiy, and A.
Bouchachia, {``A survey on concept drift adaptation,''} \emph{ACM
computing surveys (CSUR)}, vol. 46, no. 4, pp. 1--37, 2014.}

\leavevmode\vadjust pre{\hypertarget{ref-nelson2015evaluating}{}}%
\CSLLeftMargin{{[}21{]} }%
\CSLRightInline{K. Nelson, G. Corbin, M. Anania, M. Kovacs, J. Tobias,
and M. Blowers, {``Evaluating model drift in machine learning
algorithms,''} in \emph{2015 IEEE symposium on computational
intelligence for security and defense applications (CISDA)}, 2015, pp.
1--8.}

\leavevmode\vadjust pre{\hypertarget{ref-ackerman2021machine}{}}%
\CSLLeftMargin{{[}22{]} }%
\CSLRightInline{S. Ackerman, P. Dube, E. Farchi, O. Raz, and M.
Zalmanovici, {``Machine learning model drift detection via weak data
slices,''} in \emph{2021 IEEE/ACM third international workshop on deep
learning for testing and testing for deep learning (DeepTest)}, 2021,
pp. 1--8.}

\leavevmode\vadjust pre{\hypertarget{ref-de2021framework}{}}%
\CSLLeftMargin{{[}23{]} }%
\CSLRightInline{R. M. B. de Oliveira and D. Martens, {``A framework and
benchmarking study for counterfactual generating methods on tabular
data,''} \emph{Applied Sciences}, vol. 11, no. 16, p. 7274, 2021.}

\leavevmode\vadjust pre{\hypertarget{ref-gmsc_data}{}}%
\CSLLeftMargin{{[}24{]} }%
\CSLRightInline{K. Competition, {``Give me some credit, improve on the
state of the art in credit scoring by predicting the probability that
somebody will experience financial distress in the next two years.''}
\url{https://www.kaggle.com/c/GiveMeSomeCredit}}

\leavevmode\vadjust pre{\hypertarget{ref-germancredit1994}{}}%
\CSLLeftMargin{{[}25{]} }%
\CSLRightInline{H. Hoffman, {``German credit data,''} 1994.
\url{https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)}}

\leavevmode\vadjust pre{\hypertarget{ref-arcones1992bootstrap}{}}%
\CSLLeftMargin{{[}26{]} }%
\CSLRightInline{M. A. Arcones and E. Gine, {``On the bootstrap of u and
v statistics,''} \emph{The Annals of Statistics}, pp. 655--674, 1992.}

\leavevmode\vadjust pre{\hypertarget{ref-hanneke2007bound}{}}%
\CSLLeftMargin{{[}27{]} }%
\CSLRightInline{S. Hanneke, {``A bound on the label complexity of
agnostic active learning,''} in \emph{Proceedings of the 24th
international conference on machine learning}, 2007, pp. 353--360.}

\leavevmode\vadjust pre{\hypertarget{ref-borisov2021deep}{}}%
\CSLLeftMargin{{[}28{]} }%
\CSLRightInline{V. Borisov, T. Leemann, K. SeÃŸler, J. Haug, M.
Pawelczyk, and G. Kasneci, {``Deep neural networks and tabular data: A
survey,''} \emph{arXiv preprint arXiv:2110.01889}, 2021.}

\leavevmode\vadjust pre{\hypertarget{ref-grinsztajn2022tree}{}}%
\CSLLeftMargin{{[}29{]} }%
\CSLRightInline{L. Grinsztajn, E. Oyallon, and G. Varoquaux, {``Why do
tree-based models still outperform deep learning on tabular data?''}
\emph{arXiv preprint arXiv:2207.08815}, 2022.}

\end{CSLReferences}

\pagebreak

\hypertarget{tables}{%
\section{Tables}\label{tables}}

\ldots{}

\pagebreak

\hypertarget{figures}{%
\section{Figures}\label{figures}}

\ldots{}

\pagebreak

\hypertarget{code}{%
\section{Code}\label{code}}

\ldots{}



\end{document}
