# Modeling Endogenous Macrodynamics in Algorithmic Recourse {#sec-method-2}

In this ...

## Simulations {#sec-method-2-experiment}

The dynamics illustrated in @fig-poc in @sec-intro were generated through a simple experiment that aims to simulate the process of algorithmic recourse in practice. We begin in the static setting at time time $t=0$: given some pre-trained classifier $M$ we generate recourse for a random batch of $B$ individuals in the non-target class. Note that we focus our attention on classification problems, since classification poses the most common practical use-case for algorithmic recourse. In order to simulate the dynamical process we suppose that the model $M$ is retrained following the actual implementation of recourse in time $t=0$. Following the update to the model, we assume that at time $t=1$ recourse is generated for yet another random subset of individuals in the non-target class. This process is repeated for a number of time periods $T$. To get a clean read on endogenous dynamics we keep the total population of samples closed: we allow existing samples to move from factual to counterfactual states, but do not allow any entirely new samples to enter the population. The experimental setup is summarized in Algorithm \ref{algo-experiment}

\begin{algorithm}
\caption{Experiment}\label{algo-experiment}
\begin{algorithmic}[1]
\Procedure{Experiment}{$M,D,G$}
\State $t\gets 0$
\While{$t<T$}
\State $D_B \subset D$
\State $D_B\gets G(D_B)$ \Comment{Generate counterfactuals.}
\State $M\gets M(D)$ \Comment{Retrain model.}
\EndWhile
\State \textbf{return} $M,D$
\EndProcedure
\end{algorithmic}
\end{algorithm}


A noteworthy practical consideration is the choice of $T$ and $B$. The higher these values, the more factual instances undergo recourse throughout the entire experiment. Of course, this is likely to lead to more pronounced domain and model shifts by time $T$. At the same time, it is generally improbable that a very large part of the population would request an explanation of the algorithm’s decisions. In our experiments, we choose the values such that $T \cdot B$ corresponds to the application of recourse on $25-50\%$ of the negative instances from the initial dataset. As we collect data at each time $t$, we can also verify the impact of recourse when it is implemented for a smaller number of individuals. Using our framework the experiment can be conducted on an arbitrary number of algorithmic recourse generators. As all generators make use of the same initial model and initial dataset, the differences in domain and model shifts observed throughout the rounds depend solely on the employed generator.

## Evaluation Metrics {#sec-method-2-metrics}

We formulate two desiderata for the set of metrics used to measure domain and model shifts induced by recourse. First, the metrics should be applicable regardless of the dataset or classification technique so that they allow for the meaningful comparison of the generators in various scenarios. As the knowledge of the underlying probability distribution is rarely available, the metrics should be empirical and non-parametric. This further ensures that we can also measure large datasets by sampling from the available data. Moreover, while our study was conducted in a two-class classification setting, our choice of metrics should remain applicable in the future research on multi- class recourse problems. Second, the set of metrics should allow to capture various aspects of the previously mentioned magnitude, path, and tempo of changes while remaining as small as possible.

### Domain Shifts

To quantify the magnitude of domain shifts we rely on an unbiased estimate of the squared population **Maximum Mean Discrepancy (MMD)** given as:

$$
\begin{aligned}
MMD^2_u[F,{X}^\prime,\tilde{X}^\prime] &= \frac{1}{m(m-1)}\sum_{i=1}^m\sum_{j\neq i}^m k(x_i,x_j) \\ &+ \frac{1}{n(n-1)}\sum_{i=1}^n\sum_{j\neq i}^n k(\tilde{x}_i,\tilde{x}_j) \\ &- \frac{2}{mn}\sum_{i=1}^m\sum_{j=1}^n k(x_i,\tilde{x}_j)
\end{aligned}
$$ {#eq-mmd}

where $\mathcal{F}$ is a unit ball in a Reproducing Kernel Hilbert Space H [27], and $X$, $\tilde{X}$ represent independent and identically distributed samples drawn from probability distributions $p$ and $q$ respectively [28]. MMD is a measure of the distance between the kernel mean embeddings of $p$ and $q$ in RKHS $\mathcal{H}$. An important consideration is the choice of the kernel function $k(\cdot,\cdot)$. In our implementation we make use of the radial basis function (RBF) kernel with a constant length-scale parameter of $0.5$. As RBF captures all moments of distributions $p$ and $q$, we have that $MMD_u^2[F,X,\tilde{X}]=0$ if and only if $X=\tilde{X}$.

The evaluation metric in @eq-mmd is computed after every round $t=1,...,T$ of the experiment. To assess the statistical significance of the observed shifts under the null hypothesis that samples $X$ and $\tilde{X}$ were drawn from the same probability distribution we follow @arcones1992bootstrap. To that end, we combine the two samples and generate a large number of permutations of $X + \tilde{X}$. Then, we split the permuted data into two new samples $X^\prime$ and $\tilde{X}^\prime$ having the same size as the original samples. Then under the null hypothesis we should have that $MMD_u^2[F,X^\prime,\tilde{X}^\prime]$ be approximately equal to $MMD_u^2[F,X,\tilde{X}]$. The corresponding $p$-value can then be calculated by counting how these two quantities are not equal.

We calculate the MMD for both classes individually based on the ground truth labels, i.e. the labels that samples were assigned in time $t=0$. Throughout our experiments we generally do not expect the distribution of the negative class to change over time – application of recourse reduces the size of this class, but since individuals are sampled uniformly the distribution should remain unaffected. Conversely, unless a recourse generator can perfectly replicate the original probability distribution, we expect the MMD of the positive class to increase. Thus, when discussing MMD, we generally mean the shift in the distribution of the positive class.

Finally, **feature mean and feature standard deviation** are also calculated to verify how the implementation of recourse impacts every attribute in the dataset. Although MMD already captures information about the expected value and variance, we may also be interested in a more granular look at individual features. 

### Model Shifts

As our baseline for quantifying model shifts we measure perturbations to the model parameters at each point in time $t$ following @upadhyay2021towards. We define $\Delta=||\theta_{t+1}-\theta_{t}||^2$, that is the euclidean distance between the vectors of parameters before and after retraining the model $M$. We shall refer to this baseline metric simply as **Perturbations**.

We extend the metric in @eq-mmd for the purpose of quantifying model shifts. Specifically, we introduce **Predicted Probability MMD (PP MMD)**: instead of applying @eq-mmd to features directly, we apply it to the predicted probabilities assigned to a set of samples by the model $M$. If the model shifts, the probabilities assigned to each sample will change; again, this metric will equal 0 only if the two classifiers are the same. It is worth noting that while we apply the technique to samples drawn uniformly from the dataset, it can also be employed on arbitrary points in the entire feature space (or a subspace). The latter approach is theoretically more robust. Unfortunately, in practice this approachs suffers from the curse of dimensionality, since it becomes increasingly difficult to select enough points to overcome noise as the dimension $D$ grows.

As an alternative to PP MMD we use a pseudo-distance for the **Disagreement Coefficient** (Disagreement). This metric was introduced in @hanneke2007bound and estimates $p(M(x) \neq M^\prime(x))$, that is the probability that two classifiers do not agree on the predicted outcome for a randomly chosen sample. Thus, it is not relevant whether the classification is correct according to the ground truth, but only whether the sample lies on the same side of the two respective decision boundaries. In our context, this metric quantifies the overlap between the initial model (trained before the application of recourse) and the updated model. A Disagreement Coefficient unequal to zero is indicative of a model shift. The opposite is not true: even if the Disagreement Coefficient is equal to zero a model shift may still have occured. This is one reason for why PP MMD is our our preferred metric.

Finally, we introduce **Decisiveness** as a metric that quantifies the likelihood that a model assigns a high probability to its classification of any given sample. We define the metric simply as ${1\over{N}}\sum_{i=0}^N(\sigma(M(x)) − 0.5)^2$ where $M(x)$ are predicted logits from a binary classifier and $\sigma$ denotes the sigmoid function. This metric provides an unbiased estimate of the binary classifier's tendency to produce high-confidence predictions in either one of the two classes. Although the exact values for this metric are not important for our study, they can be used to detect model shifts. If decisiveness changes over time, then this is indicative of the decision boundary moves towards either one of the two classes.