# Potential Mitigation Strategies {#mitigate}

Having established in the previous section that endogenous macrodynamics in AR are substantial enough to warrent our attention, in this section we introduce and test a number of potential mitigation strategies. All of them essentially boil down to one simple principle: to avoid substantial domain and model shifts, the generated counterfactuals should comply as much as possible with the true data generating process. This principle is really at the core of Latent Space generators, and hence it is not surprising that we have found these types of generators to perform comparably well in the previous section. But as we have mentioned earlier, generators that rely on separate generative models carry an additional computational burden and - perhaps more importantly - their performance hinges on the performance of said generative models. Fortunately, it turns out that we can use a number of other, much simpler strategies, which we will discuss now. 

## More Conservative Decision Thresholds

The most obvious and trivial mitigation strategy is to simply choose a higher decision threshold $\gamma$. This threshold determines when a proposed counterfactual should be considered as valid. Under $\gamma=0.5$, counterfactuals will end up near the decision boundary by construction. Since this is the region of maximal aleotoric uncertainty, the classifier is bound to be thrown off. By simply setting a more conservative decision threshold, we can avoid this issue to some extent. A potential drawback of this approach is that a classifier with high decisiveness may classify samples with high confidence even far away from the training data. 

## Classifier Preserving ROAR

Another potential strategy draws inspiration from ROAR @upadhyay2021towards: to preserve the classifier, we propose to simply explicitly penalize the loss it incurs when evaluated on the counterfactual $x^\prime$ at given parameter values. Recall that $\text{extcost}(\cdot)$ denotes what we had defined as the external cost in Equation \@ref(eq:collective). Then formally we let

\begin{equation}
\begin{aligned}
\text{extcost}(f(s_k^\prime)) = l(M(f(s_k^\prime)),y^\prime) (\#eq:clap)
\end{aligned}
\end{equation} 

for each counterfactual $k$ where $l$ denotes the loss function used to train $M$. This approach, which we shall refer to as **ClapROAR**, is based on the intuition that (endogenous) model shifts will be triggered by counterfactuals that increase classifier loss. It is closely linked to the idea of choosing higher decision threshold, but likely better at avoiding the potential pitfalls associated with highly decisive classifiers. It also makes the private vs. external cost trade-off more explicit and hence manageable.

## Gravitational Counterfactual Explanations 

Yet another strategy simply extends Wachter as follows: instead of only penalizing the distance of the individuals' counterfactual to its factual, we propose penalizing its distance to some sensible point in the target domain, for example the sample average: $\bar{x}$:

\begin{equation}
\begin{aligned}
\text{extcost}(f(s_k^\prime)) = \text{dist}(f(s_k^\prime),\bar{x})  (\#eq:grav)
\end{aligned}
\end{equation}

Once again we can putting this in the context of Equation \@ref(eq:collective), the former penalty can be thought of here as the private cost incurred by the individual, while the latter reflects the external cost incurred by other individuals. Higher choices of $\lambda_2$ relative to $\lambda_1$ will lead counterfactuals to gravitate towards the specified point $\bar{x}$ in the target domain. In the remainder of this paper we will therefore refer to this approach as **Gravitational** generator, when we investigate its potential usefulness for mitigating endongenous macrodynamics^[Note that despite the naming convention our goal here is not to provide yet another counterfactual generator, but merely investigate the most simple penalty we can think of with respect to its effectiveness.]. 

Figure \@ref(fig:mitigation) shows an illustrative example that demonstrates the differences in counterfactual outcomes when using the various mitigation strategies compared to the baseline approach, that is Wachter with $\gamma=0.5$: choosing a higher decision threshold pushes the counterfactual a little further into the target domain; this effect is even stronger for ClapROAR; finally, using the Gravitational generator the counterfactual ends up all the way inside the target domain in the neighbourhood of $\bar{x}$^[In order for the Graviational generator and ClapROAR to work as expected, one needs to ensure that counterfactual search continues, independent of the threshold probability $\gamma$.].

```{julia, eval=FALSE, echo=FALSE}
using Random
Random.seed!(2022)

# Data:
using MLJ
N = 1000
X, ys = make_blobs(N, 2; centers=2, as_table=false, center_box=(-5 => 5), cluster_std=0.5)
ys .= ys.==2
X = X'
xs = Flux.unstack(X,2)
data = zip(xs,ys)
counterfactual_data = CounterfactualData(X,ys')

# Models:
using AlgorithmicRecourseDynamics
M = AlgorithmicRecourseDynamics.Models.FluxModel(counterfactual_data)
M = AlgorithmicRecourseDynamics.Models.train(M, counterfactual_data)

# Generators:
generators = Dict(
    "Generic (γ=0.5)" => GenericGenerator(decision_threshold=0.5),
    "Generic (γ=0.9)" => GenericGenerator(decision_threshold=0.9),
    "Gravitational" => GravitationalGenerator(),
    "ClapROAR" => ClapROARGenerator()
)

# Counterfactuals
x = select_factual(counterfactual_data, rand(1:size(X)[2])) 
y = round(probs(M, x)[1])
target = ifelse(y==1.0,0.0,1.0) # opposite label as target
T = 50
counterfactuals = Dict([name => generate_counterfactual(x, target, counterfactual_data, M, gen; T=T, latent_space=false) for (name, gen) in generators])

# Plots:
plts = []
for (name,ce) ∈ counterfactuals
    plt = plot(ce; title=name, colorbar=false, ticks = false, legend=false)
    plts = vcat(plts..., plt)
end
plt = plot(plts..., size=(750,200), layout=(1,4))
savefi\text{extcost}(plt, "dev/paper/www/mitigation.png")
```


```{r mitigation, fig.cap="Illustrative example demonstrating the properties of the various mitigation strategies."}
knitr::include_graphics("www/mitigation.png")
```

Our findings indicate that all three mitigation strategies are at least at par with Latent Space generators with respect to their effectiveness at mitigating domain and model shifts. Figure \@ref(fig:mitigate-results) presents a subset of the evaluation metrics for our synthetic data with overlapping classes. The top row in Figure \@ref(fig:mitigate-results) indicates that while domain shifts are of roughly the same magnitude for both Wachter and Latent Space generators, our proposed strategies effectively mitigate these shifts. ClapROAR appears to be particularly effective, which is positively surprising, since it is designed to explicitly address model shifts, not domain shifts. As evident from the middle row in Figure \@ref(fig:mitigate-results) model shifts can also be reduced: for the deep ensemble Latent Space search yields results that are at par with the mitigation strategies, while for both the simple MLP and logistic regression our simple strategies are actually more effective. The same overall pattern can be observed for out-of-sample model performance. With respect to the other synthetic datasets we found the following: for the Moons dataset the emerging patterns are largely the same, but the estimated model shifts are insignificant as noted earlier; the same holds for the Circles dataset, but there is no significant reduction in model performance for our neural networks; in the case of linearly separable data we find the Gravitational generator to be most effective at mitigating shifts. 

```{r mitigate-results, fig.cap="The differences in counterfactual outcomes when using the various mitigation strategies compared to the baseline approach, that is Wachter with $\\gamma=0.5$. Results for synthetic data with overlapping classes. The shown model MMD (PP MMD) was computed over a meshgrid of points. Error bars indicate the standard deviation across folds."}
knitr::include_graphics("www/mitigation_synthetic_results.png")
```

An interesting finding is also that the proposed strategies can have a complementary effect when used in combination with Latent Space generators. In experiments we conducted on the synthetic data, the benefits of Latent Space generators were exacerbated further when using a more conservative threshold or combining it with the penalties underlying Gravitational and ClapROAR. A snapshot of the results is shown in Figure \@ref(fig:mitigate-latent-results). 


```{r mitigate-latent-results, fig.cap="Combinining various mitigation strategies with Latent Space search. Results for synthetic data with overlapping classes. The shown model MMD (PP MMD) was computed over a meshgrid of points. Error bars indicate the standard deviation across folds."}
knitr::include_graphics("www/mitigation_synthetic_latent_results.png")
```
