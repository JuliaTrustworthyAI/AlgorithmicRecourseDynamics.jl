---
title: Experiments with Synthetic Data
jupyter: julia-1.7
---

In this notebook we will use toy data to see how endogenous domain shifts and the resulting model shifts can have implications on the validity and cost of algorithmic recourse.

```{julia}
include("src/AlgorithmicRecourseDynamics.jl")
using CounterfactualExplanations, Flux, Plots, PlotThemes, Random, LaplaceRedux, LinearAlgebra
theme(:wong)
output_path = output_dir("synthetic")
www_path = www_dir("synthetic")
```

## Classifiers

```{julia}
Random.seed!(123)
using MLJ
N = 1000
X, ys = make_blobs(N, 2; centers=2, as_table=false, center_box=(-2 => 2), cluster_std=0.1)
ys .= ys.==2
X = X'
xs = Flux.unstack(X,2)
data = zip(xs,ys)
plt = plot()
plot_data!(plt,X',ys)
```

### Logistic Regression

```{julia}
nn = Chain(Dense(2,1))
loss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y) 
```

```{julia}
using Flux.Optimise: update!, ADAM
opt = ADAM()
epochs = 10
function _forward_nn(nn, data, epochs)
    for epoch = 1:epochs
        for d in data
            gs = gradient(Flux.params(nn)) do
            l = loss(d...)
            end
            update!(opt, Flux.params(nn), gs)
        end
    end
    return nn
end
_forward_nn(nn, data, epochs)
```

```{julia}
using CounterfactualExplanations.Models
import CounterfactualExplanations.Models: logits, probs # import functions in order to extend
import Main.Models: retrain

# Step 1)
struct NeuralNetwork <: Models.AbstractFittedModel
    model::Any
end

# Step 2)
logits(M::NeuralNetwork, X::AbstractArray) = M.model(X)
probs(M::NeuralNetwork, X::AbstractArray)= σ.(logits(M, X))

# Step 3)
function retrain(M::NeuralNetwork, data; n_epochs=10, τ=1.0) 
    nn = M.model
    nn = _forward_nn(nn, data, n_epochs)
    M = NeuralNetwork(nn)
    return M
end

M = NeuralNetwork(nn)
```

```{julia}
#| echo: false
# Plot the posterior distribution with a contour plot.
plt_nn = plot_contour(X',ys,M;zoom=0);
savefig(plt_nn, joinpath(www_path, "nn_contour.png"))
```

![](www/nn_contour.png)

### Laplace Redux

```{julia}
λ = 0.1
la = Laplace(nn, λ=λ)
BayesLaplace.fit!(la, data)
```

```{julia}
# Step 1)
struct LaplaceNeuralNetwork <: Models.AbstractFittedModel
    la::BayesLaplace.Laplace
end

# Step 2)
logits(M::LaplaceNeuralNetwork, X::AbstractArray) = M.la.model(X)
probs(M::LaplaceNeuralNetwork, X::AbstractArray)= BayesLaplace.predict(M.la, X)

# Step 3)
function retrain(M::LaplaceNeuralNetwork, data; n_epochs=10, τ=1.0) 
    nn = M.la.model
    nn = _forward_nn(nn, data, n_epochs)
    la = Laplace(nn, λ=λ)
    BayesLaplace.fit!(la, data)
    M = LaplaceNeuralNetwork(la)
    return M
end

Mᴸ = LaplaceNeuralNetwork(la)
```

```{julia}
#| echo: false
# Plot the posterior distribution with a contour plot.
plt = plot_contour(X',ys,Mᴸ;zoom=0)
savefig(plt, joinpath(www_path, "la_contour.png"))
```

![](www/la_contour.png)

## Single Round

```{julia}
# Models:
models = (Bayesian=Mᴸ, Plugin=M)
# Generators:
generators = (Bayesian=GreedyGenerator(), Plugin=GenericGenerator(ϵ=0.01))
```

### Generate counterfactual

```{julia}
cb = false
γ = 0.75
mod = Mᴸ
gen = generators[:Plugin]
plt_original = plt_nn = plot_contour(X',ys,mod;zoom=0,colorbar=cb,title="(a)")
```

```{julia}
counterfactual_data = CounterfactualData(X,ys')
μ = 0.10
Random.seed!(123)
candidates = findall(ys.==0)
chosen = rand(candidates, Int(round(μ*length(candidates))))
X′ = copy(X)
y′ = copy(ys)
using CounterfactualExplanations.Counterfactuals: counterfactual, counterfactual_label
for i in chosen
    x = X[:,i]
    outcome = generate_counterfactual(x, 1, counterfactual_data, mod, gen,γ=γ)
    X′[:,i] = counterfactual(outcome)
    y′[i] = counterfactual_label(outcome)
end
plt_single = plot_contour(X′',y′,mod;zoom=0,colorbar=cb,title="(b)")
```

### Retrain

```{julia}
xs′ = Flux.unstack(X′,2)
data = zip(xs′,y′)
mod = retrain(mod, data)
plt_single_retrained = plot_contour(X′',y′,mod;zoom=0,colorbar=cb,title="(c)")
```

### Repeat

```{julia}
i = 2
while i <= 10
    counterfactual_data = CounterfactualData(X′,y′')
    Random.seed!(123)
    candidates = findall(y′.==0)
    chosen = rand(candidates, Int(round(μ*length(candidates))))
    xs′ = Flux.unstack(X′,2)
    data = zip(xs′,y′)
    mod = retrain(mod, data)
    for i in chosen
        x = X′[:,i]
        outcome = generate_counterfactual(x, 1, counterfactual_data, mod, gen; γ=γ)
        X′[:,i] = counterfactual(outcome)
        y′[i] = counterfactual_label(outcome)
    end
    i += 1
end
plt_single_repeat = plot_contour(X′',y′,mod;zoom=0,colorbar=cb,title="(d)")
```

```{julia}
plt = plot(plt_original, plt_single, plt_single_retrained, plt_single_repeat, layout=(1,4), legend=false, axis=nothing, size=(600,165))
savefig(plt, joinpath(www_path, "poc.png"))
```

## Experiments

```{julia}
# Variables:
μ = [0.05,0.1,0.25]
γ = [0.50,0.75,0.9]
grid_ = Experiments.GridVariables(μ, γ)
n_rounds = 10
n_folds = 5
target = 1.0
T = 1000
```

```{julia}
# Experiments:
experiment = Experiments.Experiment(X,ys,models[:Plugin],target,grid_,n_rounds)
experiment_plugin = Experiments.Experiment(X,ys,models[:Plugin],target,grid_,n_rounds)
experiments = (experiment, experiment_plugin)
```

```{julia}
#| eval: false
using Random, CSV, DataFrames
Random.seed!(1234)
for j in 1:length(generators)
    outcome, path = Experiments.run_experiment(experiments[j], generators[j], n_folds, T=T, store_path=true)
    Experiments.save_path(joinpath(output_path, string(keys(generators)[j])),path)
    CSV.write(joinpath(output_path, string(keys(generators)[j]) * ".csv"), DataFrame(outcome))
end
```

```{julia}
paths = [Experiments.load_path(joinpath(output_path, string(keys(generators)[j]))) for j ∈ 1:length(generators)]
```

```{julia}
function plot_state(path, t; γ=0.75, μ = 0.05, k = 1, length_out=500, size=(150,150), title="")
    chosen = map(p -> p.γ == γ && p.μ == μ && p.t==t && p.k==k, path)
    path = path[chosen][1]
    X = path.X̲'
    y = vec(path.y̲')
    plt = plot_contour(X,y,path.𝑴,length_out=length_out,colorbar=cb,title=title)
    plt = plot(plt, axis=nothing, size=size, legend=false)
end
plot_state(paths[2], 1)
```

```{julia}
#| eval: false
using Main.Experiments: plot_path
paths = [Experiments.load_path(joinpath(output_path, string(keys(generators)[j]))) for j ∈ 1:length(generators)]
anim = Experiments.plot_path(paths[1],Γ=γ,𝙈=μ)
gif(anim,joinpath(www_path,"bayesian.gif"), fps=2)
anim = Experiments.plot_path(paths[2],Γ=γ,𝙈=μ)
gif(anim,joinpath(www_path,"plugin.gif"), fps=2)
```

For the Bayesian classifier with greedy recourse...

![](www/synthetic/bayesian.gif)

For the non-Bayesian classifier with generic recourse...

![](www/synthetic/plugin.gif)

```{julia}
using Main.Experiments: prepare_results, plot_results
validity = DataFrame()
cost = DataFrame()
files = (Bayesian=joinpath(output_path, "Bayesian.csv"), Plugin=joinpath(output_path, "Plugin.csv"))
results = map(file -> CSV.read(file, DataFrame), files)
p_val, p_cost = plot_results(results);
```

```{julia}
p_val
```

```{julia}
p_cost
```

## Neural network

```{julia}
using Flux
data = Models.prepare_data(X', y')
nn = Models.build_model(input_dim=size(X')[1], n_hidden=100)
loss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y);
```

Training a single neural network...

```{julia}
using BSON
run = false
opt = ADAM()
if run
  # Train model:
  using Flux.Optimise: update!, ADAM
  using Statistics, StatsBase
  epochs = 10
  avg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))
  accuracy(data) = sum(map(d ->round.(Flux.σ.(nn(d[1]))) .== d[2], data))[1]/length(data)

  using Plots
  anim = Animation()
  avg_l = [avg_loss(data)]
  p1 = scatter(ylim=(0,avg_l[1]), xlim=(0,epochs), legend=false, xlab="Epoch", title="Average loss")
  acc = [accuracy(data)]
  p2 = scatter(ylim=(0.5,1), xlim=(0,epochs), legend=false, xlab="Epoch", title="Accuracy")
  
  τ = 1.1
  stopping_criterium_reached = accuracy(data) >= τ
  epoch = 1

  while epoch <= epochs && !stopping_criterium_reached
    for d in data
      gs = gradient(Flux.Flux.params(nn)) do
        l = loss(d...)
      end
      update!(opt, Flux.Flux.params(nn), gs)
    end
    avg_l = vcat(avg_l,avg_loss(data))
    plot!(p1, [0:epoch], avg_l, color=1)
    scatter!(p1, [0:epoch], avg_l, color=1)
    acc = vcat(acc,accuracy(data))
    plot!(p2, [0:epoch], acc, color=1)
    scatter!(p2, [0:epoch], acc, color=1)
    plt=plot(p1,p2, size=(600,300))
    frame(anim, plt)

    # Check if desired accuracy reached:
    stopping_criterium_reached = accuracy(data) >= τ

    epoch += 1

  end

  gif(anim, joinpath(www_path
, "single_nn.gif"), fps=10)

  BSON.@save joinpath(output_path
, "nn.bson") nn

end
```

```{julia}
using BSON: @load
@load joinpath(output_path, "nn.bson") nn
Mₙₙ = Models.FittedNeuralNet(nn, opt, loss);
```

Training a deep ensemble...

```{julia}
opt = ADAM()
loss_type = :logitbinarycrossentropy
run = false
if run
    K = 50
    ensemble = Models.build_ensemble(K,kw=(input_dim=size(X')[1], n_hidden=100));
    ensemble, anim = Models.forward(ensemble, data, opt, n_epochs=10, plot_every=10, loss_type=loss_type, τ=1.1); # fit the ensemble
    Models.save_ensemble(ensemble, root=joinpath(output_path
, "ensemble")) # save to disk
    gif(anim, joinpath(www_path
, "ensemble_loss.gif"), fps=25);
end
```

```{julia}
ensemble = Models.load_ensemble(root=joinpath(output_path, "ensemble"))
M = Models.FittedEnsemble(ensemble, opt, loss_type)
models = (Bayesian=M, Plugin=Mₙₙ);
```

```{julia}
using Main.Experiments: plot_contour
p1 = plot_contour(X,y,models[1],title="Bayesian")
p2 = plot_contour(X,y,models[2],title="Plugin")
plot(p1,p2,size=(1000,400))
```

```{julia}
# Experiments:
experiment = Experiments.Experiment(X',y',models[:Plugin],target,grid_,n_rounds)
experiment_plugin = Experiments.Experiment(X',y',models[:Plugin],target,grid_,n_rounds)
experiments = (experiment, experiment_plugin);
```

```{julia}
run = true
if run
    using Random
    Random.seed!(1234)
    for j in 1:length(generators)
        outcome, path = Experiments.run_experiment(experiments[j], generators[j], n_folds, T=T, store_path=true)
        Experiments.save_path(joinpath(output_path
    , string(keys(generators)[j])),path)
        CSV.write(joinpath(output_path
    , string(keys(generators)[j]) * "_deep.csv"), DataFrame(outcome))
    end
end
```

