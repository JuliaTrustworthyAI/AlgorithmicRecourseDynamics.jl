---
title: Experiments with Synthetic Data
jupyter: julia-1.7
---

```{julia}
using Pkg; Pkg.activate("dev")
```


In this notebook we will use toy data to see how endogenous domain shifts and the resulting model shifts can have implications on the validity and cost of algorithmic recourse.

```{julia}
include("dev/utils.jl")
using AlgorithmicRecourseDynamics
using CounterfactualExplanations, Flux, Plots, PlotThemes, Random, LaplaceRedux, LinearAlgebra
theme(:wong)
output_path = output_dir("synthetic")
www_path = www_dir("synthetic")
```

## Classifiers

```{julia}
using MLJ
N = 1000
X, ys = make_blobs(N, 2; centers=2, as_table=false, center_box=(-2 => 2), cluster_std=0.1)
ys .= ys.==2
X = X'
xs = Flux.unstack(X,2)
data = zip(xs,ys)
counterfactual_data = CounterfactualData(X,ys')
```

### Logistic Regression

```{julia}
using Flux
model = Chain(Dense(2,1))
M = FluxModel(model)
```


```{julia}
n_epochs = 10
using AlgorithmicRecourseDynamics.Models
M = Models.train(M,counterfactual_data; n_epochs=n_epochs)
```

```{julia}
plt_nn = plot(M,counterfactual_data;zoom=0)
savefig(plt_nn, joinpath(www_path, "nn_contour.png"))
```

![](../artifacts/upload/www/synthetic/nn_contour.png)

### Laplace Redux

```{julia}
Î» = 0.1
model_laplace = Laplace(M.model, Î»=Î»)
LaplaceRedux.fit!(model_laplace, data)
```

```{julia}
M_laplace = Models.LaplaceModel(model_laplace)
```

```{julia}
plt_laplace = plot(M_laplace, counterfactual_data;zoom=0)
savefig(plt_laplace, joinpath(www_path, "la_contour.png"))
```

![](../artifacts/upload/www/synthetic/la_contour.png)

## Single Round

```{julia}
# Models:
models = (Bayesian=M_laplace, Plugin=M)
# Generators:
generators = (
    Greedy=GreedyGenerator(loss=:logitbinarycrossentropy), 
    Generic=GenericGenerator(loss=:logitbinarycrossentropy),
    REVISE=REVISEGenerator(loss=:logitbinarycrossentropy)
)
```

### Generate counterfactual

```{julia}
cb = false
Î³ = 0.75
mod = M_laplace
gen = generators[:Greedy]
plt_original = plot(mod,counterfactual_data;zoom=0,colorbar=false,title="(a)")
```

```{julia}
Î¼ = 0.10
candidates = findall(ys.==0)
chosen = rand(candidates, Int(round(Î¼*length(candidates))))
Xâ€² = copy(X)
yâ€² = copy(ys)
using CounterfactualExplanations.Counterfactuals: counterfactual, counterfactual_label
for i in chosen
    x = X[:,i]
    outcome = generate_counterfactual(x, 1, counterfactual_data, mod, gen,Î³=Î³)
    Xâ€²[:,i] = counterfactual(outcome)
    yâ€²[i] = first(counterfactual_label(outcome))
end
counterfactual_dataâ€² = CounterfactualData(Xâ€²,yâ€²')
plt_single = plot(mod,counterfactual_dataâ€²;zoom=0,colorbar=false,title="(b)")
```

### Retrain

```{julia}
mod = Models.train(mod, counterfactual_dataâ€²; n_epochs=n_epochs)
plt_single_retrained = plot(mod,counterfactual_dataâ€²;zoom=0,colorbar=false,title="(c)")
```

### Repeat

```{julia}
i = 2
while i <= 10
    counterfactual_dataâ€² = CounterfactualData(Xâ€²,yâ€²')
    candidates = findall(yâ€².==0)
    chosen = rand(candidates, Int(round(Î¼*length(candidates))))
    Hâ‚€ = mod.model.H # use posterior as new prior
    mod = Models.train(mod, counterfactual_dataâ€²; n_epochs=n_epochs, Hâ‚€=Hâ‚€)
    for i in chosen
        x = Xâ€²[:,i]
        outcome = generate_counterfactual(x, 1, counterfactual_data, mod, gen; Î³=Î³)
        Xâ€²[:,i] = counterfactual(outcome)
        yâ€²[i] = first(counterfactual_label(outcome))
    end
    i += 1
end
plt_single_repeat = plot(mod,counterfactual_dataâ€²;zoom=0,colorbar=false,title="(d)")
```

```{julia}
plt = plot(plt_original, plt_single, plt_single_retrained, plt_single_repeat, layout=(1,4), legend=false, axis=nothing, size=(600,165))
savefig(plt, joinpath(www_path, "poc.png"))
```

![](../artifacts/upload/www/synthetic/poc.png)

## Experiments

```{julia}
using AlgorithmicRecourseDynamics.Experiments
using AlgorithmicRecourseDynamics.Experiments: Experiment
target = 1.0
experiment = Experiment(counterfactual_data, target, models, generators)
```

```{julia}
# Variables:
Î¼ = 0.1
Î³ = 0.75
n_rounds = 10
n_folds = 5
T = 1000
```

```{julia}
run_experiment(experiment; n_rounds=n_rounds, Î¼=Î¼, n_folds=n_folds, Î³=Î³, T=T)
```

```{julia}
# Experiments:
experiment = Experiments.Experiment(X,ys,models[:Plugin],target,grid_,n_rounds)
experiment_plugin = Experiments.Experiment(X,ys,models[:Plugin],target,grid_,n_rounds)
experiments = (experiment, experiment_plugin)
```

```{julia}
#| eval: false
using Random, CSV, DataFrames
Random.seed!(1234)
for j in 1:length(generators)
    outcome, path = Experiments.run_experiment(experiments[j], generators[j], n_folds, T=T, store_path=true)
    Experiments.save_path(joinpath(output_path, string(keys(generators)[j])),path)
    CSV.write(joinpath(output_path, string(keys(generators)[j]) * ".csv"), DataFrame(outcome))
end
```

```{julia}
paths = [Experiments.load_path(joinpath(output_path, string(keys(generators)[j]))) for j âˆˆ 1:length(generators)]
```

```{julia}
function plot_state(path, t; Î³=0.75, Î¼ = 0.05, k = 1, length_out=500, size=(150,150), title="")
    chosen = map(p -> p.Î³ == Î³ && p.Î¼ == Î¼ && p.t==t && p.k==k, path)
    path = path[chosen][1]
    X = path.XÌ²'
    y = vec(path.yÌ²')
    plt = plot_contour(X,y,path.M,length_out=length_out,colorbar=cb,title=title)
    plt = plot(plt, axis=nothing, size=size, legend=false)
end
plot_state(paths[2], 1)
```

```{julia}
#| eval: false
using Main.Experiments: plot_path
paths = [Experiments.load_path(joinpath(output_path, string(keys(generators)[j]))) for j âˆˆ 1:length(generators)]
anim = Experiments.plot_path(paths[1],Î“=Î³,ð™ˆ=Î¼)
gif(anim,joinpath(www_path,"bayesian.gif"), fps=2)
anim = Experiments.plot_path(paths[2],Î“=Î³,ð™ˆ=Î¼)
gif(anim,joinpath(www_path,"plugin.gif"), fps=2)
```

For the Bayesian classifier with greedy recourse...

![](www/synthetic/bayesian.gif)

For the non-Bayesian classifier with generic recourse...

![](www/synthetic/plugin.gif)

```{julia}
using Main.Experiments: prepare_results, plot_results
validity = DataFrame()
cost = DataFrame()
files = (Bayesian=joinpath(output_path, "Bayesian.csv"), Plugin=joinpath(output_path, "Plugin.csv"))
results = map(file -> CSV.read(file, DataFrame), files)
p_val, p_cost = plot_results(results);
```

```{julia}
p_val
```

```{julia}
p_cost
```

## Neural network

```{julia}
using Flux
data = Models.prepare_data(X', y')
nn = Models.build_model(input_dim=size(X')[1], n_hidden=100)
loss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y);
```

Training a single neural network...

```{julia}
using BSON
run = false
opt = ADAM()
if run
  # Train model:
  using Flux.Optimise: update!, ADAM
  using Statistics, StatsBase
  epochs = 10
  avg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))
  accuracy(data) = sum(map(d ->round.(Flux.Ïƒ.(nn(d[1]))) .== d[2], data))[1]/length(data)

  using Plots
  anim = Animation()
  avg_l = [avg_loss(data)]
  p1 = scatter(ylim=(0,avg_l[1]), xlim=(0,epochs), legend=false, xlab="Epoch", title="Average loss")
  acc = [accuracy(data)]
  p2 = scatter(ylim=(0.5,1), xlim=(0,epochs), legend=false, xlab="Epoch", title="Accuracy")
  
  Ï„ = 1.1
  stopping_criterium_reached = accuracy(data) >= Ï„
  epoch = 1

  while epoch <= epochs && !stopping_criterium_reached
    for d in data
      gs = gradient(Flux.Flux.params(nn)) do
        l = loss(d...)
      end
      update!(opt, Flux.Flux.params(nn), gs)
    end
    avg_l = vcat(avg_l,avg_loss(data))
    plot!(p1, [0:epoch], avg_l, color=1)
    scatter!(p1, [0:epoch], avg_l, color=1)
    acc = vcat(acc,accuracy(data))
    plot!(p2, [0:epoch], acc, color=1)
    scatter!(p2, [0:epoch], acc, color=1)
    plt=plot(p1,p2, size=(600,300))
    frame(anim, plt)

    # Check if desired accuracy reached:
    stopping_criterium_reached = accuracy(data) >= Ï„

    epoch += 1

  end

  gif(anim, joinpath(www_path
, "single_nn.gif"), fps=10)

  BSON.@save joinpath(output_path
, "nn.bson") nn

end
```

```{julia}
using BSON: @load
@load joinpath(output_path, "nn.bson") nn
Mâ‚™â‚™ = Models.FittedNeuralNet(nn, opt, loss);
```

Training a deep ensemble...

```{julia}
opt = ADAM()
loss_type = :logitbinarycrossentropy
run = false
if run
    K = 50
    ensemble = Models.build_ensemble(K,kw=(input_dim=size(X')[1], n_hidden=100));
    ensemble, anim = Models.forward(ensemble, data, opt, n_epochs=10, plot_every=10, loss_type=loss_type, Ï„=1.1); # fit the ensemble
    Models.save_ensemble(ensemble, root=joinpath(output_path
, "ensemble")) # save to disk
    gif(anim, joinpath(www_path
, "ensemble_loss.gif"), fps=25);
end
```

```{julia}
ensemble = Models.load_ensemble(root=joinpath(output_path, "ensemble"))
M = Models.FittedEnsemble(ensemble, opt, loss_type)
models = (Bayesian=M, Plugin=Mâ‚™â‚™);
```

```{julia}
using Main.Experiments: plot_contour
p1 = plot_contour(X,y,models[1],title="Bayesian")
p2 = plot_contour(X,y,models[2],title="Plugin")
plot(p1,p2,size=(1000,400))
```

```{julia}
# Experiments:
experiment = Experiments.Experiment(X',y',models[:Plugin],target,grid_,n_rounds)
experiment_plugin = Experiments.Experiment(X',y',models[:Plugin],target,grid_,n_rounds)
experiments = (experiment, experiment_plugin);
```

```{julia}
run = true
if run
    using Random
    Random.seed!(1234)
    for j in 1:length(generators)
        outcome, path = Experiments.run_experiment(experiments[j], generators[j], n_folds, T=T, store_path=true)
        Experiments.save_path(joinpath(output_path
    , string(keys(generators)[j])),path)
        CSV.write(joinpath(output_path
    , string(keys(generators)[j]) * "_deep.csv"), DataFrame(outcome))
    end
end
```

