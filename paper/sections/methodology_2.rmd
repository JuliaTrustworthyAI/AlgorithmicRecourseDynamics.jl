# Modeling Endogenous Macrodynamics in Algorithmic Recourse {#method-2}

In the following we describe the framework we propose for modeling and analysing endogenous macrodynamics in Algorithmic Recourse. We first describe the basic simulations that were generated to produce the findings in this work and also constitute the core of `AlgorithmicRecourseDynamics.jl` - the Julia package we introduced earlier. The remainder of this section then introduces various evaluation metrics that can be used to benchmark different counterfactual generators with respect to how they perform in the dynamic setting. 

## Simulations {#method-2-experiment}

The dynamics illustrated in Figure \@ref(fig:poc) in Section \@ref(intro) were generated through a simple experiment that aims to simulate the process of Algorithmic Recourse in practice. We begin in the static setting at time time $t=0$: given some classifier $M$ that was pre-trained on data $\mathcal{D}=\mathcal{D}_0 \cup \mathcal{D}_1$ we generate recourse for a random batch of $B$ individuals in the non-target class ($\mathcal{D}_0$). Note that we focus our attention on classification problems, since classification poses the most common practical use-case for Algorithmic Recourse. In order to simulate the dynamic process, we suppose that the model $M$ is retrained following the actual implementation of recourse in time $t=0$. Following the update to the model, we assume that at time $t=1$ recourse is generated for yet another random subset of individuals in the non-target class. This process is repeated for a number of time periods $T$. To get a clean read on endogenous dynamics we keep the total population of samples closed: we allow existing samples to move from factual to counterfactual states, but do not allow any entirely new samples to enter the population. The experimental setup is summarized in Algorithm \ref{algo-experiment}

\begin{algorithm}
\caption{Simulation Experiment}\label{algo-experiment}
\begin{algorithmic}[1]
\Procedure{Experiment}{$M,\mathcal{D},G$}
\State $t\gets 0$
\While{$t<T$}
\State $\mathcal{D}_B \subset \mathcal{D}_0$ \Comment{Sample from $\mathcal{D}_0$.}
\State $\mathcal{D}_B\gets G(\mathcal{D}_B)$ \Comment{Generate counterfactuals.}
\State $M\gets M(\mathcal{D})$ \Comment{Retrain model.}
\EndWhile
\State \textbf{return} $M,\mathcal{D}$
\EndProcedure
\end{algorithmic}
\end{algorithm}


A noteworthy practical consideration is the choice of $T$ and $B$. The higher these values, the more factual instances undergo recourse throughout the entire experiment. Of course, this is likely to lead to more pronounced domain and model shifts by time $T$. At the same time, it is generally improbable that a very large part of the population would request an explanation of the algorithm’s decisions. In our experiments, we choose the values such that $T \cdot B$ corresponds to the application of recourse on $\approx50\%$ of the negative instances from the initial dataset. As we collect data at each time $t$, we can also verify the impact of recourse when it is implemented for a smaller number of individuals. 

Algorithm \ref{algo-experiment} summarizes the proposed simulation experiment for a given dataset $\mathcal{D}$, model $M$ and generator $G$, but naturally we are interested in comparing simulation outcomes for different sources of data, models and generators. The framework we have built facilitates this, making use of multi-threading in order to speed up computations. Holding the initial model and dataset constant the experiments are run for all generators, since our primary concern is to benchmark different recourse methods. To ensure that each generator is faced with the exact same initial conditions in each round $t$, the candidate batch of individuals from the non-target class is randomly drawn from the intersection of all individuals in the non-target class across all experiments $\left\{\textsc{Experiment}(M,\mathcal{D},G)\right\}_{j=1}^J$ where $J$ is the total number of generators. 

## Evaluation Metrics {#method-2-metrics}

We formulate two desiderata for the set of metrics used to measure domain and model shifts induced by recourse. First, the metrics should be applicable regardless of the dataset or classification technique so that they allow for the meaningful comparison of the generators in various scenarios. As the knowledge of the underlying probability distribution is rarely available, the metrics should be empirical and non-parametric. This further ensures that we can also measure large datasets by sampling from the available data. Moreover, while our study was conducted in a two-class classification setting, our choice of metrics should remain applicable in the future research on multi- class recourse problems. Second, the set of metrics should allow to capture various aspects of the previously mentioned magnitude, path, and tempo of changes while remaining as small as possible.

### Domain Shifts

To quantify the magnitude of domain shifts we rely on an unbiased estimate of the squared population **Maximum Mean Discrepancy (MMD)** given as:

\begin{equation}
\begin{aligned}
MMD^2_u[F,{X}^\prime,\tilde{X}^\prime] &= \frac{1}{m(m-1)}\sum_{i=1}^m\sum_{j\neq i}^m k(x_i,x_j) \\ &+ \frac{1}{n(n-1)}\sum_{i=1}^n\sum_{j\neq i}^n k(\tilde{x}_i,\tilde{x}_j) \\ &- \frac{2}{mn}\sum_{i=1}^m\sum_{j=1}^n k(x_i,\tilde{x}_j) (\#eq:mmd)
\end{aligned}
\end{equation}

where $\mathcal{F}$ is a unit ball in a Reproducing Kernel Hilbert Space $\mathcal{H}$ @berlinet2011reproducing, and $X$, $\tilde{X}$ represent independent and identically distributed samples drawn from probability distributions $\mathcal{X}$ and $\mathcal{\tilde{X}}$ respectively @gretton2012kernel. MMD is a measure of the distance between the kernel mean embeddings of $\mathcal{X}$ and $\mathcal{\tilde{X}}$ in RKHS $\mathcal{H}$. An important consideration is the choice of the kernel function $k(\cdot,\cdot)$. In our implementation we make use of Gaussian kernel with a constant length-scale parameter of $0.5$. As the Gaussian kernel captures all moments of distributions $\mathcal{X}$ and $\mathcal{\tilde{X}}$, we have that $MMD_u^2[F,X,\tilde{X}]=0$ if and only if $X=\tilde{X}$.

The evaluation metric in Equation \@ref(eq:mmd) is computed after every round $t=1,...,T$ of the experiment. To assess the statistical significance of the observed shifts under the null hypothesis that samples $X$ and $\tilde{X}$ were drawn from the same probability distribution, we follow @arcones1992bootstrap. To that end, we combine the two samples and generate a large number of permutations of $X + \tilde{X}$. Then, we split the permuted data into two new samples $X^\prime$ and $\tilde{X}^\prime$ having the same size as the original samples. Then under the null hypothesis we should have that $MMD_u^2[F,X^\prime,\tilde{X}^\prime]$ be approximately equal to $MMD_u^2[F,X,\tilde{X}]$. The corresponding $p$-value can then be calculated by counting how these two quantities are not equal.

We calculate the MMD for both classes individually based on the ground truth labels, i.e. the labels that samples were assigned in time $t=0$. Throughout our experiments, we generally do not expect the distribution of the negative class to change over time – application of recourse reduces the size of this class, but since individuals are sampled uniformly the distribution should remain unaffected. Conversely, unless a recourse generator can perfectly replicate the original probability distribution, we expect the MMD of the positive class to increase. Thus, when discussing MMD, we generally mean the shift in the distribution of the positive class.

### Model Shifts

As our baseline for quantifying model shifts we measure perturbations to the model parameters at each point in time $t$ following @upadhyay2021towards. We define $\Delta=||\theta_{t+1}-\theta_{t}||^2$, that is the euclidean distance between the vectors of parameters before and after retraining the model $M$. We shall refer to this baseline metric simply as **Perturbations**.

We extend the metric in Equation \@ref(eq:mmd) for the purpose of quantifying model shifts. Specifically, we introduce **Predicted Probability MMD (PP MMD)**: instead of applying Equation \@ref(eq:mmd) to features directly, we apply it to the predicted probabilities assigned to a set of samples by the model $M$. If the model shifts, the probabilities assigned to each sample will change; again, this metric will equal 0 only if the two classifiers are the same. We compute PP MMD in two ways: firstly, we compute it over samples drawn uniformly from the dataset, and, secondly, we compute it over points spanning a meshgrid over a subspace of the entire feature space. For the latter approach we bound the subspace by the extrema of each feature. While this approach is theoretically more robust, unfortunately, it suffers from the curse of dimensionality, since it becomes increasingly difficult to select enough points to overcome noise as the dimension $D$ grows.

As an alternative to PP MMD we use a pseudo-distance for the **Disagreement Coefficient** (Disagreement). This metric was introduced in @hanneke2007bound and estimates $p(M(x) \neq M^\prime(x))$, that is the probability that two classifiers do not agree on the predicted outcome for a randomly chosen sample. Thus, it is not relevant whether the classification is correct according to the ground truth, but only whether the sample lies on the same side of the two respective decision boundaries. In our context, this metric quantifies the overlap between the initial model (trained before the application of recourse) and the updated model. A Disagreement Coefficient unequal to zero is indicative of a model shift. The opposite is not true: even if the Disagreement Coefficient is equal to zero a model shift may still have occured. This is one reason for why PP MMD is our our preferred metric.

We further introduce **Decisiveness** as a metric that quantifies the likelihood that a model assigns a high probability to its classification of any given sample. We define the metric simply as ${\frac{1}{N}}\sum_{i=0}^N(\sigma(M(x)) - 0.5)^2$ where $M(x)$ are predicted logits from a binary classifier and $\sigma$ denotes the sigmoid function. This metric provides an unbiased estimate of the binary classifier's tendency to produce high-confidence predictions in either one of the two classes. Although the exact values for this metric are not important for our study, they can be used to detect model shifts. If decisiveness changes over time, then this is indicative of the decision boundary moves towards either one of the two classes.

Finally, we also take a look at the out-of-sample **Performance** of our models. To this end, we compute their F-score on a test sample that we leave untouched throughout the experiment. 

