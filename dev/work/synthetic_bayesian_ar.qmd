---
jupyter: julia-1.6
---

# Algorithmic recourse for Bayesian classifiers (Schut et al. 2021)

In this notebook we see how algorithmic recourse can be implemented for Bayesian classifiers. In particular, we follow the implementation in Schut et al (2021).

```{julia}
include("../src/load.jl")
using AlgorithmicRecourse
using CSV;
using DataFrames;
using LinearAlgebra;
using Distributions;
output_folder = "output/synthetic"
www_folder = "www/synthetic"
using Base.Filesystem: joinpath
using Images, FileIO
```

## Fitting the Bayesian classifier

```{julia}
df = CSV.read("../data/cats_dogs_large.csv", DataFrame);
```

For plotting we will use the R package `ggplot2` which can be readily integrated into Julia (just like many other R packages, see for example [here](https://avt.im/blog/2018/03/23/R-packages-ggplot-in-julia#fn:rcall)).

```{julia}
using RCall
@rlibrary ggplot2
@rlibrary ggimage
@rlibrary emojifont
@rlibrary viridis
@rlibrary magick
```

```{julia}
y = df[:,:y];
N = length(y);
X = Matrix(df[:,Not(:y)]);
X = [ones(N) X]; # add for constant
d = size(X)[2]; # number of features
w_0 = zeros(d); # zero-mean prior
λ = 0.005
H_0 = UniformScaling(λ);
```

```{julia}
model = Models.bayes_logreg(X,y);
w = model.μ;
```

```{julia}
step = 1;
expand = 20;
W = expandgrid((minimum(df.x1)-expand):step:(maximum(df.x1)+expand),(minimum(df.x2)-expand):step:(maximum(df.x2)+expand));
N_grid = size(W)[1];
```

```{julia}
y_probit = Models.predict(model, hcat(ones(N_grid), W));
estimates = DataFrame(hcat(y_probit,W), :auto);
features = copy(df);
transform!(features, :y => (x -> ifelse.(x.==0,"../www/cat.png","../www/dog.png"))  => :emoji);
```

```{julia}
ggplot() + 
  geom_contour_filled(data=estimates, aes(x = :x2, y = :x3, z = :x1), bins=25, alpha=0.8) +
  geom_image(data=features, aes(x=:x1, y=:x2, image=:emoji), size=0.1) +
  guides(fill="none") +
  scale_fill_viridis(option="C",discrete=true) +
  scale_x_continuous(expand = R"c(0, 0)") + scale_y_continuous(expand = R"c(0, 0)") +
  labs(x="Height", y="Tail") +
  theme_bw() |>
  p -> ggsave(joinpath(www_folder, "predictive.png"), plot = p, width=5, height=5);
```

![](www/synthetic/predictive.png)

## Recourse algorithm

We have our fitted logistic regression classifier $f(\mathbf{x}^\text{CF})$. The loss for our recourse objective is $\ell(f(\mathbf{x}^\text{CF}),1))$. Using Hinge loss we have

$$\ell(f(\mathbf{x}^\text{CF}),1))=(1-z\cdot logits(f(\mathbf{x}^\text{CF}),1)))_{+}$$

where $z=1$ if $y=1$ and $z=-1$ if $y=0$. Furthermore, $logits(\cdot)$ in this particular case just refers to the linear predictions that enter the sigmoid function to produce predicted probabilities. 

This yields the following gradient for our recourse problem:

$$
\begin{aligned}
&& \nabla_{\mathbf{x}^\text{CF}} \left( \ell(f(\mathbf{x}^\text{CF}),1)) \right) &= \begin{cases} -z \cdot \hat{\mathbf{w}}  & \text{if} \ \ \ \hat{\mathbf{w}}^T\mathbf{x}^\text{CF} z \le 1\\ \mathbf{0} & \text{otherwise} \end{cases} \\
\end{aligned}
$$

### Schut et al. 

The algorithm used to generate recourse in Schut et al. (2021) is based on the Jacobian-based Saliency Map Attack (JSMA). Let's refer to the recourse variant as Jacobian-based Saliency Map CE (JSMCE). It can be coded up as follows:

## Generating recourse

```{julia}
using AlgorithmicRecourse.Models: BayesianLogisticModel
𝑴 = BayesianLogisticModel(reshape(model.μ,1,length(model.μ)), model.Σ); # repare for usage with AlgorithmicRecourse.jl
x̅ = [50.0, 60.0]; # factual
using Random
Random.seed!(1234)
y̅ = round(AlgorithmicRecourse.Models.probs(𝑴, x̅)[1])
target = ifelse(y̅==1.0,0.0,1.0) # opposite label as target
γ = ifelse(target==1.0,0.9,0.1); # desired threshold based on target
```

```{julia}
generic = GenericGenerator(0.0,0.1,1e-5,:logitbinarycrossentropy,nothing)
greedy = GreedyGenerator(1,30,:logitbinarycrossentropy,nothing)
```

```{julia}
recourse_generic = generate_recourse(generic, x̅, 𝑴, target, γ) # generate recourse
recourse_greedy = generate_recourse(greedy, x̅, 𝑴, target, γ);
```

```{julia}
path_df = DataFrame();
recourses = (generic = recourse_generic, greedy = recourse_greedy)
for i in 1:length(recourses)
    recourse_name = keys(recourses)[i]
    recourse = recourses[i]
    path = DataFrame(recourse.path, :auto)
    insertcols!(path, :type => titlecase(String(recourse_name)))
    insertcols!(path, :emoji=> "../www/cat.png")
    path_df = vcat(path_df, path)
end;
```

## Results

```{julia}
ggplot() + 
    geom_contour_filled(data=estimates, aes(x = :x2, y = :x3, z = :x1), bins=25, alpha=0.8) +
    geom_abline(intercept=1-w[1], slope=-w[2]/w[3]) +
    geom_image(data=features, aes(x=:x1, y=:x2, image=:emoji), size=0.1, image_fun=R"function(img) magick::image_fx(img, expression = '0.5*a', channel = 'alpha')") +
    geom_image(data=path_df, aes(x=:x1, y=:x2, image=:emoji), size=0.1) +
    guides(fill="none") +
    scale_fill_viridis(option="C",discrete=true) +
    scale_x_continuous(expand = R"c(0, 0)") + scale_y_continuous(expand = R"c(0, 0)") +
    labs(x="Height", y="Tail") +
    facet_wrap(R".~type") +
    theme_bw() |>
    p -> ggsave(joinpath(www_folder,"path.png"), plot = p, width=8, height=4);
```

As we can see in the figure below, the approach by Schut et al. results in a counter factual explanation with high posterior predictive likelihood. Conversely, the conventional approach (Wachter) generates recourse that lies right on the decision boundary where predictive uncertainty is very high.

```{julia}
load(joinpath(www_folder,"path.png"))
```

